{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning_from_Scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "msVccoBaWgQ1",
        "XWG4D2ZkBp51",
        "XY6Gg__qwi1K",
        "VOAfP93TXhgz",
        "H1LfYp82Xtoy",
        "TorgALP16HYK",
        "j1C-pqsytiLD",
        "nnxfN_oWz5gX",
        "c-Fr9mhc6fGT"
      ],
      "authorship_tag": "ABX9TyOT9AkqJ9dAlxnvJtyYUY2a"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArntJadCwJBJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f804624-8545-473b-f927-90366745a771"
      },
      "source": [
        "import sys, os\r\n",
        "\r\n",
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch.git\r\n",
        "os.chdir(\"/content/deep-learning-from-scratch/ch01\")\r\n",
        "sys.path.append(os.pardir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch'...\n",
            "remote: Enumerating objects: 826, done.\u001b[K\n",
            "remote: Total 826 (delta 0), reused 0 (delta 0), pack-reused 826\u001b[K\n",
            "Receiving objects: 100% (826/826), 52.21 MiB | 25.34 MiB/s, done.\n",
            "Resolving deltas: 100% (477/477), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bqvYumTYFkFW",
        "outputId": "1676eb2f-c612-4389-f10d-dbf3926df2e5"
      },
      "source": [
        "import sys, os\r\n",
        "\r\n",
        "!git clone https://github.com/WegraLee/deep-learning-from-scratch-2.git\r\n",
        "os.chdir(\"/content/deep-learning-from-scratch-2/ch07\")\r\n",
        "sys.path.append(os.pardir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'deep-learning-from-scratch-2'...\n",
            "remote: Enumerating objects: 18, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 598 (delta 10), reused 0 (delta 0), pack-reused 580\u001b[K\n",
            "Receiving objects: 100% (598/598), 29.81 MiB | 4.90 MiB/s, done.\n",
            "Resolving deltas: 100% (359/359), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4KQu_8JwKhs"
      },
      "source": [
        "import numpy as np\r\n",
        "import time\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from common.np import *  \r\n",
        "from common.util import clip_grads\r\n",
        "from common.optimizer import *\r\n",
        "from common.trainer import RnnlmTrainer\r\n",
        "from dataset import ptb, sequence\r\n",
        "from ch06.rnnlm import Rnnlm\r\n",
        "from common.util import eval_perplexity\r\n",
        "from ch06.better_rnnlm import BetterRnnlm\r\n",
        "from common.time_layers import *\r\n",
        "from common.base_model import BaseModel\r\n",
        "from common.util import eval_seq2seq\r\n",
        "from ch07.seq2seq import Seq2seq\r\n",
        "from ch07.peeky_seq2seq import PeekySeq2seq\r\n",
        "from common.trainer import Trainer\r\n",
        "from ch08.attention_seq2seq import AttentionSeq2seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msVccoBaWgQ1"
      },
      "source": [
        "# Multilayer Perceptron (MLP)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XWG4D2ZkBp51"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VmvEBy_0mCoc"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True)\r\n",
        "x_train = x_train[:8000]\r\n",
        "t_train = t_train[:8000]\r\n",
        "x_test=x_test[:2000]\r\n",
        "t_test=t_test[:2000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YLjDYbAUjTU2",
        "outputId": "3a5b6914-5147-4b56-98a5-48f5e1997e16"
      },
      "source": [
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100],\r\n",
        "                              output_size=10, use_dropout=True, dropout_ration=0.2)\r\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\r\n",
        "                  epochs=200, mini_batch_size=100,\r\n",
        "                  optimizer='sgd', optimizer_param={'lr': 0.01}, verbose=True)\r\n",
        "trainer.train()\r\n",
        "\r\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\r\n",
        "\r\n",
        "markers = {'train': 'o', 'test': 's'}\r\n",
        "x = np.arange(len(train_acc_list))\r\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\r\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\r\n",
        "plt.xlabel(\"epochs\")\r\n",
        "plt.ylabel(\"accuracy\")\r\n",
        "plt.ylim(0, 1.0)\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.00798937000711737\n",
            "train loss:0.0856124207554004\n",
            "train loss:0.08997405861909465\n",
            "train loss:0.07901276167794845\n",
            "train loss:0.05322514592176806\n",
            "train loss:0.041264759974042706\n",
            "train loss:0.010012114876264928\n",
            "train loss:0.04589387434238792\n",
            "train loss:0.017062825229241445\n",
            "train loss:0.03778103640206883\n",
            "train loss:0.11985145989990198\n",
            "train loss:0.11847311856884145\n",
            "train loss:0.04496926523271658\n",
            "train loss:0.05477236479256039\n",
            "train loss:0.007771780929959227\n",
            "train loss:0.012345853171837966\n",
            "train loss:0.015048572097519237\n",
            "train loss:0.052243827780744316\n",
            "train loss:0.004097072248072044\n",
            "train loss:0.15182658042051622\n",
            "train loss:0.04485478175053033\n",
            "train loss:0.008104112470587142\n",
            "train loss:0.009126816430342933\n",
            "train loss:0.06735174569330604\n",
            "train loss:0.04758284406061092\n",
            "train loss:0.031112956774196534\n",
            "train loss:0.04701799687701341\n",
            "train loss:0.008087939883870212\n",
            "train loss:0.020899242570752617\n",
            "train loss:0.10293882686663154\n",
            "train loss:0.01513490258297026\n",
            "train loss:0.017676582871915418\n",
            "train loss:0.009630662081508111\n",
            "train loss:0.14596631715836958\n",
            "train loss:0.010836307679986182\n",
            "train loss:0.01710628699907647\n",
            "train loss:0.08183589298350472\n",
            "train loss:0.14845333284576276\n",
            "train loss:0.016338585014934525\n",
            "train loss:0.07760653483962901\n",
            "train loss:0.012269297181104455\n",
            "train loss:0.019237080388825788\n",
            "train loss:0.01513965204514846\n",
            "train loss:0.027257615446572247\n",
            "train loss:0.040659774893672986\n",
            "train loss:0.08804768124205621\n",
            "train loss:0.028416882589394933\n",
            "train loss:0.049155749848111475\n",
            "train loss:0.017197108168978865\n",
            "train loss:0.02131055144413734\n",
            "train loss:0.04003341349137646\n",
            "train loss:0.031112491256676415\n",
            "train loss:0.1221524972151868\n",
            "train loss:0.01990426044746048\n",
            "train loss:0.07106878808945774\n",
            "train loss:0.030481372995362913\n",
            "train loss:0.09852757680572609\n",
            "train loss:0.06764635128433487\n",
            "=== epoch:140, train acc:0.987, test acc:0.9115 ===\n",
            "train loss:0.010788191095301512\n",
            "train loss:0.019498913764639074\n",
            "train loss:0.007615412078870168\n",
            "train loss:0.13539913445182108\n",
            "train loss:0.010982665996023332\n",
            "train loss:0.09022842676930926\n",
            "train loss:0.07265607666589803\n",
            "train loss:0.03891558926225151\n",
            "train loss:0.05982333507671282\n",
            "train loss:0.05322142615884669\n",
            "train loss:0.038243662079344945\n",
            "train loss:0.02248028300798498\n",
            "train loss:0.05401801380360452\n",
            "train loss:0.06957087003784386\n",
            "train loss:0.020098656396929676\n",
            "train loss:0.03921064517026184\n",
            "train loss:0.17269293755794007\n",
            "train loss:0.04018529490098752\n",
            "train loss:0.05153130770973103\n",
            "train loss:0.03447218756161175\n",
            "train loss:0.03591566308732163\n",
            "train loss:0.038204886510712216\n",
            "train loss:0.02372801601375286\n",
            "train loss:0.047833046779787614\n",
            "train loss:0.044220550827018104\n",
            "train loss:0.042797872479052604\n",
            "train loss:0.03533091976712195\n",
            "train loss:0.03341345444694542\n",
            "train loss:0.07342330179273135\n",
            "train loss:0.012757364182962921\n",
            "train loss:0.03249290034440859\n",
            "train loss:0.06349320176489515\n",
            "train loss:0.06211340391527649\n",
            "train loss:0.041510458530963676\n",
            "train loss:0.031101743687531058\n",
            "train loss:0.005100827555113669\n",
            "train loss:0.041696075302941615\n",
            "train loss:0.0319267101616495\n",
            "train loss:0.0834393568286552\n",
            "train loss:0.06269751776504778\n",
            "train loss:0.024512108258581272\n",
            "train loss:0.02353135525926894\n",
            "train loss:0.041310636938308665\n",
            "train loss:0.03705796887293287\n",
            "train loss:0.10785833251637866\n",
            "train loss:0.04467630640839605\n",
            "train loss:0.09003118000473204\n",
            "train loss:0.08581072692748436\n",
            "train loss:0.007801690911926363\n",
            "train loss:0.01054742395216933\n",
            "train loss:0.024778982875322448\n",
            "train loss:0.015542935913559486\n",
            "train loss:0.024382077516315156\n",
            "train loss:0.05866443338305407\n",
            "train loss:0.03926042439899633\n",
            "train loss:0.06021468434006353\n",
            "train loss:0.03089908046281006\n",
            "train loss:0.032600745029976425\n",
            "train loss:0.08630726824788625\n",
            "train loss:0.026051720758057992\n",
            "train loss:0.041538703302380436\n",
            "train loss:0.014342397412338632\n",
            "train loss:0.04909573474059913\n",
            "train loss:0.0693379800308253\n",
            "train loss:0.03600430315157597\n",
            "train loss:0.04044721082916517\n",
            "train loss:0.15009642751669228\n",
            "train loss:0.0034629938164961378\n",
            "train loss:0.010213859714077202\n",
            "train loss:0.07915043386255222\n",
            "train loss:0.046905480646643294\n",
            "train loss:0.04715196940841348\n",
            "train loss:0.06988972062467042\n",
            "train loss:0.009015301000154845\n",
            "train loss:0.05299297343764145\n",
            "train loss:0.05175891486144733\n",
            "train loss:0.07539762639885458\n",
            "train loss:0.01967186110838909\n",
            "train loss:0.10469030381716816\n",
            "train loss:0.009473219250229642\n",
            "=== epoch:141, train acc:0.98775, test acc:0.913 ===\n",
            "train loss:0.023510780044734284\n",
            "train loss:0.0551651397341441\n",
            "train loss:0.013858813715320226\n",
            "train loss:0.023682831154236653\n",
            "train loss:0.016661051959384797\n",
            "train loss:0.04091950493133452\n",
            "train loss:0.16038219814625887\n",
            "train loss:0.06889531746362312\n",
            "train loss:0.07973224638795419\n",
            "train loss:0.011617435418464096\n",
            "train loss:0.03273137326440349\n",
            "train loss:0.011872591681257181\n",
            "train loss:0.009252862771892623\n",
            "train loss:0.08996066244138837\n",
            "train loss:0.05300945541356624\n",
            "train loss:0.032949848442173024\n",
            "train loss:0.02103667411567372\n",
            "train loss:0.12878483231422203\n",
            "train loss:0.014609203822313267\n",
            "train loss:0.20084443040905187\n",
            "train loss:0.029103481404079465\n",
            "train loss:0.02127280362438455\n",
            "train loss:0.01461684410933988\n",
            "train loss:0.0073723524363062684\n",
            "train loss:0.01652864621638492\n",
            "train loss:0.0055523146500034435\n",
            "train loss:0.013327584863614317\n",
            "train loss:0.04105176163694082\n",
            "train loss:0.06409926925175297\n",
            "train loss:0.009656103258409425\n",
            "train loss:0.09285733280678404\n",
            "train loss:0.03594330137678642\n",
            "train loss:0.06066986145658765\n",
            "train loss:0.011905577582229217\n",
            "train loss:0.0228939468417314\n",
            "train loss:0.03507805903411975\n",
            "train loss:0.0754772734830972\n",
            "train loss:0.008419206353810134\n",
            "train loss:0.049491028011692244\n",
            "train loss:0.02328661714513808\n",
            "train loss:0.05289753467845744\n",
            "train loss:0.053400425760386255\n",
            "train loss:0.008023623841917524\n",
            "train loss:0.008069980174639299\n",
            "train loss:0.00958461689334272\n",
            "train loss:0.026221722012403706\n",
            "train loss:0.04710954143319226\n",
            "train loss:0.03872078739651515\n",
            "train loss:0.0398629612227114\n",
            "train loss:0.05355345148631989\n",
            "train loss:0.07464017172023216\n",
            "train loss:0.030327086413240015\n",
            "train loss:0.04959814586041857\n",
            "train loss:0.08498164335147677\n",
            "train loss:0.08291493696604928\n",
            "train loss:0.09863159925382123\n",
            "train loss:0.03167644852379489\n",
            "train loss:0.09049221975852693\n",
            "train loss:0.021163562631190885\n",
            "train loss:0.01624911881632456\n",
            "train loss:0.04699174181077879\n",
            "train loss:0.061501230918930776\n",
            "train loss:0.18354868956260068\n",
            "train loss:0.08345737886307811\n",
            "train loss:0.045418224396031395\n",
            "train loss:0.01669582713782734\n",
            "train loss:0.13318731035447212\n",
            "train loss:0.07680238215859812\n",
            "train loss:0.016484062030044634\n",
            "train loss:0.030362802703658067\n",
            "train loss:0.05628944282753712\n",
            "train loss:0.009448581064472144\n",
            "train loss:0.022702104907658952\n",
            "train loss:0.07629395810747858\n",
            "train loss:0.016750292474710347\n",
            "train loss:0.052193804007306346\n",
            "train loss:0.05023864888364977\n",
            "train loss:0.005050146774838345\n",
            "train loss:0.016713810908271756\n",
            "train loss:0.028622037256335376\n",
            "=== epoch:142, train acc:0.98725, test acc:0.9135 ===\n",
            "train loss:0.06644875563881543\n",
            "train loss:0.08071473451738946\n",
            "train loss:0.03025837328541173\n",
            "train loss:0.015051391028045895\n",
            "train loss:0.011186030283161525\n",
            "train loss:0.012731965224393042\n",
            "train loss:0.00803498562645279\n",
            "train loss:0.08329256064027707\n",
            "train loss:0.012743519895341003\n",
            "train loss:0.053297099290976195\n",
            "train loss:0.010573028942464053\n",
            "train loss:0.011519239601816498\n",
            "train loss:0.010378145409443451\n",
            "train loss:0.014695726366896871\n",
            "train loss:0.015402460601833916\n",
            "train loss:0.09696128035801177\n",
            "train loss:0.0788816568958402\n",
            "train loss:0.043090565746049636\n",
            "train loss:0.019949164918274136\n",
            "train loss:0.007582112504524249\n",
            "train loss:0.009307204801493388\n",
            "train loss:0.03231895318270192\n",
            "train loss:0.08304778417763384\n",
            "train loss:0.0414902519358509\n",
            "train loss:0.07766905457246716\n",
            "train loss:0.041019505050557445\n",
            "train loss:0.014521820757077301\n",
            "train loss:0.10475071305852007\n",
            "train loss:0.05827104249997125\n",
            "train loss:0.022676025171274518\n",
            "train loss:0.07080048178318533\n",
            "train loss:0.006634941994429906\n",
            "train loss:0.044331016489344746\n",
            "train loss:0.006714447924605159\n",
            "train loss:0.04132829657643536\n",
            "train loss:0.03982748160553402\n",
            "train loss:0.07470024225378562\n",
            "train loss:0.04535784575381283\n",
            "train loss:0.046893554953876926\n",
            "train loss:0.055887803621492226\n",
            "train loss:0.05506784016592178\n",
            "train loss:0.004703768707960979\n",
            "train loss:0.03234521790711172\n",
            "train loss:0.05586492560986112\n",
            "train loss:0.006955948093221861\n",
            "train loss:0.0453770926385004\n",
            "train loss:0.024981327828302494\n",
            "train loss:0.09565319618326336\n",
            "train loss:0.08413643063611832\n",
            "train loss:0.014359311657573743\n",
            "train loss:0.08399219658834872\n",
            "train loss:0.02002035768028123\n",
            "train loss:0.013540814815539476\n",
            "train loss:0.034737337045312115\n",
            "train loss:0.0236913959151256\n",
            "train loss:0.09754762178880823\n",
            "train loss:0.011482117246593896\n",
            "train loss:0.012581517208602795\n",
            "train loss:0.024754932036741076\n",
            "train loss:0.01133815011597293\n",
            "train loss:0.007054254461351144\n",
            "train loss:0.022900187873870093\n",
            "train loss:0.1437609873384196\n",
            "train loss:0.06407998322248913\n",
            "train loss:0.06123967324256503\n",
            "train loss:0.04614529615897589\n",
            "train loss:0.042796331635433754\n",
            "train loss:0.04046653744563873\n",
            "train loss:0.03205891572654798\n",
            "train loss:0.084112227356036\n",
            "train loss:0.07441293115813767\n",
            "train loss:0.009472167397129739\n",
            "train loss:0.014305800291349193\n",
            "train loss:0.041763750615965635\n",
            "train loss:0.10606728004499531\n",
            "train loss:0.00638916994908664\n",
            "train loss:0.044449423586909294\n",
            "train loss:0.06240935312920049\n",
            "train loss:0.03405691138209005\n",
            "train loss:0.018866157792373196\n",
            "=== epoch:143, train acc:0.987625, test acc:0.9155 ===\n",
            "train loss:0.04955269642751715\n",
            "train loss:0.05029178284766263\n",
            "train loss:0.050521359234714555\n",
            "train loss:0.017917240196579327\n",
            "train loss:0.023956137387769156\n",
            "train loss:0.010105405860534471\n",
            "train loss:0.021455675728641238\n",
            "train loss:0.08984623197236774\n",
            "train loss:0.04044740505351355\n",
            "train loss:0.08082936239043649\n",
            "train loss:0.06243445831429709\n",
            "train loss:0.012589467539286994\n",
            "train loss:0.02584701756313409\n",
            "train loss:0.01102594081967932\n",
            "train loss:0.025453499745957803\n",
            "train loss:0.03967754030380465\n",
            "train loss:0.011174949453165271\n",
            "train loss:0.0745109445003977\n",
            "train loss:0.020378064369114823\n",
            "train loss:0.00878201889586555\n",
            "train loss:0.010412976408050754\n",
            "train loss:0.06017540627762116\n",
            "train loss:0.06740775638907175\n",
            "train loss:0.00941755099355909\n",
            "train loss:0.05949353616516729\n",
            "train loss:0.009790999738520744\n",
            "train loss:0.04850419516128853\n",
            "train loss:0.09413954116198503\n",
            "train loss:0.01035397432224109\n",
            "train loss:0.03917533025522677\n",
            "train loss:0.03268594833526997\n",
            "train loss:0.007944360463900471\n",
            "train loss:0.06120514342446896\n",
            "train loss:0.017188931224978114\n",
            "train loss:0.009240547556892574\n",
            "train loss:0.028132181191974387\n",
            "train loss:0.03844308556012168\n",
            "train loss:0.07333925458429257\n",
            "train loss:0.02314055910227676\n",
            "train loss:0.027279832628084602\n",
            "train loss:0.04351645176768721\n",
            "train loss:0.03262769148151067\n",
            "train loss:0.055725822416376365\n",
            "train loss:0.019873339540356194\n",
            "train loss:0.012275807608769658\n",
            "train loss:0.10569326801756761\n",
            "train loss:0.09568662998153395\n",
            "train loss:0.026134073843607088\n",
            "train loss:0.006798924527465845\n",
            "train loss:0.004785226088955987\n",
            "train loss:0.031215861883231423\n",
            "train loss:0.020028357711718323\n",
            "train loss:0.009336000093217653\n",
            "train loss:0.05012925764703957\n",
            "train loss:0.02017196644610593\n",
            "train loss:0.04634245342703836\n",
            "train loss:0.027100558872830516\n",
            "train loss:0.04312385427400248\n",
            "train loss:0.06708646379117499\n",
            "train loss:0.02712478371482393\n",
            "train loss:0.02494595700729146\n",
            "train loss:0.06753537850646295\n",
            "train loss:0.07930985765237536\n",
            "train loss:0.1008229765423088\n",
            "train loss:0.007999763692239195\n",
            "train loss:0.06860278775342818\n",
            "train loss:0.043261949156632264\n",
            "train loss:0.046185716361843474\n",
            "train loss:0.06660016095946092\n",
            "train loss:0.04287178089956693\n",
            "train loss:0.017867444101405717\n",
            "train loss:0.009765799570292574\n",
            "train loss:0.07909410604990871\n",
            "train loss:0.009506953627760155\n",
            "train loss:0.0958635634177109\n",
            "train loss:0.039459222317888315\n",
            "train loss:0.02605787789063203\n",
            "train loss:0.13374106978991093\n",
            "train loss:0.033331518248516705\n",
            "train loss:0.16996847532094958\n",
            "=== epoch:144, train acc:0.987625, test acc:0.916 ===\n",
            "train loss:0.007065735674002641\n",
            "train loss:0.022211311382493864\n",
            "train loss:0.010852333157814253\n",
            "train loss:0.02538020975755798\n",
            "train loss:0.06126807732694212\n",
            "train loss:0.03869310326337774\n",
            "train loss:0.06743998675330021\n",
            "train loss:0.03346450472065147\n",
            "train loss:0.011851409370454319\n",
            "train loss:0.06688478986571296\n",
            "train loss:0.030888809584132395\n",
            "train loss:0.04207880874390994\n",
            "train loss:0.016405714833194408\n",
            "train loss:0.13525442885188416\n",
            "train loss:0.016624844758490277\n",
            "train loss:0.0598093140233209\n",
            "train loss:0.06095149532340415\n",
            "train loss:0.011497602122698243\n",
            "train loss:0.04952230361520074\n",
            "train loss:0.0800239198054943\n",
            "train loss:0.026633251524814997\n",
            "train loss:0.02158506396065125\n",
            "train loss:0.011267484407839437\n",
            "train loss:0.00425650518866804\n",
            "train loss:0.032839077262622196\n",
            "train loss:0.018892862211753816\n",
            "train loss:0.01398559664665441\n",
            "train loss:0.00716863159900307\n",
            "train loss:0.07000493276853442\n",
            "train loss:0.016430627881937686\n",
            "train loss:0.0649710256297763\n",
            "train loss:0.0823999025607333\n",
            "train loss:0.07416376059607234\n",
            "train loss:0.06089310265734213\n",
            "train loss:0.0089458114354103\n",
            "train loss:0.008809382126472216\n",
            "train loss:0.011584023711411057\n",
            "train loss:0.022925978776265853\n",
            "train loss:0.04201362577190403\n",
            "train loss:0.1539390938747422\n",
            "train loss:0.008153682532412573\n",
            "train loss:0.027845710241550657\n",
            "train loss:0.017382939784280557\n",
            "train loss:0.04705568800965636\n",
            "train loss:0.04332824898373326\n",
            "train loss:0.06454672555153632\n",
            "train loss:0.023899591547889974\n",
            "train loss:0.013390770095259843\n",
            "train loss:0.1674893565490558\n",
            "train loss:0.08719985392880641\n",
            "train loss:0.04901916152972054\n",
            "train loss:0.19259305718179764\n",
            "train loss:0.040652735171049746\n",
            "train loss:0.03138319353025328\n",
            "train loss:0.030846488479916726\n",
            "train loss:0.05022229116793568\n",
            "train loss:0.015296148349759828\n",
            "train loss:0.03009809884610906\n",
            "train loss:0.07562060719723886\n",
            "train loss:0.04620801282613399\n",
            "train loss:0.04179943017332919\n",
            "train loss:0.06430804161149553\n",
            "train loss:0.06307319061766065\n",
            "train loss:0.02055109086021632\n",
            "train loss:0.009497041007637947\n",
            "train loss:0.026507665893109426\n",
            "train loss:0.08406934387246061\n",
            "train loss:0.02738214445123157\n",
            "train loss:0.13680988441482392\n",
            "train loss:0.08696934613411512\n",
            "train loss:0.07399534728794846\n",
            "train loss:0.04171255196555951\n",
            "train loss:0.018063598091874963\n",
            "train loss:0.05505812064474614\n",
            "train loss:0.03316509276000869\n",
            "train loss:0.07853443002351185\n",
            "train loss:0.08732371431293641\n",
            "train loss:0.047167305793775934\n",
            "train loss:0.0625460039079945\n",
            "train loss:0.033552993009601156\n",
            "=== epoch:145, train acc:0.98825, test acc:0.9165 ===\n",
            "train loss:0.025953375179286312\n",
            "train loss:0.0430232726808156\n",
            "train loss:0.04716293636774533\n",
            "train loss:0.027763505822845976\n",
            "train loss:0.05775621523408221\n",
            "train loss:0.008709949560072774\n",
            "train loss:0.021576683303749324\n",
            "train loss:0.05185044598510274\n",
            "train loss:0.011333742750491404\n",
            "train loss:0.010878348296299118\n",
            "train loss:0.03834632356591521\n",
            "train loss:0.12005369977869212\n",
            "train loss:0.02583568530070903\n",
            "train loss:0.017049744783412674\n",
            "train loss:0.050699568672320304\n",
            "train loss:0.010860681922363391\n",
            "train loss:0.026478315086025397\n",
            "train loss:0.005676200853536211\n",
            "train loss:0.018367240560280752\n",
            "train loss:0.008844308498122145\n",
            "train loss:0.05477494303507106\n",
            "train loss:0.05030546094533242\n",
            "train loss:0.019979378848826997\n",
            "train loss:0.03500854279159554\n",
            "train loss:0.06466706262807237\n",
            "train loss:0.051191961817841476\n",
            "train loss:0.024899132725068465\n",
            "train loss:0.02919473839299011\n",
            "train loss:0.01269880107699758\n",
            "train loss:0.03214016266945875\n",
            "train loss:0.06967250590188594\n",
            "train loss:0.020011021179458903\n",
            "train loss:0.011611779331851582\n",
            "train loss:0.05953919405504884\n",
            "train loss:0.02965831268275765\n",
            "train loss:0.16106159725009936\n",
            "train loss:0.0924977327619003\n",
            "train loss:0.04922842810464867\n",
            "train loss:0.06710697691353902\n",
            "train loss:0.007854271957021524\n",
            "train loss:0.01046250742004584\n",
            "train loss:0.03517581369885386\n",
            "train loss:0.030457682348597123\n",
            "train loss:0.1016921671023833\n",
            "train loss:0.06123283087889118\n",
            "train loss:0.032618484769789076\n",
            "train loss:0.037905900956251175\n",
            "train loss:0.009201440204076634\n",
            "train loss:0.05707359226179917\n",
            "train loss:0.012718978978803433\n",
            "train loss:0.017468371802286824\n",
            "train loss:0.06587660117601685\n",
            "train loss:0.02979478979918015\n",
            "train loss:0.007442379812662781\n",
            "train loss:0.044064013451061236\n",
            "train loss:0.022685489787134454\n",
            "train loss:0.015560142216365076\n",
            "train loss:0.05536080193030038\n",
            "train loss:0.04839212088538224\n",
            "train loss:0.016695454660294377\n",
            "train loss:0.029899321430162714\n",
            "train loss:0.011830451149307841\n",
            "train loss:0.02097070315957734\n",
            "train loss:0.030940249486517666\n",
            "train loss:0.07856112639300152\n",
            "train loss:0.04823004614287705\n",
            "train loss:0.022500118329634115\n",
            "train loss:0.052593220052213924\n",
            "train loss:0.015866409665755442\n",
            "train loss:0.03609848705885596\n",
            "train loss:0.01101713595938807\n",
            "train loss:0.16975151176864497\n",
            "train loss:0.021349687275898907\n",
            "train loss:0.10097347284108571\n",
            "train loss:0.015218401748685296\n",
            "train loss:0.04680007996339014\n",
            "train loss:0.06035474973648808\n",
            "train loss:0.056797843950813685\n",
            "train loss:0.013101137103636024\n",
            "train loss:0.0171227591668287\n",
            "=== epoch:146, train acc:0.9885, test acc:0.919 ===\n",
            "train loss:0.08693664020150921\n",
            "train loss:0.025781143231429583\n",
            "train loss:0.03722906912137112\n",
            "train loss:0.014038945073524227\n",
            "train loss:0.1693643117889602\n",
            "train loss:0.007063620044785014\n",
            "train loss:0.11737087221986484\n",
            "train loss:0.022552133125526314\n",
            "train loss:0.06669026669746975\n",
            "train loss:0.0250374981105577\n",
            "train loss:0.04661809208182427\n",
            "train loss:0.07924240593100629\n",
            "train loss:0.046387345938634444\n",
            "train loss:0.050679315104242145\n",
            "train loss:0.010064419127894481\n",
            "train loss:0.03101343539201746\n",
            "train loss:0.05478610573419759\n",
            "train loss:0.02185270244110317\n",
            "train loss:0.038021636107942534\n",
            "train loss:0.039644209365755814\n",
            "train loss:0.011110017797163103\n",
            "train loss:0.021581995237990186\n",
            "train loss:0.014733661877788908\n",
            "train loss:0.06879375027730916\n",
            "train loss:0.02533298187424636\n",
            "train loss:0.059383122904951714\n",
            "train loss:0.022428858524053418\n",
            "train loss:0.146760559421121\n",
            "train loss:0.02328058217622571\n",
            "train loss:0.031177834833814602\n",
            "train loss:0.04278799353646402\n",
            "train loss:0.03869847337880708\n",
            "train loss:0.10412173496264984\n",
            "train loss:0.036753988764817266\n",
            "train loss:0.1019095391738309\n",
            "train loss:0.04756669913294174\n",
            "train loss:0.05362059106806641\n",
            "train loss:0.02243176049429295\n",
            "train loss:0.0687188878133936\n",
            "train loss:0.008056776569439454\n",
            "train loss:0.12038845587413434\n",
            "train loss:0.020330854185286143\n",
            "train loss:0.0573660521270352\n",
            "train loss:0.010010701013705102\n",
            "train loss:0.011199946262533804\n",
            "train loss:0.040609012516347416\n",
            "train loss:0.014702146467300906\n",
            "train loss:0.03498354179447877\n",
            "train loss:0.0148623792108918\n",
            "train loss:0.03745638985778855\n",
            "train loss:0.007491362154019655\n",
            "train loss:0.026825999640138275\n",
            "train loss:0.01796113271135681\n",
            "train loss:0.007423549569331666\n",
            "train loss:0.040115814795048305\n",
            "train loss:0.025098337434355612\n",
            "train loss:0.05406059505181289\n",
            "train loss:0.0129681434838335\n",
            "train loss:0.04135052476602587\n",
            "train loss:0.051023943956661356\n",
            "train loss:0.008223403048498954\n",
            "train loss:0.04610564580892259\n",
            "train loss:0.007804119333255806\n",
            "train loss:0.00625976173486052\n",
            "train loss:0.028973346217419104\n",
            "train loss:0.04862419082971832\n",
            "train loss:0.0065097674145790365\n",
            "train loss:0.09420534487043039\n",
            "train loss:0.042071052363740195\n",
            "train loss:0.0039010364885909586\n",
            "train loss:0.016817406273981574\n",
            "train loss:0.06500183148856992\n",
            "train loss:0.05167443423636776\n",
            "train loss:0.01709005024340253\n",
            "train loss:0.026777615439773878\n",
            "train loss:0.05636491075602421\n",
            "train loss:0.05282750242526963\n",
            "train loss:0.01366968268974819\n",
            "train loss:0.0031463739134660195\n",
            "train loss:0.03217294305923769\n",
            "=== epoch:147, train acc:0.988875, test acc:0.916 ===\n",
            "train loss:0.0087709016912778\n",
            "train loss:0.008974961877337026\n",
            "train loss:0.011662780475248052\n",
            "train loss:0.01432668006186007\n",
            "train loss:0.007696253177704127\n",
            "train loss:0.004625241491616062\n",
            "train loss:0.01199755335950427\n",
            "train loss:0.02425415070173745\n",
            "train loss:0.019035269923553425\n",
            "train loss:0.055506079770435084\n",
            "train loss:0.006924783731190986\n",
            "train loss:0.025959500823684527\n",
            "train loss:0.1210243621055595\n",
            "train loss:0.009247613311799912\n",
            "train loss:0.04322684727493855\n",
            "train loss:0.02513014701395009\n",
            "train loss:0.023166166135467226\n",
            "train loss:0.07952993552518502\n",
            "train loss:0.004500904483034284\n",
            "train loss:0.06423898052930917\n",
            "train loss:0.14061075266257317\n",
            "train loss:0.004372040636496202\n",
            "train loss:0.006515894722321946\n",
            "train loss:0.00801673610604929\n",
            "train loss:0.014250621333679037\n",
            "train loss:0.036302593345842474\n",
            "train loss:0.018955581517880002\n",
            "train loss:0.009991506790875047\n",
            "train loss:0.05461817194327008\n",
            "train loss:0.011313122000057838\n",
            "train loss:0.08745362648281876\n",
            "train loss:0.05758602057882453\n",
            "train loss:0.06675491415692703\n",
            "train loss:0.08624110500284372\n",
            "train loss:0.0591836717386909\n",
            "train loss:0.005424698004294332\n",
            "train loss:0.05450034982743276\n",
            "train loss:0.020429197984524864\n",
            "train loss:0.008779987998568392\n",
            "train loss:0.07128227652181099\n",
            "train loss:0.028677238023552466\n",
            "train loss:0.08051571972616432\n",
            "train loss:0.09167710349201778\n",
            "train loss:0.00636979972724536\n",
            "train loss:0.045728651769202114\n",
            "train loss:0.006787827892155578\n",
            "train loss:0.01645500270272018\n",
            "train loss:0.026873010113358648\n",
            "train loss:0.08145295635253444\n",
            "train loss:0.00755201697590196\n",
            "train loss:0.00896052075766457\n",
            "train loss:0.04838572404872609\n",
            "train loss:0.06362328080382919\n",
            "train loss:0.09628897048663522\n",
            "train loss:0.009716083381618047\n",
            "train loss:0.04853283548476733\n",
            "train loss:0.045522814346415925\n",
            "train loss:0.010420036753095824\n",
            "train loss:0.07028061280566296\n",
            "train loss:0.025334612297691423\n",
            "train loss:0.025179185985292535\n",
            "train loss:0.012162598256437665\n",
            "train loss:0.009879152246914097\n",
            "train loss:0.1478776425232126\n",
            "train loss:0.005906002922324362\n",
            "train loss:0.050579704850785084\n",
            "train loss:0.15452141345139897\n",
            "train loss:0.10459503321194835\n",
            "train loss:0.020211144655340553\n",
            "train loss:0.08033363024604785\n",
            "train loss:0.07401133041632749\n",
            "train loss:0.0054306813628495435\n",
            "train loss:0.03423752019902178\n",
            "train loss:0.07881511403461286\n",
            "train loss:0.018048966878248116\n",
            "train loss:0.013031282762648906\n",
            "train loss:0.013073948766728934\n",
            "train loss:0.006337324768041618\n",
            "train loss:0.03990386058807566\n",
            "train loss:0.010674521323711897\n",
            "=== epoch:148, train acc:0.988375, test acc:0.914 ===\n",
            "train loss:0.015142534956745843\n",
            "train loss:0.04391825735589706\n",
            "train loss:0.01678597686870099\n",
            "train loss:0.006572885693437324\n",
            "train loss:0.053973170130086824\n",
            "train loss:0.008233633370036061\n",
            "train loss:0.004727389200249366\n",
            "train loss:0.03678851338902831\n",
            "train loss:0.012791055087385688\n",
            "train loss:0.045793153948863935\n",
            "train loss:0.07149552121342052\n",
            "train loss:0.04812372935720052\n",
            "train loss:0.04964647417590693\n",
            "train loss:0.015674837383821874\n",
            "train loss:0.032156883190687637\n",
            "train loss:0.03749259189106189\n",
            "train loss:0.029050482464983526\n",
            "train loss:0.1646088250598917\n",
            "train loss:0.04977242546016016\n",
            "train loss:0.01700765042403897\n",
            "train loss:0.022850916028002263\n",
            "train loss:0.14661462367742506\n",
            "train loss:0.10304792041092163\n",
            "train loss:0.018155526070323287\n",
            "train loss:0.014443957472223823\n",
            "train loss:0.006333504341588529\n",
            "train loss:0.019548891680260274\n",
            "train loss:0.03505738138804774\n",
            "train loss:0.02188594721072721\n",
            "train loss:0.056999252578320864\n",
            "train loss:0.009734925097135055\n",
            "train loss:0.08618220823759154\n",
            "train loss:0.11060329503463699\n",
            "train loss:0.11638752407829923\n",
            "train loss:0.07563813913000302\n",
            "train loss:0.017963835809864527\n",
            "train loss:0.018333795343192474\n",
            "train loss:0.0202347624612216\n",
            "train loss:0.05933166738014881\n",
            "train loss:0.024182917653648932\n",
            "train loss:0.020964707777226414\n",
            "train loss:0.008329053712526341\n",
            "train loss:0.008502331991718957\n",
            "train loss:0.11091035816664745\n",
            "train loss:0.0710791314386418\n",
            "train loss:0.04481590334526815\n",
            "train loss:0.06352492373009344\n",
            "train loss:0.0588188463564175\n",
            "train loss:0.07248600716488526\n",
            "train loss:0.009781844813688722\n",
            "train loss:0.09854708421712927\n",
            "train loss:0.010316795253502868\n",
            "train loss:0.06633105440911204\n",
            "train loss:0.02475482252113816\n",
            "train loss:0.056283025749698484\n",
            "train loss:0.027265167853027506\n",
            "train loss:0.046217430691404375\n",
            "train loss:0.017214862873172712\n",
            "train loss:0.018637705485714216\n",
            "train loss:0.03782142757437051\n",
            "train loss:0.052680148571833535\n",
            "train loss:0.012315937972147084\n",
            "train loss:0.015450558998431205\n",
            "train loss:0.005227125739842714\n",
            "train loss:0.047194476509499456\n",
            "train loss:0.09193274079351654\n",
            "train loss:0.015978217414869555\n",
            "train loss:0.015688041166550027\n",
            "train loss:0.05645861554985096\n",
            "train loss:0.06520289642998216\n",
            "train loss:0.02675530006874509\n",
            "train loss:0.007131444863408556\n",
            "train loss:0.019426675484566617\n",
            "train loss:0.09101519967348787\n",
            "train loss:0.04374454554223691\n",
            "train loss:0.03872683007996602\n",
            "train loss:0.004838330871480836\n",
            "train loss:0.029253041533823618\n",
            "train loss:0.08587375875725303\n",
            "train loss:0.021359318486650163\n",
            "=== epoch:149, train acc:0.988375, test acc:0.9155 ===\n",
            "train loss:0.01355231646519631\n",
            "train loss:0.005598990476771307\n",
            "train loss:0.01324636487052393\n",
            "train loss:0.011074660965584446\n",
            "train loss:0.021956332219117122\n",
            "train loss:0.05633384475250529\n",
            "train loss:0.009752802939900275\n",
            "train loss:0.12253326446803042\n",
            "train loss:0.045207635608066704\n",
            "train loss:0.1334953697662783\n",
            "train loss:0.02028562968089043\n",
            "train loss:0.032871891958228\n",
            "train loss:0.04786100648537844\n",
            "train loss:0.011735616051654968\n",
            "train loss:0.03326238576698923\n",
            "train loss:0.10998487453278946\n",
            "train loss:0.03192906368045215\n",
            "train loss:0.050834103615892266\n",
            "train loss:0.013996008700364555\n",
            "train loss:0.01082029203629236\n",
            "train loss:0.0978081019875485\n",
            "train loss:0.01229859493037711\n",
            "train loss:0.00768806946420728\n",
            "train loss:0.10173491573080179\n",
            "train loss:0.07327070201929269\n",
            "train loss:0.004730766514250498\n",
            "train loss:0.006372535338078825\n",
            "train loss:0.05406404031305205\n",
            "train loss:0.026310262729264226\n",
            "train loss:0.031455824051579546\n",
            "train loss:0.015160343743005256\n",
            "train loss:0.06257587355941599\n",
            "train loss:0.08622775287224338\n",
            "train loss:0.06210443898590775\n",
            "train loss:0.010366730624115458\n",
            "train loss:0.043005110660744396\n",
            "train loss:0.06844982017791962\n",
            "train loss:0.054961296304231896\n",
            "train loss:0.08079060450876567\n",
            "train loss:0.038539493095400605\n",
            "train loss:0.04565098548393589\n",
            "train loss:0.04617733554458673\n",
            "train loss:0.016021819471003847\n",
            "train loss:0.048297462460834106\n",
            "train loss:0.012893842404724045\n",
            "train loss:0.09948510801498127\n",
            "train loss:0.04667544937374513\n",
            "train loss:0.03130867446565346\n",
            "train loss:0.026069697738180873\n",
            "train loss:0.005423257974094664\n",
            "train loss:0.009646471543476632\n",
            "train loss:0.05232173211428303\n",
            "train loss:0.16707420645548043\n",
            "train loss:0.033098402843738814\n",
            "train loss:0.05550162075389897\n",
            "train loss:0.03893039216836237\n",
            "train loss:0.028548331363104706\n",
            "train loss:0.05477733550644371\n",
            "train loss:0.01150440357662667\n",
            "train loss:0.03845600396221488\n",
            "train loss:0.09734736882294692\n",
            "train loss:0.10062203914105494\n",
            "train loss:0.06249059430536809\n",
            "train loss:0.05924974777433725\n",
            "train loss:0.020232868458539572\n",
            "train loss:0.15061500650887621\n",
            "train loss:0.010356482514597119\n",
            "train loss:0.017934122393727422\n",
            "train loss:0.04322235904398264\n",
            "train loss:0.036028629688742854\n",
            "train loss:0.02956729537538992\n",
            "train loss:0.01807028909341596\n",
            "train loss:0.0439433705995218\n",
            "train loss:0.011287423157339072\n",
            "train loss:0.012936098739223854\n",
            "train loss:0.027723252893077043\n",
            "train loss:0.02073679134588699\n",
            "train loss:0.020363314231523127\n",
            "train loss:0.012494856684984526\n",
            "train loss:0.005956948596664724\n",
            "=== epoch:150, train acc:0.989375, test acc:0.9155 ===\n",
            "train loss:0.0067254971454421145\n",
            "train loss:0.025107875522739954\n",
            "train loss:0.01701607653400335\n",
            "train loss:0.07044105628238005\n",
            "train loss:0.00562863157175805\n",
            "train loss:0.018470882398001223\n",
            "train loss:0.07049674713041505\n",
            "train loss:0.06559245319046286\n",
            "train loss:0.033810353164796025\n",
            "train loss:0.029499287722107147\n",
            "train loss:0.02725445996157279\n",
            "train loss:0.05030035922248474\n",
            "train loss:0.05997172207350013\n",
            "train loss:0.09070665144544365\n",
            "train loss:0.07457482134429949\n",
            "train loss:0.022169408536617804\n",
            "train loss:0.02244475451459014\n",
            "train loss:0.07338206836315467\n",
            "train loss:0.009004053787432695\n",
            "train loss:0.017976647705494668\n",
            "train loss:0.0557319499852865\n",
            "train loss:0.005271288120963593\n",
            "train loss:0.0065874316251668345\n",
            "train loss:0.008592007979132276\n",
            "train loss:0.015690031843379592\n",
            "train loss:0.02104438759466286\n",
            "train loss:0.008053516922445303\n",
            "train loss:0.040145601779548123\n",
            "train loss:0.060810073851948386\n",
            "train loss:0.03556631457580682\n",
            "train loss:0.09252874691212991\n",
            "train loss:0.031423156972132625\n",
            "train loss:0.007152034452665748\n",
            "train loss:0.013001606434724155\n",
            "train loss:0.01960275754807679\n",
            "train loss:0.022773535862628057\n",
            "train loss:0.02999940673210997\n",
            "train loss:0.007357402249902543\n",
            "train loss:0.005646184584338989\n",
            "train loss:0.06752002779112729\n",
            "train loss:0.04565649910723582\n",
            "train loss:0.15674960740389696\n",
            "train loss:0.1292708690400512\n",
            "train loss:0.042119780001504396\n",
            "train loss:0.012419017927121872\n",
            "train loss:0.009182570548095955\n",
            "train loss:0.009185924146420504\n",
            "train loss:0.011833642139986535\n",
            "train loss:0.08217948535822388\n",
            "train loss:0.007096906869611896\n",
            "train loss:0.016349118855580703\n",
            "train loss:0.03253603228209832\n",
            "train loss:0.10127008113062176\n",
            "train loss:0.03466410662556625\n",
            "train loss:0.059705833694674436\n",
            "train loss:0.04350659933009541\n",
            "train loss:0.02642382965883416\n",
            "train loss:0.05627751374352414\n",
            "train loss:0.06794384701046063\n",
            "train loss:0.10067537324869547\n",
            "train loss:0.10969550431565506\n",
            "train loss:0.049107029356612576\n",
            "train loss:0.03336181364859517\n",
            "train loss:0.044314247639662234\n",
            "train loss:0.043044498254542214\n",
            "train loss:0.02408085729824776\n",
            "train loss:0.011314002328533003\n",
            "train loss:0.02028082965269138\n",
            "train loss:0.011545827057931248\n",
            "train loss:0.05346801001430143\n",
            "train loss:0.04729206566387985\n",
            "train loss:0.02238182927569745\n",
            "train loss:0.008046733675881964\n",
            "train loss:0.0339813127105712\n",
            "train loss:0.021946488176335266\n",
            "train loss:0.050164155059752115\n",
            "train loss:0.016787559952967837\n",
            "train loss:0.13751706288385152\n",
            "train loss:0.0026716457629756612\n",
            "train loss:0.09462886475402459\n",
            "=== epoch:151, train acc:0.9885, test acc:0.916 ===\n",
            "train loss:0.03450813899464587\n",
            "train loss:0.011989437530951401\n",
            "train loss:0.03710121704073175\n",
            "train loss:0.02924493955227387\n",
            "train loss:0.015318948855028817\n",
            "train loss:0.017347791591579072\n",
            "train loss:0.006788238781906075\n",
            "train loss:0.1226910415135531\n",
            "train loss:0.02805856203069135\n",
            "train loss:0.04059137486918898\n",
            "train loss:0.011093610322165359\n",
            "train loss:0.1037754313772298\n",
            "train loss:0.06812710561960218\n",
            "train loss:0.011688507754702619\n",
            "train loss:0.008426070007274373\n",
            "train loss:0.04570430657012297\n",
            "train loss:0.016263521952886204\n",
            "train loss:0.05306079655397387\n",
            "train loss:0.01882025104331396\n",
            "train loss:0.027669745934436617\n",
            "train loss:0.03167448640648107\n",
            "train loss:0.011552036774764341\n",
            "train loss:0.028804403638325937\n",
            "train loss:0.01397254880447016\n",
            "train loss:0.0338455823603191\n",
            "train loss:0.03955297881163758\n",
            "train loss:0.012603826384306524\n",
            "train loss:0.010292785465342853\n",
            "train loss:0.011942015400303882\n",
            "train loss:0.0388846833024294\n",
            "train loss:0.004823394269566554\n",
            "train loss:0.02077003203727622\n",
            "train loss:0.0106959081869874\n",
            "train loss:0.049639432265648084\n",
            "train loss:0.005688594401324319\n",
            "train loss:0.10069846862585977\n",
            "train loss:0.08479636944864585\n",
            "train loss:0.009976536116669779\n",
            "train loss:0.02417562864191177\n",
            "train loss:0.0336230068796731\n",
            "train loss:0.004270728012705039\n",
            "train loss:0.03338848044512052\n",
            "train loss:0.05704108805513326\n",
            "train loss:0.04724861971537166\n",
            "train loss:0.06495289405005358\n",
            "train loss:0.014918323006242908\n",
            "train loss:0.012408214483539211\n",
            "train loss:0.04273662782331998\n",
            "train loss:0.035703451616121165\n",
            "train loss:0.03555206340593135\n",
            "train loss:0.06684243985394171\n",
            "train loss:0.09022449060124987\n",
            "train loss:0.027909496950367853\n",
            "train loss:0.007618264122406111\n",
            "train loss:0.004410934355853256\n",
            "train loss:0.039297496889851125\n",
            "train loss:0.020388757934513318\n",
            "train loss:0.07176244828722791\n",
            "train loss:0.012546113585623866\n",
            "train loss:0.06504144149898951\n",
            "train loss:0.012089875685708908\n",
            "train loss:0.06411333497789874\n",
            "train loss:0.06834243338678621\n",
            "train loss:0.0198129159328541\n",
            "train loss:0.004977294623815655\n",
            "train loss:0.01698462896844602\n",
            "train loss:0.006250052236920968\n",
            "train loss:0.05236838414087679\n",
            "train loss:0.020751474981963374\n",
            "train loss:0.0698981271065113\n",
            "train loss:0.07425820636824133\n",
            "train loss:0.011631531141027449\n",
            "train loss:0.05426483735765172\n",
            "train loss:0.06346069764487834\n",
            "train loss:0.020722722041915515\n",
            "train loss:0.016848182044011607\n",
            "train loss:0.014988608974110211\n",
            "train loss:0.025631044986843484\n",
            "train loss:0.09075636570899201\n",
            "train loss:0.04565307952767167\n",
            "=== epoch:152, train acc:0.989625, test acc:0.914 ===\n",
            "train loss:0.06360214086981891\n",
            "train loss:0.015361602375340242\n",
            "train loss:0.04305315888381063\n",
            "train loss:0.09953081820146181\n",
            "train loss:0.080675684428176\n",
            "train loss:0.013879721239860286\n",
            "train loss:0.009910996874492508\n",
            "train loss:0.0770666743524992\n",
            "train loss:0.07019770352806474\n",
            "train loss:0.03405038414996456\n",
            "train loss:0.043773149219778734\n",
            "train loss:0.01315211239355397\n",
            "train loss:0.05300499430632092\n",
            "train loss:0.02951294969385757\n",
            "train loss:0.039385968744552274\n",
            "train loss:0.030907564852006998\n",
            "train loss:0.007879361145194582\n",
            "train loss:0.084662172382086\n",
            "train loss:0.09367155298081452\n",
            "train loss:0.0039003222702861935\n",
            "train loss:0.03340546629320348\n",
            "train loss:0.005796960246086642\n",
            "train loss:0.013437504272664793\n",
            "train loss:0.028700670465376756\n",
            "train loss:0.008760656829016064\n",
            "train loss:0.04521990403695995\n",
            "train loss:0.027330966851487948\n",
            "train loss:0.009916385623047913\n",
            "train loss:0.0575850739085207\n",
            "train loss:0.0361149755532156\n",
            "train loss:0.11502204900205673\n",
            "train loss:0.01361893496394374\n",
            "train loss:0.0031405360538910377\n",
            "train loss:0.010702032628677138\n",
            "train loss:0.00981175570042529\n",
            "train loss:0.04053079008491472\n",
            "train loss:0.03504268135262352\n",
            "train loss:0.033374660151377404\n",
            "train loss:0.08726556960127561\n",
            "train loss:0.042857138868274997\n",
            "train loss:0.009853742115627942\n",
            "train loss:0.0229992084315247\n",
            "train loss:0.024352316399423625\n",
            "train loss:0.03381611168508539\n",
            "train loss:0.021737523628623898\n",
            "train loss:0.011951619650278189\n",
            "train loss:0.021167250449213387\n",
            "train loss:0.010272984215460579\n",
            "train loss:0.058650129205098205\n",
            "train loss:0.07561647026333594\n",
            "train loss:0.02522720991628537\n",
            "train loss:0.01221344349994617\n",
            "train loss:0.06778699380056229\n",
            "train loss:0.00841046254660828\n",
            "train loss:0.004267640171765589\n",
            "train loss:0.012737764728599449\n",
            "train loss:0.032051091293044724\n",
            "train loss:0.04915080476992444\n",
            "train loss:0.014014004441984935\n",
            "train loss:0.029725650678447728\n",
            "train loss:0.05652979078066899\n",
            "train loss:0.01457007342483149\n",
            "train loss:0.05040479282887901\n",
            "train loss:0.04376266765137023\n",
            "train loss:0.043430302964599245\n",
            "train loss:0.03980400654297871\n",
            "train loss:0.00747875736928746\n",
            "train loss:0.010374812319893987\n",
            "train loss:0.08408290591757779\n",
            "train loss:0.016596129917764425\n",
            "train loss:0.024610688094802925\n",
            "train loss:0.030107992625736966\n",
            "train loss:0.0033513410539406474\n",
            "train loss:0.034496787942074086\n",
            "train loss:0.012151143490424086\n",
            "train loss:0.00907643516801676\n",
            "train loss:0.05062044487035768\n",
            "train loss:0.05696968492483156\n",
            "train loss:0.007630032883766851\n",
            "train loss:0.05117923898082191\n",
            "=== epoch:153, train acc:0.98975, test acc:0.9195 ===\n",
            "train loss:0.006550145579991522\n",
            "train loss:0.01636889001271306\n",
            "train loss:0.1405554028684701\n",
            "train loss:0.07827313049111026\n",
            "train loss:0.03551824049608065\n",
            "train loss:0.050579088199649264\n",
            "train loss:0.012089037631818449\n",
            "train loss:0.0157541184184369\n",
            "train loss:0.01318583801015023\n",
            "train loss:0.06021097664258059\n",
            "train loss:0.02412040407205172\n",
            "train loss:0.010157793604433727\n",
            "train loss:0.04155585764581483\n",
            "train loss:0.021444630120835508\n",
            "train loss:0.019977464190688073\n",
            "train loss:0.0410483800407214\n",
            "train loss:0.04454020481688607\n",
            "train loss:0.010217905043515147\n",
            "train loss:0.012179925889364195\n",
            "train loss:0.041340498771831255\n",
            "train loss:0.0053464933394322955\n",
            "train loss:0.028666722023193324\n",
            "train loss:0.017485919371180478\n",
            "train loss:0.0076431929470642665\n",
            "train loss:0.006129861850312834\n",
            "train loss:0.04261354477191001\n",
            "train loss:0.12992074450220112\n",
            "train loss:0.04453314018681181\n",
            "train loss:0.013894001456049253\n",
            "train loss:0.008092316850102812\n",
            "train loss:0.07216431586843776\n",
            "train loss:0.05416272476784276\n",
            "train loss:0.034223704078846555\n",
            "train loss:0.0260385316065772\n",
            "train loss:0.09353824619605429\n",
            "train loss:0.026952367767125337\n",
            "train loss:0.07397471573195674\n",
            "train loss:0.024107805531876608\n",
            "train loss:0.0681158031086057\n",
            "train loss:0.0967775100128847\n",
            "train loss:0.0022643418630345085\n",
            "train loss:0.03335816560502408\n",
            "train loss:0.05286160955149571\n",
            "train loss:0.07187904820928534\n",
            "train loss:0.0077080384145373606\n",
            "train loss:0.01180925373434464\n",
            "train loss:0.11318419896564234\n",
            "train loss:0.02837361919652786\n",
            "train loss:0.026571334241699357\n",
            "train loss:0.05611496645744971\n",
            "train loss:0.011131420872834084\n",
            "train loss:0.026883697310888147\n",
            "train loss:0.05632162755244238\n",
            "train loss:0.021394360087165207\n",
            "train loss:0.015640223024818257\n",
            "train loss:0.027696994806579913\n",
            "train loss:0.005461921731346499\n",
            "train loss:0.16661919645224912\n",
            "train loss:0.03620281406660607\n",
            "train loss:0.06015157087407176\n",
            "train loss:0.06463485327228963\n",
            "train loss:0.011194042903818495\n",
            "train loss:0.01589808010647744\n",
            "train loss:0.06754022128789042\n",
            "train loss:0.02032922223937066\n",
            "train loss:0.013179553415941424\n",
            "train loss:0.032940052869748326\n",
            "train loss:0.10220820566544425\n",
            "train loss:0.036499381923566616\n",
            "train loss:0.05439767865428493\n",
            "train loss:0.04053775057156827\n",
            "train loss:0.04852068768354348\n",
            "train loss:0.014047217482582667\n",
            "train loss:0.00592887386507427\n",
            "train loss:0.02584001210117239\n",
            "train loss:0.02272972610036891\n",
            "train loss:0.09412491681019508\n",
            "train loss:0.022537398171362646\n",
            "train loss:0.03344801939505735\n",
            "train loss:0.017299995525701398\n",
            "=== epoch:154, train acc:0.989375, test acc:0.918 ===\n",
            "train loss:0.017996368466093805\n",
            "train loss:0.06234395214654694\n",
            "train loss:0.014147334326135989\n",
            "train loss:0.016535181794115224\n",
            "train loss:0.027313277659445894\n",
            "train loss:0.06907315164123455\n",
            "train loss:0.009121374002810191\n",
            "train loss:0.06971077451261408\n",
            "train loss:0.032676482061843586\n",
            "train loss:0.09285482402642281\n",
            "train loss:0.005922896668200029\n",
            "train loss:0.027687900192985823\n",
            "train loss:0.03066143632404444\n",
            "train loss:0.04093426856577386\n",
            "train loss:0.024416172780730282\n",
            "train loss:0.03947575802422206\n",
            "train loss:0.010527278863314058\n",
            "train loss:0.047151679391279185\n",
            "train loss:0.06338458448431167\n",
            "train loss:0.011758149488050233\n",
            "train loss:0.02042627371637976\n",
            "train loss:0.05191782008284996\n",
            "train loss:0.011241985841162991\n",
            "train loss:0.00537393284995198\n",
            "train loss:0.013309520564522715\n",
            "train loss:0.008112088455178084\n",
            "train loss:0.04734379138308529\n",
            "train loss:0.03191225524977138\n",
            "train loss:0.005813198876062337\n",
            "train loss:0.04025399397220583\n",
            "train loss:0.017775409257228172\n",
            "train loss:0.011283714660267057\n",
            "train loss:0.008299783912650596\n",
            "train loss:0.019931274195072863\n",
            "train loss:0.05706449249339966\n",
            "train loss:0.011251803844057249\n",
            "train loss:0.01841989362735778\n",
            "train loss:0.004930301719644324\n",
            "train loss:0.07812890897719679\n",
            "train loss:0.007005795600745271\n",
            "train loss:0.011299443005634915\n",
            "train loss:0.1348890521367897\n",
            "train loss:0.05998627354883366\n",
            "train loss:0.03178118005034102\n",
            "train loss:0.06412628204381322\n",
            "train loss:0.030243994620096814\n",
            "train loss:0.014121227954525974\n",
            "train loss:0.013608077915975592\n",
            "train loss:0.006301581424464338\n",
            "train loss:0.018161134404954486\n",
            "train loss:0.005812766816293735\n",
            "train loss:0.010289078751591085\n",
            "train loss:0.011656752271809455\n",
            "train loss:0.006604487277158745\n",
            "train loss:0.015499330938102785\n",
            "train loss:0.0031223833779949633\n",
            "train loss:0.012546000053032157\n",
            "train loss:0.03725775053564285\n",
            "train loss:0.0064529290276848575\n",
            "train loss:0.07295404485706711\n",
            "train loss:0.028535376858673828\n",
            "train loss:0.04591511865006793\n",
            "train loss:0.053191414917373024\n",
            "train loss:0.017308062864181496\n",
            "train loss:0.04679312445484442\n",
            "train loss:0.00523650522730291\n",
            "train loss:0.013157102561990423\n",
            "train loss:0.019851122356881815\n",
            "train loss:0.017833634422696094\n",
            "train loss:0.050216830612025955\n",
            "train loss:0.014411629297083603\n",
            "train loss:0.0293538473928162\n",
            "train loss:0.007871692162912584\n",
            "train loss:0.14732200668240059\n",
            "train loss:0.02020270809137882\n",
            "train loss:0.06375443106874239\n",
            "train loss:0.04534712047856676\n",
            "train loss:0.04124466834038487\n",
            "train loss:0.03384067213023227\n",
            "train loss:0.0027525445032907147\n",
            "=== epoch:155, train acc:0.990125, test acc:0.915 ===\n",
            "train loss:0.017258242950831865\n",
            "train loss:0.027273574954026204\n",
            "train loss:0.0075065836401437295\n",
            "train loss:0.0372766249192966\n",
            "train loss:0.008972424095660186\n",
            "train loss:0.02026797008496335\n",
            "train loss:0.04230318772591147\n",
            "train loss:0.05589237668262488\n",
            "train loss:0.03666881421984538\n",
            "train loss:0.03213046100409745\n",
            "train loss:0.06515602977257096\n",
            "train loss:0.008710477816373215\n",
            "train loss:0.028470251107804826\n",
            "train loss:0.026050450587134604\n",
            "train loss:0.008800462147340659\n",
            "train loss:0.010881739663261149\n",
            "train loss:0.0034249871298777414\n",
            "train loss:0.0691050027326704\n",
            "train loss:0.006981436928379762\n",
            "train loss:0.007147321820596044\n",
            "train loss:0.04910079461879028\n",
            "train loss:0.014297595041030165\n",
            "train loss:0.1225856428365688\n",
            "train loss:0.03692781558921377\n",
            "train loss:0.05449895695067415\n",
            "train loss:0.012377840187930984\n",
            "train loss:0.06021387773084186\n",
            "train loss:0.03769257307276364\n",
            "train loss:0.011732799407182606\n",
            "train loss:0.022292037658731684\n",
            "train loss:0.01567953903417032\n",
            "train loss:0.012141619526155903\n",
            "train loss:0.009832769086974362\n",
            "train loss:0.01055017173882112\n",
            "train loss:0.0048595676385500605\n",
            "train loss:0.0037734422251362904\n",
            "train loss:0.029821605432821738\n",
            "train loss:0.048617181899727475\n",
            "train loss:0.006106447794263072\n",
            "train loss:0.00613476019216277\n",
            "train loss:0.017090860784005436\n",
            "train loss:0.007299703680640989\n",
            "train loss:0.06856372930885705\n",
            "train loss:0.015097047905200594\n",
            "train loss:0.06907544393412748\n",
            "train loss:0.0332036280102576\n",
            "train loss:0.03907973817379569\n",
            "train loss:0.06454807004667874\n",
            "train loss:0.043921705036670075\n",
            "train loss:0.06402410820377398\n",
            "train loss:0.015926266358136276\n",
            "train loss:0.00760589039299326\n",
            "train loss:0.015569373222836722\n",
            "train loss:0.10428959124074176\n",
            "train loss:0.05680132395876313\n",
            "train loss:0.0458024067545306\n",
            "train loss:0.03455937817730963\n",
            "train loss:0.025913214169252758\n",
            "train loss:0.009242554940725324\n",
            "train loss:0.008803858006446137\n",
            "train loss:0.09671729646897768\n",
            "train loss:0.03148679658573496\n",
            "train loss:0.03459676575140156\n",
            "train loss:0.043182366833409355\n",
            "train loss:0.0530885589137462\n",
            "train loss:0.027381990011240767\n",
            "train loss:0.08811157507479106\n",
            "train loss:0.039735030347648845\n",
            "train loss:0.0832981289064381\n",
            "train loss:0.014040368697297401\n",
            "train loss:0.006517140815224473\n",
            "train loss:0.009487376257874823\n",
            "train loss:0.006183949530225354\n",
            "train loss:0.026471257512513387\n",
            "train loss:0.06444147012011604\n",
            "train loss:0.07232496070903612\n",
            "train loss:0.03360317275460953\n",
            "train loss:0.11833982601419903\n",
            "train loss:0.015930980546723605\n",
            "train loss:0.03196530123097665\n",
            "=== epoch:156, train acc:0.989375, test acc:0.919 ===\n",
            "train loss:0.009267104403752485\n",
            "train loss:0.006123469265172901\n",
            "train loss:0.043140908922738024\n",
            "train loss:0.007303526177503004\n",
            "train loss:0.23199216804257417\n",
            "train loss:0.006129026028270487\n",
            "train loss:0.07852726634885134\n",
            "train loss:0.03514512292555379\n",
            "train loss:0.019128813905053322\n",
            "train loss:0.012755098606070912\n",
            "train loss:0.015827553432242927\n",
            "train loss:0.013731055060980893\n",
            "train loss:0.015893338869471488\n",
            "train loss:0.011413917334764642\n",
            "train loss:0.0247607488645465\n",
            "train loss:0.08852577608548923\n",
            "train loss:0.0077187842016813194\n",
            "train loss:0.0024724747135787474\n",
            "train loss:0.029794375653660832\n",
            "train loss:0.05501713975699578\n",
            "train loss:0.010828346784501026\n",
            "train loss:0.0168720294485829\n",
            "train loss:0.023675116558243885\n",
            "train loss:0.01977499789538869\n",
            "train loss:0.07609195149013707\n",
            "train loss:0.04995835721882687\n",
            "train loss:0.06602817796045786\n",
            "train loss:0.028396366650421338\n",
            "train loss:0.007761660131586844\n",
            "train loss:0.013442109893159115\n",
            "train loss:0.028311928692990092\n",
            "train loss:0.04597821445488929\n",
            "train loss:0.08518704748426209\n",
            "train loss:0.00889255954854541\n",
            "train loss:0.017078858482447095\n",
            "train loss:0.008760299932189212\n",
            "train loss:0.0746075728692765\n",
            "train loss:0.0671136557726896\n",
            "train loss:0.0688043607110781\n",
            "train loss:0.020512760098306413\n",
            "train loss:0.01989508585161366\n",
            "train loss:0.007188673670688797\n",
            "train loss:0.04667165727344984\n",
            "train loss:0.0423108575318481\n",
            "train loss:0.022463035755585937\n",
            "train loss:0.017777345300347094\n",
            "train loss:0.011513405140481197\n",
            "train loss:0.012677152817763122\n",
            "train loss:0.018115203191965426\n",
            "train loss:0.008656111822946845\n",
            "train loss:0.1336370736042164\n",
            "train loss:0.06997204226932005\n",
            "train loss:0.04232653351430975\n",
            "train loss:0.02529134306044764\n",
            "train loss:0.04419758713125898\n",
            "train loss:0.010121110887726346\n",
            "train loss:0.008297870078738284\n",
            "train loss:0.06590954676092524\n",
            "train loss:0.0060629248036737715\n",
            "train loss:0.03558831819583633\n",
            "train loss:0.06123442153723446\n",
            "train loss:0.0063561040583680825\n",
            "train loss:0.008888381216182921\n",
            "train loss:0.04871851704919278\n",
            "train loss:0.050532884448911554\n",
            "train loss:0.03532614118250405\n",
            "train loss:0.006797511595074213\n",
            "train loss:0.01959451490906138\n",
            "train loss:0.006886534650998457\n",
            "train loss:0.03910211559895477\n",
            "train loss:0.0652437250603128\n",
            "train loss:0.063873901143958\n",
            "train loss:0.10316617288039368\n",
            "train loss:0.09640775501152918\n",
            "train loss:0.006947696221669548\n",
            "train loss:0.009187251479583193\n",
            "train loss:0.02366594156579589\n",
            "train loss:0.017828270950550246\n",
            "train loss:0.047789941671894924\n",
            "train loss:0.0154595914758285\n",
            "=== epoch:157, train acc:0.990125, test acc:0.9185 ===\n",
            "train loss:0.0645461814641604\n",
            "train loss:0.041993952881266325\n",
            "train loss:0.0249240712972182\n",
            "train loss:0.03128376878119146\n",
            "train loss:0.06462281063914554\n",
            "train loss:0.03366915133846055\n",
            "train loss:0.009065882735689411\n",
            "train loss:0.09923916919358494\n",
            "train loss:0.016296638499786596\n",
            "train loss:0.06262333159364562\n",
            "train loss:0.05635935312365497\n",
            "train loss:0.0065803465922572735\n",
            "train loss:0.005470230249091978\n",
            "train loss:0.06208525122212155\n",
            "train loss:0.018680088237433303\n",
            "train loss:0.07726187879974207\n",
            "train loss:0.009507501760281893\n",
            "train loss:0.009714571318277722\n",
            "train loss:0.018881828067939345\n",
            "train loss:0.06267554960227448\n",
            "train loss:0.09242054550897355\n",
            "train loss:0.05004850410302583\n",
            "train loss:0.14508175864799994\n",
            "train loss:0.018763529679678325\n",
            "train loss:0.011352671085425673\n",
            "train loss:0.010742388842538402\n",
            "train loss:0.01625321679667203\n",
            "train loss:0.02374140114881113\n",
            "train loss:0.003387983071339429\n",
            "train loss:0.019141109427539445\n",
            "train loss:0.05761223529612224\n",
            "train loss:0.014137691007957276\n",
            "train loss:0.008377540292395139\n",
            "train loss:0.054127053977053106\n",
            "train loss:0.007608861688011236\n",
            "train loss:0.03363063836878994\n",
            "train loss:0.009100876398855482\n",
            "train loss:0.008785845918476937\n",
            "train loss:0.05186961663295144\n",
            "train loss:0.036271287630470515\n",
            "train loss:0.041845435500742795\n",
            "train loss:0.011968113760813519\n",
            "train loss:0.034094096528776395\n",
            "train loss:0.039359270999932264\n",
            "train loss:0.010538980211012842\n",
            "train loss:0.055355018072531854\n",
            "train loss:0.005303001021189841\n",
            "train loss:0.018050492972732247\n",
            "train loss:0.037652729388932236\n",
            "train loss:0.006869282327752368\n",
            "train loss:0.02639251716200757\n",
            "train loss:0.006298829733746836\n",
            "train loss:0.02900582576153552\n",
            "train loss:0.01246465051341793\n",
            "train loss:0.003938706639846175\n",
            "train loss:0.06046407546363598\n",
            "train loss:0.028981095368067082\n",
            "train loss:0.05077638220035265\n",
            "train loss:0.05747610516414953\n",
            "train loss:0.004988484319960077\n",
            "train loss:0.01609209835299509\n",
            "train loss:0.09103072437731191\n",
            "train loss:0.008024819215297985\n",
            "train loss:0.036745913576219036\n",
            "train loss:0.017824076282194738\n",
            "train loss:0.03656480841178232\n",
            "train loss:0.006361138983332737\n",
            "train loss:0.061991110760124465\n",
            "train loss:0.09092221663348042\n",
            "train loss:0.014054881987524477\n",
            "train loss:0.07278386900128699\n",
            "train loss:0.09260247797966406\n",
            "train loss:0.010250512100461233\n",
            "train loss:0.038839599038078834\n",
            "train loss:0.012674928901841447\n",
            "train loss:0.05611494091244498\n",
            "train loss:0.032811547243094846\n",
            "train loss:0.076703717312074\n",
            "train loss:0.007849644550906918\n",
            "train loss:0.042770266285944915\n",
            "=== epoch:158, train acc:0.990875, test acc:0.914 ===\n",
            "train loss:0.07338330290300495\n",
            "train loss:0.008726498561776106\n",
            "train loss:0.006902094747191165\n",
            "train loss:0.01318612418594332\n",
            "train loss:0.007774974623635068\n",
            "train loss:0.018193811284493336\n",
            "train loss:0.007783687218360346\n",
            "train loss:0.00594010173208379\n",
            "train loss:0.03587870928957485\n",
            "train loss:0.023741814755581774\n",
            "train loss:0.009254802126271432\n",
            "train loss:0.03849538791206802\n",
            "train loss:0.08193788317296744\n",
            "train loss:0.02744309206011049\n",
            "train loss:0.018087749279387555\n",
            "train loss:0.008881933775442724\n",
            "train loss:0.054155410592783844\n",
            "train loss:0.045849907890831876\n",
            "train loss:0.08067585316780061\n",
            "train loss:0.050686146339567036\n",
            "train loss:0.033158651860213884\n",
            "train loss:0.008554941327294577\n",
            "train loss:0.005035402656388783\n",
            "train loss:0.05540527342457789\n",
            "train loss:0.09505097374984611\n",
            "train loss:0.01840292905237578\n",
            "train loss:0.010209139261242261\n",
            "train loss:0.040726630166838224\n",
            "train loss:0.019939980265844467\n",
            "train loss:0.07578410108004727\n",
            "train loss:0.09125442582385643\n",
            "train loss:0.007590810521415395\n",
            "train loss:0.0240524661529097\n",
            "train loss:0.02823613633047563\n",
            "train loss:0.0516630840583829\n",
            "train loss:0.011112116941844264\n",
            "train loss:0.025232131834757456\n",
            "train loss:0.05699217317868968\n",
            "train loss:0.048764221949569954\n",
            "train loss:0.024387470527884744\n",
            "train loss:0.03252603465011755\n",
            "train loss:0.019820331012465213\n",
            "train loss:0.02034617911570003\n",
            "train loss:0.05498576416355853\n",
            "train loss:0.13992459679231437\n",
            "train loss:0.0075554616747955695\n",
            "train loss:0.004741398762215787\n",
            "train loss:0.01576270854016884\n",
            "train loss:0.01569752070378398\n",
            "train loss:0.03419955939495483\n",
            "train loss:0.06550316080274157\n",
            "train loss:0.04038835586137617\n",
            "train loss:0.008919065942526167\n",
            "train loss:0.027803452685520546\n",
            "train loss:0.011267878408143828\n",
            "train loss:0.008797736635738428\n",
            "train loss:0.056148092995857705\n",
            "train loss:0.02145510575036523\n",
            "train loss:0.05519280507251573\n",
            "train loss:0.024004870011005798\n",
            "train loss:0.06045721050700287\n",
            "train loss:0.05223723051937881\n",
            "train loss:0.005807296514565936\n",
            "train loss:0.0532902698296944\n",
            "train loss:0.008652486120966085\n",
            "train loss:0.016520191285584955\n",
            "train loss:0.09300851098750584\n",
            "train loss:0.005882540229012083\n",
            "train loss:0.009870907293512575\n",
            "train loss:0.04193360054077075\n",
            "train loss:0.010324834844689707\n",
            "train loss:0.018070947654656694\n",
            "train loss:0.011350146895313652\n",
            "train loss:0.012360605745281195\n",
            "train loss:0.029831116649807532\n",
            "train loss:0.09985664092635237\n",
            "train loss:0.03664953096170964\n",
            "train loss:0.00843243140200018\n",
            "train loss:0.030098504325337654\n",
            "train loss:0.007498198440859201\n",
            "=== epoch:159, train acc:0.99125, test acc:0.9185 ===\n",
            "train loss:0.02232509507687211\n",
            "train loss:0.027181552689592766\n",
            "train loss:0.0413501465682831\n",
            "train loss:0.047440555871279926\n",
            "train loss:0.03309257331224467\n",
            "train loss:0.0035961265524621917\n",
            "train loss:0.01995203477881638\n",
            "train loss:0.024512634939243525\n",
            "train loss:0.04002557399212854\n",
            "train loss:0.0045270402709906\n",
            "train loss:0.00419207081132287\n",
            "train loss:0.011928483629074126\n",
            "train loss:0.006938817075844147\n",
            "train loss:0.008563193814784499\n",
            "train loss:0.01027722929754911\n",
            "train loss:0.00417856603273509\n",
            "train loss:0.09912315628265889\n",
            "train loss:0.0449997961410681\n",
            "train loss:0.07524133423499664\n",
            "train loss:0.00921302520876896\n",
            "train loss:0.06836884937713081\n",
            "train loss:0.044824937301854044\n",
            "train loss:0.07685577909298028\n",
            "train loss:0.008940779250058851\n",
            "train loss:0.058690203035913\n",
            "train loss:0.02508384601937316\n",
            "train loss:0.10040352568421888\n",
            "train loss:0.07465324445558003\n",
            "train loss:0.07915966571273833\n",
            "train loss:0.025965901851276695\n",
            "train loss:0.020229384435411148\n",
            "train loss:0.015546920655481761\n",
            "train loss:0.08012092482832926\n",
            "train loss:0.004786182815839909\n",
            "train loss:0.006341783585540942\n",
            "train loss:0.04197715294145936\n",
            "train loss:0.09610368241943165\n",
            "train loss:0.016051152783677445\n",
            "train loss:0.006530124753709\n",
            "train loss:0.03317145869034752\n",
            "train loss:0.02161483626362941\n",
            "train loss:0.09224009370057568\n",
            "train loss:0.04041453001782616\n",
            "train loss:0.008688067272184987\n",
            "train loss:0.005339180107376935\n",
            "train loss:0.012587586996474704\n",
            "train loss:0.05868971238895264\n",
            "train loss:0.1221900184770718\n",
            "train loss:0.006636637562846579\n",
            "train loss:0.0035012451456614187\n",
            "train loss:0.004437960373875739\n",
            "train loss:0.008513416619399795\n",
            "train loss:0.06404548004516002\n",
            "train loss:0.011114614712074242\n",
            "train loss:0.11673123279844287\n",
            "train loss:0.02535744574383691\n",
            "train loss:0.03536460378378653\n",
            "train loss:0.04374167651709062\n",
            "train loss:0.02470150222144597\n",
            "train loss:0.06540271548376145\n",
            "train loss:0.02588169934406813\n",
            "train loss:0.03627668560304438\n",
            "train loss:0.03942629817550691\n",
            "train loss:0.0778090598246042\n",
            "train loss:0.013031041772610642\n",
            "train loss:0.04506305926161441\n",
            "train loss:0.05101434431863186\n",
            "train loss:0.00798157225237439\n",
            "train loss:0.04174646187070115\n",
            "train loss:0.00662429464065291\n",
            "train loss:0.007679733062715744\n",
            "train loss:0.04910658903474497\n",
            "train loss:0.014027203628925877\n",
            "train loss:0.0445972803504656\n",
            "train loss:0.020200725036753183\n",
            "train loss:0.024136035329181182\n",
            "train loss:0.016627840590192997\n",
            "train loss:0.06855132042323396\n",
            "train loss:0.0053312037796876275\n",
            "train loss:0.06342826362121005\n",
            "=== epoch:160, train acc:0.9905, test acc:0.915 ===\n",
            "train loss:0.006964248159442252\n",
            "train loss:0.013528101366931816\n",
            "train loss:0.009117227095692954\n",
            "train loss:0.007372701667201081\n",
            "train loss:0.037604321857138\n",
            "train loss:0.020385123073026912\n",
            "train loss:0.025278703084906807\n",
            "train loss:0.009314936629998471\n",
            "train loss:0.005274151597771626\n",
            "train loss:0.006183633093496918\n",
            "train loss:0.036646061804229996\n",
            "train loss:0.027408843967804245\n",
            "train loss:0.004634990029453206\n",
            "train loss:0.05745920077379027\n",
            "train loss:0.010443239087980373\n",
            "train loss:0.011683028526736456\n",
            "train loss:0.06878966275847699\n",
            "train loss:0.01566224381507067\n",
            "train loss:0.044516324381376086\n",
            "train loss:0.031622091178746176\n",
            "train loss:0.02635324587957236\n",
            "train loss:0.054736155933151086\n",
            "train loss:0.011339623843698615\n",
            "train loss:0.0030478408078315877\n",
            "train loss:0.01745245610974949\n",
            "train loss:0.006213191060393134\n",
            "train loss:0.0049364547696032655\n",
            "train loss:0.04559351209551957\n",
            "train loss:0.11005819197015348\n",
            "train loss:0.018639303517936613\n",
            "train loss:0.016154018435520345\n",
            "train loss:0.00846757947770687\n",
            "train loss:0.016259426153637648\n",
            "train loss:0.007192909020726469\n",
            "train loss:0.047799143619084584\n",
            "train loss:0.10506285886955627\n",
            "train loss:0.05986906968007564\n",
            "train loss:0.007987553866718997\n",
            "train loss:0.05879191353370825\n",
            "train loss:0.007014512149000452\n",
            "train loss:0.0450880659634846\n",
            "train loss:0.026757700685423554\n",
            "train loss:0.08490148877949964\n",
            "train loss:0.007072319571878051\n",
            "train loss:0.040616590555443956\n",
            "train loss:0.13770184221860934\n",
            "train loss:0.009682212624221637\n",
            "train loss:0.05728874259037399\n",
            "train loss:0.0897809714633261\n",
            "train loss:0.026893624617235865\n",
            "train loss:0.02374968156003467\n",
            "train loss:0.032411213743758716\n",
            "train loss:0.013339410867682266\n",
            "train loss:0.009434705835576495\n",
            "train loss:0.08260352701458704\n",
            "train loss:0.05346462652256049\n",
            "train loss:0.011193279342685497\n",
            "train loss:0.047960111259738133\n",
            "train loss:0.00675357368295233\n",
            "train loss:0.009437988939928123\n",
            "train loss:0.03368765753250617\n",
            "train loss:0.0376012325175993\n",
            "train loss:0.03715643775901349\n",
            "train loss:0.02767763508902272\n",
            "train loss:0.08928083079659763\n",
            "train loss:0.08820145203225122\n",
            "train loss:0.03090824216455228\n",
            "train loss:0.054730671648920586\n",
            "train loss:0.023469823750068328\n",
            "train loss:0.03474025672585486\n",
            "train loss:0.07074082858150477\n",
            "train loss:0.010321240718660071\n",
            "train loss:0.02325409390975886\n",
            "train loss:0.01659297004249894\n",
            "train loss:0.007536690543643532\n",
            "train loss:0.009762641455022154\n",
            "train loss:0.02241151518443378\n",
            "train loss:0.01888941476981145\n",
            "train loss:0.033498032225649704\n",
            "train loss:0.00883848576204849\n",
            "=== epoch:161, train acc:0.990875, test acc:0.919 ===\n",
            "train loss:0.012385176487270394\n",
            "train loss:0.0580426492871473\n",
            "train loss:0.016912536089420074\n",
            "train loss:0.017299335505759426\n",
            "train loss:0.08184896929061225\n",
            "train loss:0.00869908876636276\n",
            "train loss:0.008336173615815295\n",
            "train loss:0.024302533309034865\n",
            "train loss:0.007468805971886375\n",
            "train loss:0.011794953560439891\n",
            "train loss:0.02407948116658679\n",
            "train loss:0.017245313979457447\n",
            "train loss:0.006675038749556752\n",
            "train loss:0.052385370674600244\n",
            "train loss:0.1560879615008983\n",
            "train loss:0.0034767399758166323\n",
            "train loss:0.008123991551488557\n",
            "train loss:0.06792173220945896\n",
            "train loss:0.07952847608850655\n",
            "train loss:0.032879074535561735\n",
            "train loss:0.006164732461593362\n",
            "train loss:0.004410350929194378\n",
            "train loss:0.040697981612667834\n",
            "train loss:0.008991415334102398\n",
            "train loss:0.06239562357458261\n",
            "train loss:0.002845369836849598\n",
            "train loss:0.004489858453370938\n",
            "train loss:0.012159777290103011\n",
            "train loss:0.03661050660327424\n",
            "train loss:0.09714424657971284\n",
            "train loss:0.06491370993712804\n",
            "train loss:0.015769557229463146\n",
            "train loss:0.03502731609037112\n",
            "train loss:0.0699041698402134\n",
            "train loss:0.0539147353547219\n",
            "train loss:0.01560867338129153\n",
            "train loss:0.028648032290097204\n",
            "train loss:0.07706709790957862\n",
            "train loss:0.010005627791954548\n",
            "train loss:0.008010956161807225\n",
            "train loss:0.12861182700256454\n",
            "train loss:0.022972210031332572\n",
            "train loss:0.005947438598182768\n",
            "train loss:0.04103695994748012\n",
            "train loss:0.011015547878165664\n",
            "train loss:0.011750865842783958\n",
            "train loss:0.06687885882827328\n",
            "train loss:0.04191104999594149\n",
            "train loss:0.03970803835927708\n",
            "train loss:0.030298911114897534\n",
            "train loss:0.01954943053749683\n",
            "train loss:0.006871525540377263\n",
            "train loss:0.009072089472137418\n",
            "train loss:0.019260212723210157\n",
            "train loss:0.040226747591628795\n",
            "train loss:0.0540761936432531\n",
            "train loss:0.024884045662466193\n",
            "train loss:0.004074005271580011\n",
            "train loss:0.004574814491323608\n",
            "train loss:0.0033808640037369135\n",
            "train loss:0.0035721967823136697\n",
            "train loss:0.001258269424931207\n",
            "train loss:0.03548108223024638\n",
            "train loss:0.06332677918739572\n",
            "train loss:0.01690024995675771\n",
            "train loss:0.0056254005464816004\n",
            "train loss:0.009758036152815012\n",
            "train loss:0.014674923941259292\n",
            "train loss:0.008963499933272959\n",
            "train loss:0.01772492776057035\n",
            "train loss:0.009255533501138424\n",
            "train loss:0.007342482460361852\n",
            "train loss:0.018763720559348048\n",
            "train loss:0.03485732479036024\n",
            "train loss:0.0035271143569108056\n",
            "train loss:0.020914156823097284\n",
            "train loss:0.019813272969260304\n",
            "train loss:0.007270260891975256\n",
            "train loss:0.01654428481087757\n",
            "train loss:0.009455087238153177\n",
            "=== epoch:162, train acc:0.991625, test acc:0.919 ===\n",
            "train loss:0.012648512297511696\n",
            "train loss:0.006759026573234706\n",
            "train loss:0.03453267087129802\n",
            "train loss:0.00548260082327958\n",
            "train loss:0.0036190810486953833\n",
            "train loss:0.054829153438410175\n",
            "train loss:0.003704392912677762\n",
            "train loss:0.008408116588012658\n",
            "train loss:0.007247665856277495\n",
            "train loss:0.009382703991589823\n",
            "train loss:0.0796519822267808\n",
            "train loss:0.006153403253926995\n",
            "train loss:0.1104940105953249\n",
            "train loss:0.004563040753339444\n",
            "train loss:0.009661750543780195\n",
            "train loss:0.038004988097145545\n",
            "train loss:0.02847133848254169\n",
            "train loss:0.054705129284677884\n",
            "train loss:0.012944863993047053\n",
            "train loss:0.01265128579622667\n",
            "train loss:0.03421389121503216\n",
            "train loss:0.025084037636278766\n",
            "train loss:0.017839230679553668\n",
            "train loss:0.025570325923049288\n",
            "train loss:0.059183537378914124\n",
            "train loss:0.04272508062155425\n",
            "train loss:0.010787308983974333\n",
            "train loss:0.028887523222254276\n",
            "train loss:0.06155292038818664\n",
            "train loss:0.010282933424460365\n",
            "train loss:0.011224445567625417\n",
            "train loss:0.007857158771790811\n",
            "train loss:0.01398464728799424\n",
            "train loss:0.04161319946818036\n",
            "train loss:0.08505645282870164\n",
            "train loss:0.016058259640481198\n",
            "train loss:0.00760844250248174\n",
            "train loss:0.009298541709333823\n",
            "train loss:0.020673335569318\n",
            "train loss:0.11372762646069287\n",
            "train loss:0.04574049530769633\n",
            "train loss:0.04414728316314943\n",
            "train loss:0.0074161581803201095\n",
            "train loss:0.020493221882886315\n",
            "train loss:0.02036478603330729\n",
            "train loss:0.041582944919762\n",
            "train loss:0.012473587063655518\n",
            "train loss:0.06246232814541555\n",
            "train loss:0.007187146319997632\n",
            "train loss:0.03894710344878594\n",
            "train loss:0.03206651131903213\n",
            "train loss:0.03067778387348116\n",
            "train loss:0.005860234778983957\n",
            "train loss:0.0023929005054506673\n",
            "train loss:0.06525362473779547\n",
            "train loss:0.008254623566124303\n",
            "train loss:0.009300490578635974\n",
            "train loss:0.06421300297146783\n",
            "train loss:0.01762320759774226\n",
            "train loss:0.052733869786272744\n",
            "train loss:0.023515358472283435\n",
            "train loss:0.03865505320301812\n",
            "train loss:0.02675988141348916\n",
            "train loss:0.020415653191244797\n",
            "train loss:0.037999894872808256\n",
            "train loss:0.009260427978493656\n",
            "train loss:0.08356695873951608\n",
            "train loss:0.019051046375337263\n",
            "train loss:0.048422897905632124\n",
            "train loss:0.037258420166026575\n",
            "train loss:0.010146630788881221\n",
            "train loss:0.008138187327721164\n",
            "train loss:0.004463278871190796\n",
            "train loss:0.05073341880938204\n",
            "train loss:0.12423505923884981\n",
            "train loss:0.045710676632351045\n",
            "train loss:0.03509526642541016\n",
            "train loss:0.022839851290610794\n",
            "train loss:0.09996871868602325\n",
            "train loss:0.07281258024260304\n",
            "=== epoch:163, train acc:0.991875, test acc:0.919 ===\n",
            "train loss:0.021282166822295487\n",
            "train loss:0.030242311251571632\n",
            "train loss:0.029230596696223674\n",
            "train loss:0.007316916556735716\n",
            "train loss:0.01639278085535918\n",
            "train loss:0.05289533821202249\n",
            "train loss:0.013922151641243887\n",
            "train loss:0.00770218789759896\n",
            "train loss:0.010742125837391472\n",
            "train loss:0.013295595301077354\n",
            "train loss:0.0255183388077568\n",
            "train loss:0.014094634867408536\n",
            "train loss:0.02065285123325275\n",
            "train loss:0.08119550566511707\n",
            "train loss:0.00714250378266256\n",
            "train loss:0.05542130114841308\n",
            "train loss:0.03445132002954862\n",
            "train loss:0.010666741365748689\n",
            "train loss:0.06976074337275077\n",
            "train loss:0.03991833428896716\n",
            "train loss:0.0523570189078907\n",
            "train loss:0.02256970147482077\n",
            "train loss:0.010824805917144046\n",
            "train loss:0.01615970658759528\n",
            "train loss:0.035144433179154756\n",
            "train loss:0.008661383413079718\n",
            "train loss:0.0807182823121409\n",
            "train loss:0.007287084882173303\n",
            "train loss:0.004034857502386403\n",
            "train loss:0.014170013353873625\n",
            "train loss:0.07251325406891872\n",
            "train loss:0.05731285617996547\n",
            "train loss:0.11336374780854287\n",
            "train loss:0.010288979619092031\n",
            "train loss:0.01951842595671001\n",
            "train loss:0.023475663601835527\n",
            "train loss:0.07035320707635905\n",
            "train loss:0.12718935787013064\n",
            "train loss:0.03133498785270189\n",
            "train loss:0.0323676022548233\n",
            "train loss:0.006658231139985938\n",
            "train loss:0.026948171725336492\n",
            "train loss:0.010757823585687373\n",
            "train loss:0.07481196862456846\n",
            "train loss:0.031725107649132715\n",
            "train loss:0.005927553898082035\n",
            "train loss:0.010174955730912988\n",
            "train loss:0.09379212988869395\n",
            "train loss:0.02228766635855139\n",
            "train loss:0.005283945397996901\n",
            "train loss:0.022416147379697084\n",
            "train loss:0.009144143669774668\n",
            "train loss:0.029265532071052182\n",
            "train loss:0.0023048510011993927\n",
            "train loss:0.014191944685151066\n",
            "train loss:0.005455721668230975\n",
            "train loss:0.029669159233622566\n",
            "train loss:0.01428374410835169\n",
            "train loss:0.027877602720032195\n",
            "train loss:0.027313911492318658\n",
            "train loss:0.008211673504939178\n",
            "train loss:0.034292798149746624\n",
            "train loss:0.06605683019154425\n",
            "train loss:0.04907903175991028\n",
            "train loss:0.05477547139376235\n",
            "train loss:0.0552548470662681\n",
            "train loss:0.05225835047466184\n",
            "train loss:0.03584272621857269\n",
            "train loss:0.014265737410313204\n",
            "train loss:0.09723728476680008\n",
            "train loss:0.02453952118050974\n",
            "train loss:0.0692782852239307\n",
            "train loss:0.010927281221609264\n",
            "train loss:0.0494830450324919\n",
            "train loss:0.030008320402168182\n",
            "train loss:0.008638734059276393\n",
            "train loss:0.009452308055685695\n",
            "train loss:0.027985447186725586\n",
            "train loss:0.0931614603341567\n",
            "train loss:0.08586866823745026\n",
            "=== epoch:164, train acc:0.992, test acc:0.919 ===\n",
            "train loss:0.01774431547608302\n",
            "train loss:0.0028571704116314585\n",
            "train loss:0.014014963301168195\n",
            "train loss:0.009308013325503869\n",
            "train loss:0.007725286315312708\n",
            "train loss:0.048436750372540874\n",
            "train loss:0.013166929094423454\n",
            "train loss:0.007271012442968186\n",
            "train loss:0.09460075896440084\n",
            "train loss:0.009936427246330575\n",
            "train loss:0.027595544017116704\n",
            "train loss:0.004353522715188459\n",
            "train loss:0.011480502804364676\n",
            "train loss:0.13447908853215584\n",
            "train loss:0.009498114069626194\n",
            "train loss:0.019818044971128802\n",
            "train loss:0.031363924966711566\n",
            "train loss:0.05323027398041211\n",
            "train loss:0.019869625238550494\n",
            "train loss:0.00658141651143783\n",
            "train loss:0.013513038771407769\n",
            "train loss:0.012705119797932233\n",
            "train loss:0.016143544829096284\n",
            "train loss:0.011220710118788002\n",
            "train loss:0.03548733407556189\n",
            "train loss:0.08756094649773541\n",
            "train loss:0.0731930256168755\n",
            "train loss:0.010715992713314465\n",
            "train loss:0.05632164501912\n",
            "train loss:0.026504766944792188\n",
            "train loss:0.07357429751911121\n",
            "train loss:0.05598336664676241\n",
            "train loss:0.025288590093066436\n",
            "train loss:0.09080880594324801\n",
            "train loss:0.05697852079687189\n",
            "train loss:0.008812591657655543\n",
            "train loss:0.0768605720093629\n",
            "train loss:0.009162079386197924\n",
            "train loss:0.015899044617051005\n",
            "train loss:0.026041484634166757\n",
            "train loss:0.007424127653704998\n",
            "train loss:0.049929059592753046\n",
            "train loss:0.023075800065732163\n",
            "train loss:0.03782988912281021\n",
            "train loss:0.024140596430670572\n",
            "train loss:0.015233857441620162\n",
            "train loss:0.033293753275591136\n",
            "train loss:0.038035545455202546\n",
            "train loss:0.011627916551189853\n",
            "train loss:0.022979083080142248\n",
            "train loss:0.009135490781149523\n",
            "train loss:0.030765342612683436\n",
            "train loss:0.005852408223023425\n",
            "train loss:0.010059894598708702\n",
            "train loss:0.011770173509244676\n",
            "train loss:0.12060350911351997\n",
            "train loss:0.013275101548893163\n",
            "train loss:0.008710057465417451\n",
            "train loss:0.01013431123272303\n",
            "train loss:0.007038501547126643\n",
            "train loss:0.08808317730731091\n",
            "train loss:0.005734874762183697\n",
            "train loss:0.0351503201733297\n",
            "train loss:0.09099229743667926\n",
            "train loss:0.007865010709155172\n",
            "train loss:0.05079885550354016\n",
            "train loss:0.018112427771497956\n",
            "train loss:0.011214706915062824\n",
            "train loss:0.021905372271374186\n",
            "train loss:0.04933970122946275\n",
            "train loss:0.02937939920941444\n",
            "train loss:0.04378391986222312\n",
            "train loss:0.02560389177451519\n",
            "train loss:0.006067262517542563\n",
            "train loss:0.09144150709490802\n",
            "train loss:0.005433103706091723\n",
            "train loss:0.07318032545732016\n",
            "train loss:0.030528459687625326\n",
            "train loss:0.007103734907891942\n",
            "train loss:0.008410223317214777\n",
            "=== epoch:165, train acc:0.99175, test acc:0.918 ===\n",
            "train loss:0.022102758400639032\n",
            "train loss:0.011997038287754726\n",
            "train loss:0.08574747715398738\n",
            "train loss:0.043313480603133184\n",
            "train loss:0.0023195256167149866\n",
            "train loss:0.012700513666940414\n",
            "train loss:0.05953005990030655\n",
            "train loss:0.004635768855316667\n",
            "train loss:0.02502405936360695\n",
            "train loss:0.0063052546931026245\n",
            "train loss:0.03685692944445318\n",
            "train loss:0.035259402257679234\n",
            "train loss:0.0491976657589186\n",
            "train loss:0.006036824100634507\n",
            "train loss:0.009377603010586388\n",
            "train loss:0.036884868247774066\n",
            "train loss:0.00570628370845876\n",
            "train loss:0.02773323249700987\n",
            "train loss:0.004526979487602743\n",
            "train loss:0.008873380622359615\n",
            "train loss:0.0259383762829131\n",
            "train loss:0.032132467708143285\n",
            "train loss:0.09108211392216173\n",
            "train loss:0.06940617802761109\n",
            "train loss:0.061106220403292257\n",
            "train loss:0.09098177487439242\n",
            "train loss:0.02963546923653729\n",
            "train loss:0.009051561155648247\n",
            "train loss:0.015692412234414588\n",
            "train loss:0.004142543483134423\n",
            "train loss:0.06534009885640833\n",
            "train loss:0.07766702563067045\n",
            "train loss:0.017604351905569856\n",
            "train loss:0.04215672282672788\n",
            "train loss:0.052782482599057705\n",
            "train loss:0.008340341123368993\n",
            "train loss:0.011671446234246357\n",
            "train loss:0.02809181002239897\n",
            "train loss:0.0057960487116899325\n",
            "train loss:0.028720359227436102\n",
            "train loss:0.05518444969867808\n",
            "train loss:0.004683315793266851\n",
            "train loss:0.004619647722994689\n",
            "train loss:0.04034321366349194\n",
            "train loss:0.019234809428277284\n",
            "train loss:0.026045416411091548\n",
            "train loss:0.07501330731019212\n",
            "train loss:0.045163317529024416\n",
            "train loss:0.021701768063870797\n",
            "train loss:0.06905073801103839\n",
            "train loss:0.015357744506796698\n",
            "train loss:0.005588600270913147\n",
            "train loss:0.05577943489130534\n",
            "train loss:0.012467504323403301\n",
            "train loss:0.04900959456493553\n",
            "train loss:0.00922623249991274\n",
            "train loss:0.006150055678098822\n",
            "train loss:0.007451385390334974\n",
            "train loss:0.0077817898910237234\n",
            "train loss:0.051777213553784306\n",
            "train loss:0.003721111190467407\n",
            "train loss:0.02307174160638793\n",
            "train loss:0.04145307652359703\n",
            "train loss:0.014101986451079861\n",
            "train loss:0.034913102829681804\n",
            "train loss:0.08600013635979607\n",
            "train loss:0.09568057886178355\n",
            "train loss:0.04901692588990934\n",
            "train loss:0.031771838666908234\n",
            "train loss:0.0066008238732299315\n",
            "train loss:0.04385400548472379\n",
            "train loss:0.01663595449388949\n",
            "train loss:0.013980207228132402\n",
            "train loss:0.001956852444026221\n",
            "train loss:0.06001605662193846\n",
            "train loss:0.023498820002484197\n",
            "train loss:0.0032134238947084605\n",
            "train loss:0.009011081168738942\n",
            "train loss:0.025281521919710937\n",
            "train loss:0.004012625195680069\n",
            "=== epoch:166, train acc:0.99125, test acc:0.9205 ===\n",
            "train loss:0.03890250182719875\n",
            "train loss:0.010719013829304083\n",
            "train loss:0.00679529327745561\n",
            "train loss:0.01077404345266361\n",
            "train loss:0.012020391594459983\n",
            "train loss:0.00738571053374401\n",
            "train loss:0.04573673611662604\n",
            "train loss:0.002676068771458576\n",
            "train loss:0.013065747808719547\n",
            "train loss:0.015258266588631149\n",
            "train loss:0.022204097524266622\n",
            "train loss:0.015038235680017569\n",
            "train loss:0.006836719095221924\n",
            "train loss:0.0665116633903319\n",
            "train loss:0.0050773469189783595\n",
            "train loss:0.01033100346093048\n",
            "train loss:0.0433039268837822\n",
            "train loss:0.023387223578404264\n",
            "train loss:0.008777479558100605\n",
            "train loss:0.037978924337946626\n",
            "train loss:0.004855989858614253\n",
            "train loss:0.025967289818374865\n",
            "train loss:0.012363262151631735\n",
            "train loss:0.05541614370042442\n",
            "train loss:0.004021319069333662\n",
            "train loss:0.01361239170886662\n",
            "train loss:0.06275660714377307\n",
            "train loss:0.012364294500178777\n",
            "train loss:0.0526575256876292\n",
            "train loss:0.03495412403611124\n",
            "train loss:0.018721427952655186\n",
            "train loss:0.013355174841164297\n",
            "train loss:0.006889248689235656\n",
            "train loss:0.03751677193800903\n",
            "train loss:0.11786428723285583\n",
            "train loss:0.006429068547281747\n",
            "train loss:0.040130328367929444\n",
            "train loss:0.0035262791233659154\n",
            "train loss:0.10576427761504853\n",
            "train loss:0.010004813146701435\n",
            "train loss:0.06809837946174375\n",
            "train loss:0.007243345441514626\n",
            "train loss:0.0492070510524123\n",
            "train loss:0.02144409036251604\n",
            "train loss:0.0626520136199097\n",
            "train loss:0.031053149611379064\n",
            "train loss:0.0076984833753959096\n",
            "train loss:0.04997567553913738\n",
            "train loss:0.0024007998050833814\n",
            "train loss:0.006915476331936876\n",
            "train loss:0.07682666133978297\n",
            "train loss:0.07943231455051061\n",
            "train loss:0.011161278458556648\n",
            "train loss:0.06443172777935696\n",
            "train loss:0.039818377151598854\n",
            "train loss:0.005064197161301223\n",
            "train loss:0.04919302210809065\n",
            "train loss:0.012150739461806295\n",
            "train loss:0.023882843443191235\n",
            "train loss:0.03941732861558276\n",
            "train loss:0.029548639207598256\n",
            "train loss:0.004865587100695317\n",
            "train loss:0.007004691184206127\n",
            "train loss:0.008606479937602707\n",
            "train loss:0.03844091308887815\n",
            "train loss:0.005396417924410884\n",
            "train loss:0.061521331242032795\n",
            "train loss:0.003391934800087045\n",
            "train loss:0.006837323557217696\n",
            "train loss:0.06562265536966985\n",
            "train loss:0.006195217470912807\n",
            "train loss:0.009266745001231062\n",
            "train loss:0.014913613374356247\n",
            "train loss:0.05939577027882742\n",
            "train loss:0.09799113185175586\n",
            "train loss:0.006424180545387506\n",
            "train loss:0.004625900266982829\n",
            "train loss:0.007685019914503123\n",
            "train loss:0.04340267458694671\n",
            "train loss:0.027504436147066814\n",
            "=== epoch:167, train acc:0.99175, test acc:0.9215 ===\n",
            "train loss:0.041086974308245046\n",
            "train loss:0.048311996006990675\n",
            "train loss:0.015148414240717702\n",
            "train loss:0.011685296596798556\n",
            "train loss:0.01452180073892835\n",
            "train loss:0.05969852161579115\n",
            "train loss:0.05398820303557018\n",
            "train loss:0.04532743695229988\n",
            "train loss:0.004580165416052648\n",
            "train loss:0.007007618520224449\n",
            "train loss:0.009304174650879227\n",
            "train loss:0.050790962083116654\n",
            "train loss:0.05943232682183272\n",
            "train loss:0.003054337182361721\n",
            "train loss:0.0175025411884937\n",
            "train loss:0.08299125826201317\n",
            "train loss:0.04741649448903415\n",
            "train loss:0.04169944082115214\n",
            "train loss:0.007783556704142519\n",
            "train loss:0.02597542750199503\n",
            "train loss:0.03198596030220569\n",
            "train loss:0.030304021308515373\n",
            "train loss:0.028832809494510645\n",
            "train loss:0.017161783695276062\n",
            "train loss:0.011246184770073579\n",
            "train loss:0.012132438467628593\n",
            "train loss:0.062151350688907474\n",
            "train loss:0.03767235954541701\n",
            "train loss:0.017296822894287207\n",
            "train loss:0.00538850851254677\n",
            "train loss:0.009213918749665652\n",
            "train loss:0.017941558986741303\n",
            "train loss:0.024901860621134295\n",
            "train loss:0.016725570260771788\n",
            "train loss:0.052127389225188746\n",
            "train loss:0.020533935899554295\n",
            "train loss:0.03607571636006939\n",
            "train loss:0.005077866394840871\n",
            "train loss:0.02456943241853595\n",
            "train loss:0.02870507885106472\n",
            "train loss:0.0070965873065313425\n",
            "train loss:0.0052475348089238705\n",
            "train loss:0.09473436223985539\n",
            "train loss:0.07391246927251861\n",
            "train loss:0.028505467507853117\n",
            "train loss:0.012954925553956276\n",
            "train loss:0.053562729834746164\n",
            "train loss:0.03459090940187454\n",
            "train loss:0.022692596595092436\n",
            "train loss:0.034720980072037076\n",
            "train loss:0.008000887470250575\n",
            "train loss:0.055273268561854876\n",
            "train loss:0.03280398893205017\n",
            "train loss:0.006380152264016681\n",
            "train loss:0.04202561262295878\n",
            "train loss:0.028950523923486297\n",
            "train loss:0.007463058353783875\n",
            "train loss:0.02171284066597168\n",
            "train loss:0.020394258271135234\n",
            "train loss:0.046840886165970506\n",
            "train loss:0.05061878176111951\n",
            "train loss:0.025535233231229443\n",
            "train loss:0.0073653967261025285\n",
            "train loss:0.06925482566855962\n",
            "train loss:0.0021520236166149205\n",
            "train loss:0.050825397428555234\n",
            "train loss:0.04145274356281666\n",
            "train loss:0.0034010866260144362\n",
            "train loss:0.01739792582686001\n",
            "train loss:0.019826012156732287\n",
            "train loss:0.06911728172867954\n",
            "train loss:0.04203910582488587\n",
            "train loss:0.02622755929104127\n",
            "train loss:0.03745504661902365\n",
            "train loss:0.008273283803790747\n",
            "train loss:0.04885682944257314\n",
            "train loss:0.05264258768679203\n",
            "train loss:0.008457284051629915\n",
            "train loss:0.020881374128210194\n",
            "train loss:0.01233351712169679\n",
            "=== epoch:168, train acc:0.992, test acc:0.92 ===\n",
            "train loss:0.07110690688886263\n",
            "train loss:0.006323579593810289\n",
            "train loss:0.08225245628921461\n",
            "train loss:0.008396157227212634\n",
            "train loss:0.07828853860532135\n",
            "train loss:0.02848712071747054\n",
            "train loss:0.04052315917653096\n",
            "train loss:0.010489802477252817\n",
            "train loss:0.004781816182383813\n",
            "train loss:0.008296254221913484\n",
            "train loss:0.02019945038928556\n",
            "train loss:0.0063231265652611245\n",
            "train loss:0.02315095449412685\n",
            "train loss:0.007581299909228222\n",
            "train loss:0.01112342078672796\n",
            "train loss:0.0036287894884720435\n",
            "train loss:0.008811446640471711\n",
            "train loss:0.049616424804084396\n",
            "train loss:0.017919090059346575\n",
            "train loss:0.005982844875379152\n",
            "train loss:0.00191043056816351\n",
            "train loss:0.022382853416604228\n",
            "train loss:0.003359642525977567\n",
            "train loss:0.0014808427358303283\n",
            "train loss:0.05695660651873325\n",
            "train loss:0.06097958958160609\n",
            "train loss:0.016381530586126458\n",
            "train loss:0.007528383705057678\n",
            "train loss:0.02844371644414949\n",
            "train loss:0.0055628671556762176\n",
            "train loss:0.025484847938855148\n",
            "train loss:0.030966033932775104\n",
            "train loss:0.0041810452730390215\n",
            "train loss:0.01604189355385329\n",
            "train loss:0.007991237252973859\n",
            "train loss:0.014719572803477774\n",
            "train loss:0.11240477097016069\n",
            "train loss:0.017910766564970336\n",
            "train loss:0.011306031185012435\n",
            "train loss:0.00938263462223391\n",
            "train loss:0.011844625937689392\n",
            "train loss:0.021872051659513177\n",
            "train loss:0.006922310683863041\n",
            "train loss:0.056717496654903846\n",
            "train loss:0.02899764177851567\n",
            "train loss:0.04798952342707317\n",
            "train loss:0.025959775669130568\n",
            "train loss:0.038344445089684785\n",
            "train loss:0.07682851504532659\n",
            "train loss:0.0039929616992880265\n",
            "train loss:0.04254482050194579\n",
            "train loss:0.008910419115594193\n",
            "train loss:0.03570262585148763\n",
            "train loss:0.009072283742885342\n",
            "train loss:0.02656993684815872\n",
            "train loss:0.01763869301022395\n",
            "train loss:0.006663857422467993\n",
            "train loss:0.010499571334545465\n",
            "train loss:0.007600371498948708\n",
            "train loss:0.018377669401878415\n",
            "train loss:0.011303109579048448\n",
            "train loss:0.09852019833336392\n",
            "train loss:0.015599845899161477\n",
            "train loss:0.046534889956500375\n",
            "train loss:0.00654588986226544\n",
            "train loss:0.024007093952009964\n",
            "train loss:0.004291954570519719\n",
            "train loss:0.013835128776651441\n",
            "train loss:0.028869372916748494\n",
            "train loss:0.06045275388889998\n",
            "train loss:0.013617625255677756\n",
            "train loss:0.017753665440203018\n",
            "train loss:0.0018770737564969813\n",
            "train loss:0.05047779093908555\n",
            "train loss:0.01154478369935086\n",
            "train loss:0.00501352823625961\n",
            "train loss:0.008323993806049416\n",
            "train loss:0.053123231439418064\n",
            "train loss:0.012629613215188771\n",
            "train loss:0.00493724231336353\n",
            "=== epoch:169, train acc:0.992, test acc:0.925 ===\n",
            "train loss:0.03641101465112551\n",
            "train loss:0.0033856361742462967\n",
            "train loss:0.011711557195008493\n",
            "train loss:0.05359591567319117\n",
            "train loss:0.04707636320677348\n",
            "train loss:0.05176922229035484\n",
            "train loss:0.00832199782965596\n",
            "train loss:0.02022815157413893\n",
            "train loss:0.03201116611301555\n",
            "train loss:0.0963061022791735\n",
            "train loss:0.008099914388877522\n",
            "train loss:0.004724679162119856\n",
            "train loss:0.007262962132794431\n",
            "train loss:0.054839618892067114\n",
            "train loss:0.01902548583548046\n",
            "train loss:0.013771289383384615\n",
            "train loss:0.029740446106906595\n",
            "train loss:0.029152643948370548\n",
            "train loss:0.01664660429353519\n",
            "train loss:0.010689074868612933\n",
            "train loss:0.003323836738774521\n",
            "train loss:0.008742733183553335\n",
            "train loss:0.06278773776426737\n",
            "train loss:0.007520653005103427\n",
            "train loss:0.045199682552475695\n",
            "train loss:0.05497616179982463\n",
            "train loss:0.010247628848293182\n",
            "train loss:0.005972202714300217\n",
            "train loss:0.011268920630143295\n",
            "train loss:0.0376599612950744\n",
            "train loss:0.055851404605090804\n",
            "train loss:0.00860034226379012\n",
            "train loss:0.006789542729903116\n",
            "train loss:0.06503336193508366\n",
            "train loss:0.015204107343856355\n",
            "train loss:0.004768309223387692\n",
            "train loss:0.049898456594290395\n",
            "train loss:0.07461541161605767\n",
            "train loss:0.008442704018322377\n",
            "train loss:0.0448168590573125\n",
            "train loss:0.06462628622466202\n",
            "train loss:0.006084576248809129\n",
            "train loss:0.0202589039097177\n",
            "train loss:0.04325022551304605\n",
            "train loss:0.010945639512156576\n",
            "train loss:0.10872154907294908\n",
            "train loss:0.01714414908701556\n",
            "train loss:0.0040368143619965895\n",
            "train loss:0.00537218249529942\n",
            "train loss:0.02451836194458579\n",
            "train loss:0.01829027204175088\n",
            "train loss:0.00865732259296215\n",
            "train loss:0.0022907987089621474\n",
            "train loss:0.02940914488781177\n",
            "train loss:0.006033177780850762\n",
            "train loss:0.060433987778390284\n",
            "train loss:0.014245057315184858\n",
            "train loss:0.05456140451178209\n",
            "train loss:0.01507906909973022\n",
            "train loss:0.03059292206754276\n",
            "train loss:0.054524166787616506\n",
            "train loss:0.013333198827099458\n",
            "train loss:0.008686671365877944\n",
            "train loss:0.011933970233837424\n",
            "train loss:0.0732814447392686\n",
            "train loss:0.00462996057144745\n",
            "train loss:0.025562187088769218\n",
            "train loss:0.017916881624201523\n",
            "train loss:0.018628141935227452\n",
            "train loss:0.028033768426785854\n",
            "train loss:0.08180931036013227\n",
            "train loss:0.07116093667138461\n",
            "train loss:0.006966211601671735\n",
            "train loss:0.07031778092341252\n",
            "train loss:0.012954486363288959\n",
            "train loss:0.08561619177039093\n",
            "train loss:0.05676182306377677\n",
            "train loss:0.018041472799376856\n",
            "train loss:0.04687655915400301\n",
            "train loss:0.09468674261562002\n",
            "=== epoch:170, train acc:0.99225, test acc:0.9195 ===\n",
            "train loss:0.03147107376334294\n",
            "train loss:0.04521520637102086\n",
            "train loss:0.04680049519106251\n",
            "train loss:0.011343558310526936\n",
            "train loss:0.005607654184759611\n",
            "train loss:0.0027794213338996248\n",
            "train loss:0.04205951670522985\n",
            "train loss:0.0025262608005936315\n",
            "train loss:0.008440424248820402\n",
            "train loss:0.02865600036209901\n",
            "train loss:0.008834956133600436\n",
            "train loss:0.02195454930618144\n",
            "train loss:0.005787582750607795\n",
            "train loss:0.005735902112835051\n",
            "train loss:0.06585163509907031\n",
            "train loss:0.05239134951798955\n",
            "train loss:0.022758791205552256\n",
            "train loss:0.08278945463755988\n",
            "train loss:0.007694556846069971\n",
            "train loss:0.00857809092433961\n",
            "train loss:0.006585270895491868\n",
            "train loss:0.005981451608930908\n",
            "train loss:0.013631562492931444\n",
            "train loss:0.00941003764227357\n",
            "train loss:0.022054004456340284\n",
            "train loss:0.01252249313264028\n",
            "train loss:0.009432614452871817\n",
            "train loss:0.00444739520742491\n",
            "train loss:0.06637867787146712\n",
            "train loss:0.03390089609984084\n",
            "train loss:0.012798717386943152\n",
            "train loss:0.048705877663728116\n",
            "train loss:0.09256788394855311\n",
            "train loss:0.04753368199762406\n",
            "train loss:0.004233020180232554\n",
            "train loss:0.013815428108243108\n",
            "train loss:0.09346290420635542\n",
            "train loss:0.014531007657995223\n",
            "train loss:0.049272680173814566\n",
            "train loss:0.03727752450444184\n",
            "train loss:0.007980545364516116\n",
            "train loss:0.007277137211835364\n",
            "train loss:0.011114502366062129\n",
            "train loss:0.039576797811096874\n",
            "train loss:0.01676955954684742\n",
            "train loss:0.01778633855589696\n",
            "train loss:0.014884658443516314\n",
            "train loss:0.007034114352758181\n",
            "train loss:0.09153962488512998\n",
            "train loss:0.011123560861954075\n",
            "train loss:0.02824670272472237\n",
            "train loss:0.009820380784774901\n",
            "train loss:0.01551085604141028\n",
            "train loss:0.03491107986369082\n",
            "train loss:0.006180776912998628\n",
            "train loss:0.002877849717691063\n",
            "train loss:0.03569930187202676\n",
            "train loss:0.004558425524421486\n",
            "train loss:0.007947187942974774\n",
            "train loss:0.026814765488922646\n",
            "train loss:0.005790111595092709\n",
            "train loss:0.03017684985950929\n",
            "train loss:0.004860160582886429\n",
            "train loss:0.030924336417743604\n",
            "train loss:0.0218075969032387\n",
            "train loss:0.011847597267811967\n",
            "train loss:0.008056655805977551\n",
            "train loss:0.0034468418794710103\n",
            "train loss:0.03453980349329696\n",
            "train loss:0.013626423604514121\n",
            "train loss:0.015947523761932026\n",
            "train loss:0.05059305574256725\n",
            "train loss:0.009935425141641164\n",
            "train loss:0.009421437690190748\n",
            "train loss:0.06154401017073347\n",
            "train loss:0.06028526708620039\n",
            "train loss:0.007678004428508918\n",
            "train loss:0.006412908067462726\n",
            "train loss:0.00412031238223481\n",
            "train loss:0.056961773668172413\n",
            "=== epoch:171, train acc:0.992375, test acc:0.915 ===\n",
            "train loss:0.012581791034243655\n",
            "train loss:0.0053540827620524615\n",
            "train loss:0.011911263807520094\n",
            "train loss:0.010740052955200328\n",
            "train loss:0.013291391405724074\n",
            "train loss:0.022163371218478937\n",
            "train loss:0.014853038702389456\n",
            "train loss:0.007494677868292714\n",
            "train loss:0.017158361645335133\n",
            "train loss:0.051881784972273026\n",
            "train loss:0.04306634251882499\n",
            "train loss:0.031117339624008807\n",
            "train loss:0.03402408313478371\n",
            "train loss:0.033243505918051405\n",
            "train loss:0.009445164146116214\n",
            "train loss:0.027023865030971317\n",
            "train loss:0.005244283064932504\n",
            "train loss:0.007934139333392768\n",
            "train loss:0.012112440670669473\n",
            "train loss:0.010797502952096461\n",
            "train loss:0.02911221324035626\n",
            "train loss:0.011314502141318857\n",
            "train loss:0.03587591560416508\n",
            "train loss:0.010760023074021468\n",
            "train loss:0.01662124548458796\n",
            "train loss:0.012332129524234434\n",
            "train loss:0.005534298950424709\n",
            "train loss:0.016649607661755016\n",
            "train loss:0.023373278080778336\n",
            "train loss:0.04619691427523101\n",
            "train loss:0.02810085351772599\n",
            "train loss:0.007786857108428321\n",
            "train loss:0.03850414608746018\n",
            "train loss:0.02556265992340293\n",
            "train loss:0.01351960796042019\n",
            "train loss:0.03719660844380703\n",
            "train loss:0.009383972717999355\n",
            "train loss:0.0028003255081709482\n",
            "train loss:0.006560015226591889\n",
            "train loss:0.04160799060753331\n",
            "train loss:0.014802447229631567\n",
            "train loss:0.04065096205518578\n",
            "train loss:0.012231012460482112\n",
            "train loss:0.010546449261492473\n",
            "train loss:0.009733590968993337\n",
            "train loss:0.028905570226894044\n",
            "train loss:0.00256031951365237\n",
            "train loss:0.009503130700965251\n",
            "train loss:0.006171832558463479\n",
            "train loss:0.045237524493614456\n",
            "train loss:0.0037013722867667537\n",
            "train loss:0.029624802231906826\n",
            "train loss:0.07842609063638686\n",
            "train loss:0.003356521453616112\n",
            "train loss:0.015466711219608697\n",
            "train loss:0.003904679400270779\n",
            "train loss:0.013398578017732359\n",
            "train loss:0.09078470464748936\n",
            "train loss:0.03145135217908718\n",
            "train loss:0.11250989208500878\n",
            "train loss:0.004913232429991651\n",
            "train loss:0.007599709860293528\n",
            "train loss:0.026164422511650996\n",
            "train loss:0.043275675509686265\n",
            "train loss:0.004171478877459114\n",
            "train loss:0.005802479005821623\n",
            "train loss:0.036708831279340305\n",
            "train loss:0.011237083093611721\n",
            "train loss:0.005346525038253538\n",
            "train loss:0.005694639981695695\n",
            "train loss:0.0790955228841115\n",
            "train loss:0.07714495493015547\n",
            "train loss:0.010556544248256673\n",
            "train loss:0.028263848267705928\n",
            "train loss:0.09032403116008997\n",
            "train loss:0.07062455656494752\n",
            "train loss:0.04287244490597101\n",
            "train loss:0.02926061703744918\n",
            "train loss:0.007422576077862924\n",
            "train loss:0.013945112328100515\n",
            "=== epoch:172, train acc:0.993125, test acc:0.9125 ===\n",
            "train loss:0.031575778962772645\n",
            "train loss:0.006388750223283716\n",
            "train loss:0.022896546063542057\n",
            "train loss:0.054797380965609835\n",
            "train loss:0.008858488047917777\n",
            "train loss:0.02508111307306415\n",
            "train loss:0.024803640514869336\n",
            "train loss:0.017288222138168198\n",
            "train loss:0.0017479558459570575\n",
            "train loss:0.06661142776737954\n",
            "train loss:0.029854415520313142\n",
            "train loss:0.06235434707661223\n",
            "train loss:0.010743314495395472\n",
            "train loss:0.05685379693838035\n",
            "train loss:0.021514612246480644\n",
            "train loss:0.014482622175489644\n",
            "train loss:0.08110165241030794\n",
            "train loss:0.0074592057964155255\n",
            "train loss:0.005208967070929744\n",
            "train loss:0.003934907151036565\n",
            "train loss:0.008823203214701772\n",
            "train loss:0.009396529882787625\n",
            "train loss:0.01804429668529592\n",
            "train loss:0.0899302740957293\n",
            "train loss:0.004736782722703657\n",
            "train loss:0.008135434556958577\n",
            "train loss:0.014955079538773062\n",
            "train loss:0.006802422566292314\n",
            "train loss:0.01600577983715212\n",
            "train loss:0.006450381929992881\n",
            "train loss:0.014929331596680383\n",
            "train loss:0.007245917737898416\n",
            "train loss:0.03277248388372886\n",
            "train loss:0.029908154020798734\n",
            "train loss:0.003999058501897568\n",
            "train loss:0.004375167209646304\n",
            "train loss:0.052344109516547735\n",
            "train loss:0.007229922326601612\n",
            "train loss:0.013047566004331572\n",
            "train loss:0.010170275969446474\n",
            "train loss:0.08392226045371565\n",
            "train loss:0.003501488663214231\n",
            "train loss:0.010356326459151912\n",
            "train loss:0.013189167925571838\n",
            "train loss:0.0027514766764807396\n",
            "train loss:0.006411572633765087\n",
            "train loss:0.021929451891919007\n",
            "train loss:0.009812501595048614\n",
            "train loss:0.004425577548162853\n",
            "train loss:0.020851026818307136\n",
            "train loss:0.048048572664260186\n",
            "train loss:0.006799795245085734\n",
            "train loss:0.006600250344684996\n",
            "train loss:0.006638607376078333\n",
            "train loss:0.009536833101103518\n",
            "train loss:0.056147361355728995\n",
            "train loss:0.02179198546937327\n",
            "train loss:0.02347610734531445\n",
            "train loss:0.010958091744579526\n",
            "train loss:0.016183135972497376\n",
            "train loss:0.004901293507126939\n",
            "train loss:0.007844024164688967\n",
            "train loss:0.024250423002218066\n",
            "train loss:0.0023523354233307256\n",
            "train loss:0.04946131737223012\n",
            "train loss:0.03246303089239365\n",
            "train loss:0.0450607143608436\n",
            "train loss:0.007110852300432858\n",
            "train loss:0.006969131041603746\n",
            "train loss:0.0133135110235911\n",
            "train loss:0.008845488209803845\n",
            "train loss:0.05832564335112287\n",
            "train loss:0.01543183044479749\n",
            "train loss:0.003266906714182361\n",
            "train loss:0.005486641077407812\n",
            "train loss:0.011581319901348166\n",
            "train loss:0.004053198864882718\n",
            "train loss:0.04278472476956327\n",
            "train loss:0.005043178382950279\n",
            "train loss:0.017252613639903727\n",
            "=== epoch:173, train acc:0.993, test acc:0.92 ===\n",
            "train loss:0.01736873527623741\n",
            "train loss:0.010301157181104026\n",
            "train loss:0.004422015753771865\n",
            "train loss:0.023205842389368177\n",
            "train loss:0.014497446994222854\n",
            "train loss:0.017609901387668426\n",
            "train loss:0.06071207224300351\n",
            "train loss:0.012441146978975524\n",
            "train loss:0.0444539950732171\n",
            "train loss:0.008217173649470425\n",
            "train loss:0.012999061548496222\n",
            "train loss:0.05997184166073353\n",
            "train loss:0.004275877313680446\n",
            "train loss:0.07024051864000953\n",
            "train loss:0.009106313495267618\n",
            "train loss:0.00746888125727987\n",
            "train loss:0.03746346572092392\n",
            "train loss:0.013254433310471613\n",
            "train loss:0.007296522679270611\n",
            "train loss:0.00590222357079555\n",
            "train loss:0.03084942119676727\n",
            "train loss:0.006531756752851282\n",
            "train loss:0.04640439699980327\n",
            "train loss:0.028232978900453783\n",
            "train loss:0.008256553656008398\n",
            "train loss:0.005854018744768149\n",
            "train loss:0.040382224079787726\n",
            "train loss:0.0465204299498979\n",
            "train loss:0.0888268444843955\n",
            "train loss:0.007815080861394163\n",
            "train loss:0.01964775559814999\n",
            "train loss:0.025075070401953527\n",
            "train loss:0.008832585720086173\n",
            "train loss:0.028097150656093882\n",
            "train loss:0.012575561943624365\n",
            "train loss:0.0051596151145817445\n",
            "train loss:0.04071745449478299\n",
            "train loss:0.06830920527658947\n",
            "train loss:0.02430592953614302\n",
            "train loss:0.012288055061967413\n",
            "train loss:0.005218882982248922\n",
            "train loss:0.00563679070996971\n",
            "train loss:0.028454853391020447\n",
            "train loss:0.008158184901117135\n",
            "train loss:0.03423616928085055\n",
            "train loss:0.01648743976644993\n",
            "train loss:0.0036369430347176312\n",
            "train loss:0.004260107360925173\n",
            "train loss:0.01137910448159956\n",
            "train loss:0.024352506717084454\n",
            "train loss:0.060360707027466216\n",
            "train loss:0.02526700127580188\n",
            "train loss:0.05935293353268079\n",
            "train loss:0.009898561771745723\n",
            "train loss:0.005028253280317671\n",
            "train loss:0.005021075104866368\n",
            "train loss:0.013625040802596043\n",
            "train loss:0.023101971873999424\n",
            "train loss:0.0035184840025092655\n",
            "train loss:0.005432702095510444\n",
            "train loss:0.00693996499409295\n",
            "train loss:0.014098110005298727\n",
            "train loss:0.00852388631484193\n",
            "train loss:0.05689756024079274\n",
            "train loss:0.008308529854222762\n",
            "train loss:0.006815896008712888\n",
            "train loss:0.004645505396345432\n",
            "train loss:0.0047331628492094545\n",
            "train loss:0.020291332568165572\n",
            "train loss:0.09874015820979971\n",
            "train loss:0.04043825845595072\n",
            "train loss:0.007444465311205383\n",
            "train loss:0.045277297402729\n",
            "train loss:0.020376495692791777\n",
            "train loss:0.027833964652125825\n",
            "train loss:0.02047463795577349\n",
            "train loss:0.005109087333428064\n",
            "train loss:0.024001369015840684\n",
            "train loss:0.014603085763837464\n",
            "train loss:0.003206845909818441\n",
            "=== epoch:174, train acc:0.992875, test acc:0.921 ===\n",
            "train loss:0.005884290567300867\n",
            "train loss:0.024217247489763113\n",
            "train loss:0.00923038101563732\n",
            "train loss:0.020031585855675094\n",
            "train loss:0.02557707373270828\n",
            "train loss:0.004318595592968921\n",
            "train loss:0.05757478911541408\n",
            "train loss:0.008134236735282565\n",
            "train loss:0.010928828153843113\n",
            "train loss:0.03150375011365814\n",
            "train loss:0.03592799255029404\n",
            "train loss:0.011491739965196273\n",
            "train loss:0.045324649826541545\n",
            "train loss:0.08635552291688102\n",
            "train loss:0.02203480421984324\n",
            "train loss:0.05533448996773769\n",
            "train loss:0.005824424609220199\n",
            "train loss:0.016838883310075044\n",
            "train loss:0.03345827923577996\n",
            "train loss:0.01179757713806466\n",
            "train loss:0.019719896408708183\n",
            "train loss:0.005327796234474915\n",
            "train loss:0.0035685437028519424\n",
            "train loss:0.005435993497449559\n",
            "train loss:0.06581636754068061\n",
            "train loss:0.05384772766335785\n",
            "train loss:0.055772298407040316\n",
            "train loss:0.04125260202956071\n",
            "train loss:0.003733459870673772\n",
            "train loss:0.014719302705408482\n",
            "train loss:0.00846736507781513\n",
            "train loss:0.021315640654314367\n",
            "train loss:0.011373221364115458\n",
            "train loss:0.016807426839506347\n",
            "train loss:0.007823095666345837\n",
            "train loss:0.07727881161794231\n",
            "train loss:0.04117054547932618\n",
            "train loss:0.010049716773852198\n",
            "train loss:0.06150695158213664\n",
            "train loss:0.004279254847535353\n",
            "train loss:0.013847727261516585\n",
            "train loss:0.00980560886147122\n",
            "train loss:0.029414406513460972\n",
            "train loss:0.009299286230146173\n",
            "train loss:0.009807685442810231\n",
            "train loss:0.0043352149928130215\n",
            "train loss:0.010610755668950504\n",
            "train loss:0.00467752898881851\n",
            "train loss:0.05620732628566952\n",
            "train loss:0.029775358768721352\n",
            "train loss:0.009219085076544448\n",
            "train loss:0.009036433638908667\n",
            "train loss:0.04270799431287955\n",
            "train loss:0.01758476869467712\n",
            "train loss:0.0076378143634368845\n",
            "train loss:0.009146531333917421\n",
            "train loss:0.02910640950727122\n",
            "train loss:0.09098929580550093\n",
            "train loss:0.005174469406396506\n",
            "train loss:0.035026355203956666\n",
            "train loss:0.016617414762584885\n",
            "train loss:0.07916767963615187\n",
            "train loss:0.04899870264643005\n",
            "train loss:0.08698385977562034\n",
            "train loss:0.018338155082434845\n",
            "train loss:0.023088067710171637\n",
            "train loss:0.08508930824699355\n",
            "train loss:0.003177533501543571\n",
            "train loss:0.05873766278414769\n",
            "train loss:0.038489437486043526\n",
            "train loss:0.034320622376497606\n",
            "train loss:0.06059778102711556\n",
            "train loss:0.007935374551513538\n",
            "train loss:0.023626776720375272\n",
            "train loss:0.024682019620818053\n",
            "train loss:0.0876540710549979\n",
            "train loss:0.005258022972205263\n",
            "train loss:0.01811659955634137\n",
            "train loss:0.05400361994290996\n",
            "train loss:0.011690619279405755\n",
            "=== epoch:175, train acc:0.9925, test acc:0.919 ===\n",
            "train loss:0.0043939957565428435\n",
            "train loss:0.08104251029516629\n",
            "train loss:0.00922127683852895\n",
            "train loss:0.014158203886617239\n",
            "train loss:0.005014798704520702\n",
            "train loss:0.05343018021960235\n",
            "train loss:0.011020875335204015\n",
            "train loss:0.05430046685053527\n",
            "train loss:0.047705459295204894\n",
            "train loss:0.0016147596218685394\n",
            "train loss:0.05739633793779805\n",
            "train loss:0.008256213002785845\n",
            "train loss:0.023211811560913086\n",
            "train loss:0.105663656535742\n",
            "train loss:0.11597306460906509\n",
            "train loss:0.010604269802903552\n",
            "train loss:0.0060700774958671435\n",
            "train loss:0.046335372298462865\n",
            "train loss:0.004227782395246994\n",
            "train loss:0.03500321060092202\n",
            "train loss:0.0062138757666149015\n",
            "train loss:0.003927022485150437\n",
            "train loss:0.023625825686035825\n",
            "train loss:0.012586382405059183\n",
            "train loss:0.003841113009803562\n",
            "train loss:0.005854846832367414\n",
            "train loss:0.01521896101692388\n",
            "train loss:0.006496554873617523\n",
            "train loss:0.003352328184061065\n",
            "train loss:0.0034289335971264133\n",
            "train loss:0.004969317301329795\n",
            "train loss:0.03278831958459305\n",
            "train loss:0.050246767934112826\n",
            "train loss:0.015461966017145855\n",
            "train loss:0.01590439265376321\n",
            "train loss:0.05027591190621874\n",
            "train loss:0.008849978282562848\n",
            "train loss:0.029417565205687172\n",
            "train loss:0.04658307890568619\n",
            "train loss:0.00585114882633803\n",
            "train loss:0.01693760773023559\n",
            "train loss:0.011651244887554176\n",
            "train loss:0.007733470821972874\n",
            "train loss:0.09926468161503482\n",
            "train loss:0.06685884912824845\n",
            "train loss:0.016007545910829174\n",
            "train loss:0.008696475745694479\n",
            "train loss:0.0051285772395225785\n",
            "train loss:0.015219546214348139\n",
            "train loss:0.0015577257532409152\n",
            "train loss:0.005482809942463877\n",
            "train loss:0.006603751426756118\n",
            "train loss:0.05939537786170196\n",
            "train loss:0.004228673496808812\n",
            "train loss:0.0852067482241522\n",
            "train loss:0.010294576106297044\n",
            "train loss:0.014197080374598204\n",
            "train loss:0.0029068300134401925\n",
            "train loss:0.030323675999422078\n",
            "train loss:0.0065218694823506625\n",
            "train loss:0.08521106310582237\n",
            "train loss:0.029429397946715757\n",
            "train loss:0.00475064005041955\n",
            "train loss:0.05863193498338877\n",
            "train loss:0.010368780266328016\n",
            "train loss:0.007412291089971126\n",
            "train loss:0.062011905328492656\n",
            "train loss:0.008572000950549235\n",
            "train loss:0.007622683107427815\n",
            "train loss:0.007157131807312872\n",
            "train loss:0.0038113411533867323\n",
            "train loss:0.021649295277676794\n",
            "train loss:0.055102390435958555\n",
            "train loss:0.003277155759669448\n",
            "train loss:0.018780528498327287\n",
            "train loss:0.059849184403676946\n",
            "train loss:0.008867431937088882\n",
            "train loss:0.011222595420601897\n",
            "train loss:0.02222255513072871\n",
            "train loss:0.008386290777683287\n",
            "=== epoch:176, train acc:0.99325, test acc:0.9195 ===\n",
            "train loss:0.019386876083519217\n",
            "train loss:0.05861699462764393\n",
            "train loss:0.004816649709657272\n",
            "train loss:0.030708727483623025\n",
            "train loss:0.004774743667166906\n",
            "train loss:0.03508230593797718\n",
            "train loss:0.004281178483261836\n",
            "train loss:0.010946371460451222\n",
            "train loss:0.004755206608180155\n",
            "train loss:0.005895000121770661\n",
            "train loss:0.004232195914932548\n",
            "train loss:0.056881181204100885\n",
            "train loss:0.08138961395221479\n",
            "train loss:0.05228681692681845\n",
            "train loss:0.00629672439114542\n",
            "train loss:0.026556964349934025\n",
            "train loss:0.06594678020585701\n",
            "train loss:0.022884070869978467\n",
            "train loss:0.022547128354613335\n",
            "train loss:0.057244146388886016\n",
            "train loss:0.005868764465976915\n",
            "train loss:0.016969908751403487\n",
            "train loss:0.061665399647079205\n",
            "train loss:0.022644168432378046\n",
            "train loss:0.04557993563158076\n",
            "train loss:0.034468210210658906\n",
            "train loss:0.010804723736508846\n",
            "train loss:0.007538757947671406\n",
            "train loss:0.009699430250454815\n",
            "train loss:0.007059538607020847\n",
            "train loss:0.015335934769064316\n",
            "train loss:0.03760793196570485\n",
            "train loss:0.03349187198453376\n",
            "train loss:0.04527423207383118\n",
            "train loss:0.04149620597279391\n",
            "train loss:0.033363300617300376\n",
            "train loss:0.006768674911865085\n",
            "train loss:0.054065435926776324\n",
            "train loss:0.010957864525031582\n",
            "train loss:0.005263704817934778\n",
            "train loss:0.021251451522878235\n",
            "train loss:0.006135590442438793\n",
            "train loss:0.05924892609389576\n",
            "train loss:0.03280909180550008\n",
            "train loss:0.0030442522018819494\n",
            "train loss:0.014890725246671172\n",
            "train loss:0.020539362623949953\n",
            "train loss:0.004582496764636446\n",
            "train loss:0.014829141496225308\n",
            "train loss:0.03353997464419739\n",
            "train loss:0.004207647469393378\n",
            "train loss:0.03146435109050491\n",
            "train loss:0.049150902718162685\n",
            "train loss:0.00555104688834366\n",
            "train loss:0.003688559966639064\n",
            "train loss:0.008659021376618866\n",
            "train loss:0.016674615979718258\n",
            "train loss:0.15140007771041528\n",
            "train loss:0.0026101855338446695\n",
            "train loss:0.009397379022060335\n",
            "train loss:0.017203566219822185\n",
            "train loss:0.007416490745070491\n",
            "train loss:0.006791784514806772\n",
            "train loss:0.0540856191123427\n",
            "train loss:0.036774929355708454\n",
            "train loss:0.007731702568853082\n",
            "train loss:0.026856836988248436\n",
            "train loss:0.0014870615339058234\n",
            "train loss:0.0066035679164166685\n",
            "train loss:0.08478939775167225\n",
            "train loss:0.014779163485957978\n",
            "train loss:0.0024619016961350184\n",
            "train loss:0.007162962652390471\n",
            "train loss:0.03851627140308576\n",
            "train loss:0.006332133533019343\n",
            "train loss:0.1449241698152763\n",
            "train loss:0.07851352735369346\n",
            "train loss:0.00896277726939773\n",
            "train loss:0.006736108637885375\n",
            "train loss:0.030717917549261856\n",
            "=== epoch:177, train acc:0.99325, test acc:0.918 ===\n",
            "train loss:0.03555460831590195\n",
            "train loss:0.05391309109008341\n",
            "train loss:0.04628316799527456\n",
            "train loss:0.017278488122063944\n",
            "train loss:0.028142961933349895\n",
            "train loss:0.004563095570354803\n",
            "train loss:0.026165414724383444\n",
            "train loss:0.006439117201009925\n",
            "train loss:0.022193354702114164\n",
            "train loss:0.02540837685822499\n",
            "train loss:0.00927810820002732\n",
            "train loss:0.01749761425388303\n",
            "train loss:0.020822549662828812\n",
            "train loss:0.012669640229190588\n",
            "train loss:0.008822740547629618\n",
            "train loss:0.018011975631669508\n",
            "train loss:0.018390220164722616\n",
            "train loss:0.026862020087448667\n",
            "train loss:0.005347596986247508\n",
            "train loss:0.02814946281692571\n",
            "train loss:0.0036748762212800074\n",
            "train loss:0.004562737488846513\n",
            "train loss:0.04712683210965389\n",
            "train loss:0.014200030324547148\n",
            "train loss:0.03142523318496945\n",
            "train loss:0.008819156723467557\n",
            "train loss:0.01603887651835401\n",
            "train loss:0.03700288936139382\n",
            "train loss:0.017589969176287558\n",
            "train loss:0.03257504089953798\n",
            "train loss:0.004009785661449253\n",
            "train loss:0.06590837012313713\n",
            "train loss:0.006942173861797304\n",
            "train loss:0.007782207595308507\n",
            "train loss:0.013752663049894103\n",
            "train loss:0.006600380150983074\n",
            "train loss:0.00490958047917427\n",
            "train loss:0.072018885663493\n",
            "train loss:0.10017444707491654\n",
            "train loss:0.003706532017077467\n",
            "train loss:0.004531834827249218\n",
            "train loss:0.005412470555239243\n",
            "train loss:0.005194073481044441\n",
            "train loss:0.030521514476986044\n",
            "train loss:0.005868638747583751\n",
            "train loss:0.005945633732434306\n",
            "train loss:0.011597141244128669\n",
            "train loss:0.017466511084664082\n",
            "train loss:0.012214361269762863\n",
            "train loss:0.005375760800703734\n",
            "train loss:0.007118415471985925\n",
            "train loss:0.006145468672379996\n",
            "train loss:0.03603192831167665\n",
            "train loss:0.025212953560245065\n",
            "train loss:0.03503649963043008\n",
            "train loss:0.02350199729913347\n",
            "train loss:0.006476239202632302\n",
            "train loss:0.014621744394008868\n",
            "train loss:0.03675332117552587\n",
            "train loss:0.03725464563451584\n",
            "train loss:0.010443122046538928\n",
            "train loss:0.012418393129469978\n",
            "train loss:0.01511953764455381\n",
            "train loss:0.030166845307813363\n",
            "train loss:0.003865726134411008\n",
            "train loss:0.02350118139053422\n",
            "train loss:0.04936365790319727\n",
            "train loss:0.006425605158824258\n",
            "train loss:0.004009049931015701\n",
            "train loss:0.013766240417128726\n",
            "train loss:0.004919732104676434\n",
            "train loss:0.009498656508004254\n",
            "train loss:0.012843099796152476\n",
            "train loss:0.018569944826082308\n",
            "train loss:0.004844104238342787\n",
            "train loss:0.021561751327503514\n",
            "train loss:0.05376987801999197\n",
            "train loss:0.0019643918447877014\n",
            "train loss:0.0038362710765760728\n",
            "train loss:0.06039040346280943\n",
            "=== epoch:178, train acc:0.99325, test acc:0.919 ===\n",
            "train loss:0.005871299353928351\n",
            "train loss:0.004525834922186578\n",
            "train loss:0.020378312308826543\n",
            "train loss:0.004205070852060973\n",
            "train loss:0.005843868880348985\n",
            "train loss:0.017033569543982507\n",
            "train loss:0.02802120428170915\n",
            "train loss:0.004573826736387763\n",
            "train loss:0.008745582783303422\n",
            "train loss:0.004824864401790314\n",
            "train loss:0.0058577681721748735\n",
            "train loss:0.0020831175335664235\n",
            "train loss:0.008848397149703285\n",
            "train loss:0.00594101829481392\n",
            "train loss:0.020905569621981507\n",
            "train loss:0.02253045518914779\n",
            "train loss:0.06782505319121782\n",
            "train loss:0.022984980290363465\n",
            "train loss:0.023336282062496005\n",
            "train loss:0.005643208263902826\n",
            "train loss:0.023939171493631662\n",
            "train loss:0.011349080087975132\n",
            "train loss:0.005287775562962487\n",
            "train loss:0.033623774728067615\n",
            "train loss:0.02582036590127171\n",
            "train loss:0.030935640178319432\n",
            "train loss:0.0035996653221585452\n",
            "train loss:0.024708503239320116\n",
            "train loss:0.008040782274909595\n",
            "train loss:0.05959376385255283\n",
            "train loss:0.043389279953248615\n",
            "train loss:0.004182381993931905\n",
            "train loss:0.02277342316775592\n",
            "train loss:0.0014522069351699165\n",
            "train loss:0.023028300262530648\n",
            "train loss:0.00262918852438863\n",
            "train loss:0.027525265148320596\n",
            "train loss:0.014390497212194351\n",
            "train loss:0.008736625170039772\n",
            "train loss:0.0807938165217365\n",
            "train loss:0.03804298925530712\n",
            "train loss:0.03719982036982476\n",
            "train loss:0.006274036559703943\n",
            "train loss:0.01684308646241398\n",
            "train loss:0.04206884154194422\n",
            "train loss:0.00787627751828403\n",
            "train loss:0.014269286321777592\n",
            "train loss:0.0165985518176122\n",
            "train loss:0.003902553489798003\n",
            "train loss:0.006940202301199438\n",
            "train loss:0.030719244528507855\n",
            "train loss:0.007515716317546579\n",
            "train loss:0.012703664374233637\n",
            "train loss:0.004797009824386194\n",
            "train loss:0.005289044912128303\n",
            "train loss:0.04680108076507795\n",
            "train loss:0.03754397654381053\n",
            "train loss:0.005212067705423124\n",
            "train loss:0.0286449276800561\n",
            "train loss:0.026609442312400436\n",
            "train loss:0.002207228270606245\n",
            "train loss:0.02166868811015369\n",
            "train loss:0.004341056079653664\n",
            "train loss:0.005774145911576649\n",
            "train loss:0.023654269478907346\n",
            "train loss:0.027924656226259854\n",
            "train loss:0.07991415292780951\n",
            "train loss:0.025506811856788845\n",
            "train loss:0.037528670303594865\n",
            "train loss:0.054841686217074755\n",
            "train loss:0.02327323142331363\n",
            "train loss:0.02859460399443134\n",
            "train loss:0.004038553063049424\n",
            "train loss:0.006393485525271133\n",
            "train loss:0.01443638865241604\n",
            "train loss:0.002153117056390455\n",
            "train loss:0.004279468910697261\n",
            "train loss:0.0058243797717788945\n",
            "train loss:0.0415963575545655\n",
            "train loss:0.01400987062627652\n",
            "=== epoch:179, train acc:0.99325, test acc:0.9205 ===\n",
            "train loss:0.0031829824403668432\n",
            "train loss:0.012907302418072777\n",
            "train loss:0.05221332380446371\n",
            "train loss:0.028217143251704265\n",
            "train loss:0.008569885677386715\n",
            "train loss:0.0648537357833081\n",
            "train loss:0.02428345588984151\n",
            "train loss:0.07281789630670353\n",
            "train loss:0.018247007556299343\n",
            "train loss:0.03914334559342781\n",
            "train loss:0.002701498297130293\n",
            "train loss:0.004489828234520347\n",
            "train loss:0.0874860180461516\n",
            "train loss:0.020950560912656062\n",
            "train loss:0.006338013768659707\n",
            "train loss:0.048578413962857336\n",
            "train loss:0.004846529736276694\n",
            "train loss:0.01864208620531442\n",
            "train loss:0.04824553403863077\n",
            "train loss:0.00903232378476448\n",
            "train loss:0.003819647259136941\n",
            "train loss:0.040782547366621465\n",
            "train loss:0.011171880358065172\n",
            "train loss:0.0431330459343712\n",
            "train loss:0.016051019180190668\n",
            "train loss:0.03977315484665599\n",
            "train loss:0.0054682622741429425\n",
            "train loss:0.02418677162155812\n",
            "train loss:0.003581259969700246\n",
            "train loss:0.007788854925239176\n",
            "train loss:0.010269346611105097\n",
            "train loss:0.051889271675801873\n",
            "train loss:0.03931657450575992\n",
            "train loss:0.002967446896513739\n",
            "train loss:0.01661001140260978\n",
            "train loss:0.12657414863048672\n",
            "train loss:0.011159153349954766\n",
            "train loss:0.022709505378957707\n",
            "train loss:0.011712895227245888\n",
            "train loss:0.009797085576085647\n",
            "train loss:0.02626853302733726\n",
            "train loss:0.003106750016238778\n",
            "train loss:0.0070329888364031945\n",
            "train loss:0.028797107724092285\n",
            "train loss:0.009864780422936951\n",
            "train loss:0.04480749810149109\n",
            "train loss:0.03751430863742691\n",
            "train loss:0.004867856382874137\n",
            "train loss:0.03899292557653194\n",
            "train loss:0.016637796833295086\n",
            "train loss:0.004116756360967138\n",
            "train loss:0.058425171082839275\n",
            "train loss:0.0032086436665916852\n",
            "train loss:0.026741118003866683\n",
            "train loss:0.01099060976794161\n",
            "train loss:0.012232125381463533\n",
            "train loss:0.0089438654645832\n",
            "train loss:0.059162153520621305\n",
            "train loss:0.02294921609605234\n",
            "train loss:0.008751668392469481\n",
            "train loss:0.022902111533339525\n",
            "train loss:0.011714981913424613\n",
            "train loss:0.009124190565698518\n",
            "train loss:0.005731707877779851\n",
            "train loss:0.003915550013997215\n",
            "train loss:0.01667228652970365\n",
            "train loss:0.002523289985288775\n",
            "train loss:0.008125543405951485\n",
            "train loss:0.012534794853002152\n",
            "train loss:0.007916027682442671\n",
            "train loss:0.003812775586723042\n",
            "train loss:0.02454743294062905\n",
            "train loss:0.055405223500920815\n",
            "train loss:0.052917156280418355\n",
            "train loss:0.02693285525159304\n",
            "train loss:0.08294997165762606\n",
            "train loss:0.004405403146645108\n",
            "train loss:0.005181643038077118\n",
            "train loss:0.008864838709218933\n",
            "train loss:0.023052751811428504\n",
            "=== epoch:180, train acc:0.993625, test acc:0.9195 ===\n",
            "train loss:0.03333284578374642\n",
            "train loss:0.016992183561976695\n",
            "train loss:0.03486642263017839\n",
            "train loss:0.0054261159246194405\n",
            "train loss:0.004039720282461933\n",
            "train loss:0.0160376771537645\n",
            "train loss:0.06723480169193312\n",
            "train loss:0.07316224757996591\n",
            "train loss:0.010498954474882465\n",
            "train loss:0.015899921925676994\n",
            "train loss:0.0045550490376410156\n",
            "train loss:0.008588574621376221\n",
            "train loss:0.008519175359030825\n",
            "train loss:0.023393327607007023\n",
            "train loss:0.002130446112144817\n",
            "train loss:0.011505606261388907\n",
            "train loss:0.026541254091644855\n",
            "train loss:0.004671386499824384\n",
            "train loss:0.05672686487907379\n",
            "train loss:0.013613549234143\n",
            "train loss:0.011918841744418967\n",
            "train loss:0.002818643720620969\n",
            "train loss:0.007905263443967421\n",
            "train loss:0.004424434551310819\n",
            "train loss:0.008569673146061768\n",
            "train loss:0.004034163382439113\n",
            "train loss:0.00571564643649037\n",
            "train loss:0.015517000579867997\n",
            "train loss:0.048064026059061335\n",
            "train loss:0.004579675733146504\n",
            "train loss:0.0012080288114524856\n",
            "train loss:0.0045458245734448805\n",
            "train loss:0.016103032294392982\n",
            "train loss:0.00408978265457206\n",
            "train loss:0.0036971351884719683\n",
            "train loss:0.0017133906785125746\n",
            "train loss:0.04178883787663597\n",
            "train loss:0.014763314529068332\n",
            "train loss:0.023820016833315028\n",
            "train loss:0.006976994951408993\n",
            "train loss:0.003954836357501838\n",
            "train loss:0.02973125651359697\n",
            "train loss:0.0045046001970823046\n",
            "train loss:0.03085648972248851\n",
            "train loss:0.007164012201909474\n",
            "train loss:0.008475110047363347\n",
            "train loss:0.005669695936026533\n",
            "train loss:0.05115419688431331\n",
            "train loss:0.07525595262249549\n",
            "train loss:0.006009099134034171\n",
            "train loss:0.01553361788330987\n",
            "train loss:0.007671980047439286\n",
            "train loss:0.004156768828094271\n",
            "train loss:0.01233648824416304\n",
            "train loss:0.08308028426203345\n",
            "train loss:0.0458075492157239\n",
            "train loss:0.0034921768622193437\n",
            "train loss:0.006313781773517117\n",
            "train loss:0.012741894444287806\n",
            "train loss:0.004187186754193567\n",
            "train loss:0.09830378214346945\n",
            "train loss:0.012695005618652699\n",
            "train loss:0.030514696576066046\n",
            "train loss:0.012705889397310509\n",
            "train loss:0.011757582842351106\n",
            "train loss:0.07549958733859685\n",
            "train loss:0.003638087975104754\n",
            "train loss:0.011767522004739581\n",
            "train loss:0.05657394892624952\n",
            "train loss:0.04852870445266595\n",
            "train loss:0.020362842543113793\n",
            "train loss:0.010059901859491203\n",
            "train loss:0.021881468060931414\n",
            "train loss:0.023091076231213318\n",
            "train loss:0.017888632873437584\n",
            "train loss:0.008101544658420978\n",
            "train loss:0.031440554609520166\n",
            "train loss:0.0668127135953179\n",
            "train loss:0.07219214266655949\n",
            "train loss:0.014696072514652693\n",
            "=== epoch:181, train acc:0.993875, test acc:0.9195 ===\n",
            "train loss:0.05191785867584144\n",
            "train loss:0.005102898383723602\n",
            "train loss:0.005245949589349034\n",
            "train loss:0.010194926742914348\n",
            "train loss:0.005499964286768186\n",
            "train loss:0.0019054199155539688\n",
            "train loss:0.044958043858801394\n",
            "train loss:0.03386920260491006\n",
            "train loss:0.02040227527444429\n",
            "train loss:0.05477421707107864\n",
            "train loss:0.006778424656810739\n",
            "train loss:0.004274869087809646\n",
            "train loss:0.05119631325276872\n",
            "train loss:0.008546912359654123\n",
            "train loss:0.025804428649764243\n",
            "train loss:0.004310972086331373\n",
            "train loss:0.039013131337879245\n",
            "train loss:0.0068708928733952545\n",
            "train loss:0.003926634009835344\n",
            "train loss:0.01718757394111971\n",
            "train loss:0.05575076064653918\n",
            "train loss:0.045530983133757925\n",
            "train loss:0.021107627039208193\n",
            "train loss:0.04476076185896813\n",
            "train loss:0.0049454985481994975\n",
            "train loss:0.01396627652721309\n",
            "train loss:0.005430979440413029\n",
            "train loss:0.005312391023694374\n",
            "train loss:0.012967132761576471\n",
            "train loss:0.035349231694357296\n",
            "train loss:0.006992654521219174\n",
            "train loss:0.011506971515286901\n",
            "train loss:0.02194446090782579\n",
            "train loss:0.002699147125008085\n",
            "train loss:0.00754668628718999\n",
            "train loss:0.044822423690437097\n",
            "train loss:0.01043255463459739\n",
            "train loss:0.01244991294681631\n",
            "train loss:0.00877287407404291\n",
            "train loss:0.047384193744841235\n",
            "train loss:0.011522765353322008\n",
            "train loss:0.023827558585287654\n",
            "train loss:0.0018941906169132564\n",
            "train loss:0.03766439280004881\n",
            "train loss:0.015261658168828723\n",
            "train loss:0.004575789920746407\n",
            "train loss:0.05126130601242743\n",
            "train loss:0.0550805256824876\n",
            "train loss:0.007277133991345239\n",
            "train loss:0.040386221827485115\n",
            "train loss:0.004925102597657435\n",
            "train loss:0.05980852050045791\n",
            "train loss:0.0028472743812583\n",
            "train loss:0.0038699952681051407\n",
            "train loss:0.037257923872499876\n",
            "train loss:0.054251296661874955\n",
            "train loss:0.006963320863583957\n",
            "train loss:0.051620692744214054\n",
            "train loss:0.0840552933750038\n",
            "train loss:0.017250576325592685\n",
            "train loss:0.0036052603367668925\n",
            "train loss:0.0025504608326789076\n",
            "train loss:0.005178077618129409\n",
            "train loss:0.015877359948106948\n",
            "train loss:0.014347791373608844\n",
            "train loss:0.04663006695802598\n",
            "train loss:0.009048433552751358\n",
            "train loss:0.004225769249500533\n",
            "train loss:0.005953579712354812\n",
            "train loss:0.03653215668413978\n",
            "train loss:0.027730942384178173\n",
            "train loss:0.0026460512090820346\n",
            "train loss:0.006811277262066682\n",
            "train loss:0.008937746297667836\n",
            "train loss:0.009743933933590417\n",
            "train loss:0.049018726854175146\n",
            "train loss:0.027370770073034283\n",
            "train loss:0.019963304918620688\n",
            "train loss:0.015884603518394456\n",
            "train loss:0.00861343979452557\n",
            "=== epoch:182, train acc:0.993875, test acc:0.9225 ===\n",
            "train loss:0.04919143816960018\n",
            "train loss:0.01035172050056462\n",
            "train loss:0.0038091819685272233\n",
            "train loss:0.010921228433076849\n",
            "train loss:0.012894487749193948\n",
            "train loss:0.00953570401580134\n",
            "train loss:0.020040609248669957\n",
            "train loss:0.06492327107217685\n",
            "train loss:0.013337359311033949\n",
            "train loss:0.014467027423754249\n",
            "train loss:0.010886630466281735\n",
            "train loss:0.12422426969792832\n",
            "train loss:0.0055190765721549314\n",
            "train loss:0.031568041415219426\n",
            "train loss:0.05122755946580317\n",
            "train loss:0.009225806769071181\n",
            "train loss:0.007571987765318945\n",
            "train loss:0.0025941485846885624\n",
            "train loss:0.014087817876328165\n",
            "train loss:0.010593099563092916\n",
            "train loss:0.017726567249625003\n",
            "train loss:0.03671823948889469\n",
            "train loss:0.04342872245519363\n",
            "train loss:0.041208855863832536\n",
            "train loss:0.016152484704200328\n",
            "train loss:0.024205287230752104\n",
            "train loss:0.04240039485273799\n",
            "train loss:0.037253937639488255\n",
            "train loss:0.0060744277904938495\n",
            "train loss:0.03634077717684134\n",
            "train loss:0.021657819270669756\n",
            "train loss:0.02349087825912326\n",
            "train loss:0.003563720171450576\n",
            "train loss:0.03181477878172648\n",
            "train loss:0.04985423203966453\n",
            "train loss:0.0041529491940807605\n",
            "train loss:0.03133975959433981\n",
            "train loss:0.003386690036860998\n",
            "train loss:0.007640443278545817\n",
            "train loss:0.028440161226307912\n",
            "train loss:0.011761037935012661\n",
            "train loss:0.01610426178849332\n",
            "train loss:0.02472727071425026\n",
            "train loss:0.005755829414246393\n",
            "train loss:0.0012987332677226971\n",
            "train loss:0.05321296751983131\n",
            "train loss:0.038796110954850575\n",
            "train loss:0.0071090084286413975\n",
            "train loss:0.004751795582885086\n",
            "train loss:0.026777809745042815\n",
            "train loss:0.002019166164553078\n",
            "train loss:0.021700428525562487\n",
            "train loss:0.002994785260534238\n",
            "train loss:0.003685822119152871\n",
            "train loss:0.00999002997009099\n",
            "train loss:0.023629153006026447\n",
            "train loss:0.009661047105224281\n",
            "train loss:0.009847418263290744\n",
            "train loss:0.040488274891589085\n",
            "train loss:0.009901466394248735\n",
            "train loss:0.053508949909929236\n",
            "train loss:0.0033467007158274236\n",
            "train loss:0.004118432784653986\n",
            "train loss:0.023865330138154776\n",
            "train loss:0.009020453778216498\n",
            "train loss:0.010491603908239213\n",
            "train loss:0.0025857856904555137\n",
            "train loss:0.0067262116077982746\n",
            "train loss:0.04193349455309559\n",
            "train loss:0.010599382917999728\n",
            "train loss:0.019203225612181418\n",
            "train loss:0.004504562458386618\n",
            "train loss:0.010447047762959467\n",
            "train loss:0.0054988031542495975\n",
            "train loss:0.009027114168925418\n",
            "train loss:0.03667407480343245\n",
            "train loss:0.008937878175903363\n",
            "train loss:0.003043447094726509\n",
            "train loss:0.07535992786517953\n",
            "train loss:0.0014263476385742327\n",
            "=== epoch:183, train acc:0.99375, test acc:0.9215 ===\n",
            "train loss:0.05957638007194805\n",
            "train loss:0.022629080842095185\n",
            "train loss:0.008124552924079513\n",
            "train loss:0.0028960209944339506\n",
            "train loss:0.002474181761373513\n",
            "train loss:0.0040036606487075885\n",
            "train loss:0.018930423260122203\n",
            "train loss:0.033278500395596274\n",
            "train loss:0.0166534815477513\n",
            "train loss:0.0031816753229729267\n",
            "train loss:0.031142668184858335\n",
            "train loss:0.009568793479817542\n",
            "train loss:0.01519417124335213\n",
            "train loss:0.00382861781424528\n",
            "train loss:0.009631317867944727\n",
            "train loss:0.004140263334569943\n",
            "train loss:0.005947581639347736\n",
            "train loss:0.02282139452113656\n",
            "train loss:0.01777251915035144\n",
            "train loss:0.007405644732001226\n",
            "train loss:0.023398552888706455\n",
            "train loss:0.01484377563799336\n",
            "train loss:0.0038602683093059108\n",
            "train loss:0.007062701238834822\n",
            "train loss:0.005613479736280363\n",
            "train loss:0.009372858060734506\n",
            "train loss:0.03367167194451065\n",
            "train loss:0.023609509085581137\n",
            "train loss:0.0031750748995831484\n",
            "train loss:0.009144383461958833\n",
            "train loss:0.004968328561367912\n",
            "train loss:0.10440422003210034\n",
            "train loss:0.007891710167486705\n",
            "train loss:0.011928127981216015\n",
            "train loss:0.07150896443901054\n",
            "train loss:0.04746346538139438\n",
            "train loss:0.0034313336657613176\n",
            "train loss:0.026854076140192133\n",
            "train loss:0.029149181892863196\n",
            "train loss:0.00338170489503626\n",
            "train loss:0.004239938997235105\n",
            "train loss:0.03794615373326074\n",
            "train loss:0.005645655694555737\n",
            "train loss:0.0035882921181889984\n",
            "train loss:0.002760044812387529\n",
            "train loss:0.0045228523341284\n",
            "train loss:0.008319620408923529\n",
            "train loss:0.014001896667425956\n",
            "train loss:0.002069923879842598\n",
            "train loss:0.015431739612932302\n",
            "train loss:0.007668677601736998\n",
            "train loss:0.006312475209026432\n",
            "train loss:0.008817556984598844\n",
            "train loss:0.0072410694109979025\n",
            "train loss:0.022470680406190033\n",
            "train loss:0.002904324566964037\n",
            "train loss:0.003495562188418026\n",
            "train loss:0.008878294152276527\n",
            "train loss:0.0033196572753580876\n",
            "train loss:0.055544312102424706\n",
            "train loss:0.007499408955263886\n",
            "train loss:0.004522881584598664\n",
            "train loss:0.046532265310015045\n",
            "train loss:0.004839620444847806\n",
            "train loss:0.0018346937507249471\n",
            "train loss:0.007609927976658005\n",
            "train loss:0.012465050999414473\n",
            "train loss:0.005554182769471985\n",
            "train loss:0.061517024751455526\n",
            "train loss:0.029808295506368807\n",
            "train loss:0.012097337425731255\n",
            "train loss:0.009548413036827064\n",
            "train loss:0.08722821147435227\n",
            "train loss:0.0057142525625022665\n",
            "train loss:0.0031934154240199466\n",
            "train loss:0.019281640034114027\n",
            "train loss:0.007251825770193839\n",
            "train loss:0.008118920591110261\n",
            "train loss:0.009313561001185583\n",
            "train loss:0.003842034624648579\n",
            "=== epoch:184, train acc:0.99375, test acc:0.922 ===\n",
            "train loss:0.0032801916708226665\n",
            "train loss:0.0512101773241125\n",
            "train loss:0.014404385011578417\n",
            "train loss:0.006265033553761916\n",
            "train loss:0.007905156012887415\n",
            "train loss:0.00841522909303967\n",
            "train loss:0.011700448519835366\n",
            "train loss:0.02813164947029501\n",
            "train loss:0.0033157960765559935\n",
            "train loss:0.0045322415236804595\n",
            "train loss:0.008413993241791961\n",
            "train loss:0.037038962424842996\n",
            "train loss:0.0029229098313938613\n",
            "train loss:0.015834094975069535\n",
            "train loss:0.012316319959820911\n",
            "train loss:0.028693142567814453\n",
            "train loss:0.015300368474722605\n",
            "train loss:0.0198240174502858\n",
            "train loss:0.0033915823043328104\n",
            "train loss:0.010380851868004326\n",
            "train loss:0.0032658571687896386\n",
            "train loss:0.00853331781000105\n",
            "train loss:0.019288611401931922\n",
            "train loss:0.010455220320976193\n",
            "train loss:0.015111507991388762\n",
            "train loss:0.012931665600612098\n",
            "train loss:0.05739403578508704\n",
            "train loss:0.0025040204662818465\n",
            "train loss:0.004035324505456687\n",
            "train loss:0.0173638384081579\n",
            "train loss:0.004422491805164539\n",
            "train loss:0.04401291260544285\n",
            "train loss:0.0070457961974378525\n",
            "train loss:0.08572576522446237\n",
            "train loss:0.03191402467793102\n",
            "train loss:0.04634331188612995\n",
            "train loss:0.036668191870731\n",
            "train loss:0.005870701886306256\n",
            "train loss:0.11909461952363148\n",
            "train loss:0.04143197398051375\n",
            "train loss:0.00815779475075469\n",
            "train loss:0.02858305287349077\n",
            "train loss:0.017870390831848174\n",
            "train loss:0.04656839825533556\n",
            "train loss:0.006030454434134537\n",
            "train loss:0.07930910125856659\n",
            "train loss:0.039581341494492446\n",
            "train loss:0.0079475932284986\n",
            "train loss:0.005772917104552293\n",
            "train loss:0.06021073329325016\n",
            "train loss:0.0072493337305999375\n",
            "train loss:0.03146119800304312\n",
            "train loss:0.00501997795526725\n",
            "train loss:0.012295437075478047\n",
            "train loss:0.0493603274957601\n",
            "train loss:0.011097557456211087\n",
            "train loss:0.02282977824724984\n",
            "train loss:0.005778319650682343\n",
            "train loss:0.009221151344236013\n",
            "train loss:0.004668938981634041\n",
            "train loss:0.004380925861644557\n",
            "train loss:0.0034179413809951394\n",
            "train loss:0.006615696107533363\n",
            "train loss:0.017075880124607618\n",
            "train loss:0.013636501821431734\n",
            "train loss:0.03606581509745819\n",
            "train loss:0.023676336052912222\n",
            "train loss:0.0031601428827280377\n",
            "train loss:0.003998124687793671\n",
            "train loss:0.029014157672749317\n",
            "train loss:0.035504476692303\n",
            "train loss:0.044687508862965755\n",
            "train loss:0.011549903274625135\n",
            "train loss:0.03230788570168446\n",
            "train loss:0.004072247230519082\n",
            "train loss:0.003726111806073862\n",
            "train loss:0.005917181664897582\n",
            "train loss:0.011577230133318099\n",
            "train loss:0.008631221025892892\n",
            "train loss:0.027793549391758345\n",
            "=== epoch:185, train acc:0.993625, test acc:0.9205 ===\n",
            "train loss:0.009662120716897597\n",
            "train loss:0.00733956051041949\n",
            "train loss:0.005485235349109045\n",
            "train loss:0.002803760422447188\n",
            "train loss:0.02322189261998436\n",
            "train loss:0.057634666635723766\n",
            "train loss:0.036131882967864196\n",
            "train loss:0.025512240820527703\n",
            "train loss:0.014853966331319206\n",
            "train loss:0.01275588885450143\n",
            "train loss:0.04983448037212161\n",
            "train loss:0.004185852098945423\n",
            "train loss:0.006277516970229942\n",
            "train loss:0.03810323411366223\n",
            "train loss:0.004378968621328932\n",
            "train loss:0.023722530493407425\n",
            "train loss:0.0031506143407335064\n",
            "train loss:0.031042830504457598\n",
            "train loss:0.04084256524228251\n",
            "train loss:0.004569707241455556\n",
            "train loss:0.008090639903772137\n",
            "train loss:0.03002297176220187\n",
            "train loss:0.008746058805756064\n",
            "train loss:0.004322925762388573\n",
            "train loss:0.005090384386506279\n",
            "train loss:0.007154323166454955\n",
            "train loss:0.03442400931337871\n",
            "train loss:0.023522893007286758\n",
            "train loss:0.0058855588893191755\n",
            "train loss:0.037532362249928335\n",
            "train loss:0.017553236367121784\n",
            "train loss:0.012972876876905324\n",
            "train loss:0.003824724953291368\n",
            "train loss:0.015344206898770052\n",
            "train loss:0.0032958320108422904\n",
            "train loss:0.00434353816748049\n",
            "train loss:0.009565634174787997\n",
            "train loss:0.004319380137627168\n",
            "train loss:0.04391487334417493\n",
            "train loss:0.07275482324504294\n",
            "train loss:0.004899965154120761\n",
            "train loss:0.01704710400910333\n",
            "train loss:0.004200533567941991\n",
            "train loss:0.05358807362984066\n",
            "train loss:0.018799144228293173\n",
            "train loss:0.00791659834656708\n",
            "train loss:0.008523298298828457\n",
            "train loss:0.006139567876189727\n",
            "train loss:0.02068845020533817\n",
            "train loss:0.041693805170293\n",
            "train loss:0.025729919337609\n",
            "train loss:0.006606107883611074\n",
            "train loss:0.007804435835016655\n",
            "train loss:0.009120515081343138\n",
            "train loss:0.0032386334397491333\n",
            "train loss:0.01564720982529485\n",
            "train loss:0.010995337454018402\n",
            "train loss:0.032586861922597764\n",
            "train loss:0.005722573331842562\n",
            "train loss:0.029588894544528774\n",
            "train loss:0.007057935801174063\n",
            "train loss:0.01849197408937224\n",
            "train loss:0.121089891237455\n",
            "train loss:0.05872024839723475\n",
            "train loss:0.004859281465071721\n",
            "train loss:0.0037789005843496975\n",
            "train loss:0.0043330850676375305\n",
            "train loss:0.003248447775283281\n",
            "train loss:0.024310050472675603\n",
            "train loss:0.006920424659394579\n",
            "train loss:0.0068530030403673925\n",
            "train loss:0.03513261410758356\n",
            "train loss:0.021145048886190967\n",
            "train loss:0.01950095060408803\n",
            "train loss:0.0065974456575067265\n",
            "train loss:0.020112859650432354\n",
            "train loss:0.006430451667197547\n",
            "train loss:0.013569372588186452\n",
            "train loss:0.002872376390865944\n",
            "train loss:0.02958263497871302\n",
            "=== epoch:186, train acc:0.9945, test acc:0.9225 ===\n",
            "train loss:0.0071375036541021075\n",
            "train loss:0.0057971567145443095\n",
            "train loss:0.007001743867581877\n",
            "train loss:0.008166434773254676\n",
            "train loss:0.028353036172213953\n",
            "train loss:0.006164038410585016\n",
            "train loss:0.0030722476635740457\n",
            "train loss:0.006982404640527216\n",
            "train loss:0.023430576638361303\n",
            "train loss:0.012036994045299105\n",
            "train loss:0.01108526664664579\n",
            "train loss:0.005286135581678508\n",
            "train loss:0.010110740030515139\n",
            "train loss:0.008273873541547034\n",
            "train loss:0.0059693711154373075\n",
            "train loss:0.00458016086923867\n",
            "train loss:0.00991393503881749\n",
            "train loss:0.032742327411162855\n",
            "train loss:0.007902073580126972\n",
            "train loss:0.032629118078217494\n",
            "train loss:0.008846774930988887\n",
            "train loss:0.004131453972816261\n",
            "train loss:0.013309859816181937\n",
            "train loss:0.010329141720487933\n",
            "train loss:0.0045825172691802175\n",
            "train loss:0.016982992939405927\n",
            "train loss:0.0038910270959969344\n",
            "train loss:0.043453496018526175\n",
            "train loss:0.013542484060733977\n",
            "train loss:0.01506910842602458\n",
            "train loss:0.006099258393976142\n",
            "train loss:0.008357120247004204\n",
            "train loss:0.0611150937789408\n",
            "train loss:0.01156693061501778\n",
            "train loss:0.003227928657021968\n",
            "train loss:0.09373023293504286\n",
            "train loss:0.009988408575169692\n",
            "train loss:0.00812184860789019\n",
            "train loss:0.00225835580428387\n",
            "train loss:0.011089776646167901\n",
            "train loss:0.008344454048636008\n",
            "train loss:0.08546791333851605\n",
            "train loss:0.002148286706876769\n",
            "train loss:0.03351537210929074\n",
            "train loss:0.005345366134075415\n",
            "train loss:0.0036892093334803984\n",
            "train loss:0.05057045205639715\n",
            "train loss:0.008148791246355107\n",
            "train loss:0.08150504109976245\n",
            "train loss:0.0782167005017302\n",
            "train loss:0.004547497910671398\n",
            "train loss:0.04041734684896037\n",
            "train loss:0.010121374277145158\n",
            "train loss:0.00615173279216483\n",
            "train loss:0.0509569649218241\n",
            "train loss:0.07571570679051419\n",
            "train loss:0.036078049614615144\n",
            "train loss:0.061031729970810834\n",
            "train loss:0.005774865270700377\n",
            "train loss:0.01111115299572748\n",
            "train loss:0.004397502509337262\n",
            "train loss:0.006010945801733785\n",
            "train loss:0.008439182374274337\n",
            "train loss:0.0225458554401607\n",
            "train loss:0.04830674790405023\n",
            "train loss:0.0542806190650757\n",
            "train loss:0.005306321911253389\n",
            "train loss:0.004216757253825366\n",
            "train loss:0.04401654059608731\n",
            "train loss:0.01623114503297877\n",
            "train loss:0.004040781297363533\n",
            "train loss:0.007859525678600094\n",
            "train loss:0.002535424605042931\n",
            "train loss:0.009256533334670133\n",
            "train loss:0.005986583283345711\n",
            "train loss:0.05622202459272778\n",
            "train loss:0.00201650143782109\n",
            "train loss:0.0053479540742280085\n",
            "train loss:0.005578207633145058\n",
            "train loss:0.008244174656412093\n",
            "=== epoch:187, train acc:0.99425, test acc:0.9205 ===\n",
            "train loss:0.004162796106368031\n",
            "train loss:0.025761459200605796\n",
            "train loss:0.0481440949765603\n",
            "train loss:0.006442650906485917\n",
            "train loss:0.004223685280169293\n",
            "train loss:0.0628440439279326\n",
            "train loss:0.047341559204489035\n",
            "train loss:0.004197010071914481\n",
            "train loss:0.021166588017031866\n",
            "train loss:0.004147218074446621\n",
            "train loss:0.0054128485969385555\n",
            "train loss:0.004637111504314948\n",
            "train loss:0.0054289822103032355\n",
            "train loss:0.007695245296092793\n",
            "train loss:0.0033253874727027454\n",
            "train loss:0.03749564311713426\n",
            "train loss:0.005274253294371307\n",
            "train loss:0.002980049051661886\n",
            "train loss:0.003654569956345574\n",
            "train loss:0.008424165032419407\n",
            "train loss:0.023834441254664055\n",
            "train loss:0.01177738502326125\n",
            "train loss:0.020904492622790177\n",
            "train loss:0.004206177638874788\n",
            "train loss:0.00918832749131606\n",
            "train loss:0.0027626451715394725\n",
            "train loss:0.04007215179434726\n",
            "train loss:0.05630645496423046\n",
            "train loss:0.0025551441057328926\n",
            "train loss:0.006221779628651028\n",
            "train loss:0.036875117995008734\n",
            "train loss:0.010849908575276339\n",
            "train loss:0.003512593139779438\n",
            "train loss:0.011669381990856136\n",
            "train loss:0.015253120733230931\n",
            "train loss:0.0463300787139538\n",
            "train loss:0.01093126925297978\n",
            "train loss:0.0034783403765969627\n",
            "train loss:0.0037548058939273605\n",
            "train loss:0.003101515649658189\n",
            "train loss:0.060345330244381434\n",
            "train loss:0.0019512340057155082\n",
            "train loss:0.0037284016577056236\n",
            "train loss:0.04695285241028311\n",
            "train loss:0.001461978305443544\n",
            "train loss:0.006025395235534102\n",
            "train loss:0.017040055296397502\n",
            "train loss:0.0522493573927347\n",
            "train loss:0.004102611169797387\n",
            "train loss:0.00435321010561321\n",
            "train loss:0.003949452371472183\n",
            "train loss:0.02017646510006684\n",
            "train loss:0.0070021424739056225\n",
            "train loss:0.006656151262251464\n",
            "train loss:0.05928329962311926\n",
            "train loss:0.054301910765877634\n",
            "train loss:0.03809275695858953\n",
            "train loss:0.012395046413012872\n",
            "train loss:0.005869296489383189\n",
            "train loss:0.034611344090537254\n",
            "train loss:0.006640267237288951\n",
            "train loss:0.0026241140826977053\n",
            "train loss:0.002784224600796997\n",
            "train loss:0.0025975304143005255\n",
            "train loss:0.005063120748668779\n",
            "train loss:0.0030043005659442057\n",
            "train loss:0.04829966296697592\n",
            "train loss:0.019035555117377895\n",
            "train loss:0.002904146256503377\n",
            "train loss:0.03467152844087265\n",
            "train loss:0.05374369286017618\n",
            "train loss:0.004703474017304737\n",
            "train loss:0.0024923379599375954\n",
            "train loss:0.04313654508969799\n",
            "train loss:0.002931095164389953\n",
            "train loss:0.018939761779079255\n",
            "train loss:0.050544308436509505\n",
            "train loss:0.012048024248249672\n",
            "train loss:0.058866467527618295\n",
            "train loss:0.002551196292506156\n",
            "=== epoch:188, train acc:0.994375, test acc:0.922 ===\n",
            "train loss:0.055729384671287734\n",
            "train loss:0.04595304181121919\n",
            "train loss:0.035406764671469464\n",
            "train loss:0.032142035213065134\n",
            "train loss:0.00454423999026741\n",
            "train loss:0.006140777546165361\n",
            "train loss:0.02863580846361088\n",
            "train loss:0.015927451688796446\n",
            "train loss:0.014405560587745813\n",
            "train loss:0.00630575467937438\n",
            "train loss:0.07521915170731285\n",
            "train loss:0.005662657579899422\n",
            "train loss:0.002904280152222954\n",
            "train loss:0.025138742974470104\n",
            "train loss:0.038428836404940776\n",
            "train loss:0.004713409291019376\n",
            "train loss:0.01600399046705297\n",
            "train loss:0.0033545495446980827\n",
            "train loss:0.007496489415241918\n",
            "train loss:0.04480756188170518\n",
            "train loss:0.007870598082077226\n",
            "train loss:0.013469494101274182\n",
            "train loss:0.0028211100883429522\n",
            "train loss:0.007701547549831388\n",
            "train loss:0.06550727129829315\n",
            "train loss:0.003125338111026216\n",
            "train loss:0.0023819985167040786\n",
            "train loss:0.07680209308189379\n",
            "train loss:0.003988025876079241\n",
            "train loss:0.00510369186498548\n",
            "train loss:0.02732478838183964\n",
            "train loss:0.003950517691509713\n",
            "train loss:0.0018532170741913474\n",
            "train loss:0.0036970417595617792\n",
            "train loss:0.0025256009065532563\n",
            "train loss:0.02820120945528439\n",
            "train loss:0.003794875364865839\n",
            "train loss:0.002293426440499309\n",
            "train loss:0.024858665687048906\n",
            "train loss:0.029543358011039293\n",
            "train loss:0.00471512087531103\n",
            "train loss:0.03644464060006512\n",
            "train loss:0.03076125136258577\n",
            "train loss:0.0027098146499472276\n",
            "train loss:0.0037666363950137603\n",
            "train loss:0.014253906517240542\n",
            "train loss:0.013366361783837228\n",
            "train loss:0.0032802511954623105\n",
            "train loss:0.024659983418086347\n",
            "train loss:0.05843738791248488\n",
            "train loss:0.01662574224273582\n",
            "train loss:0.02397745260637203\n",
            "train loss:0.003311530712902607\n",
            "train loss:0.003377889041353538\n",
            "train loss:0.023609456937784507\n",
            "train loss:0.07212276356550217\n",
            "train loss:0.004507842411097172\n",
            "train loss:0.010265070565896015\n",
            "train loss:0.019836368446991024\n",
            "train loss:0.003699962921655477\n",
            "train loss:0.008164800618977664\n",
            "train loss:0.009338128150480206\n",
            "train loss:0.0037382937023675263\n",
            "train loss:0.00390869046770464\n",
            "train loss:0.005405703672793891\n",
            "train loss:0.02271429427703118\n",
            "train loss:0.019140037067343765\n",
            "train loss:0.012674304514665368\n",
            "train loss:0.0038843444379447582\n",
            "train loss:0.01798944013030581\n",
            "train loss:0.004312552031763666\n",
            "train loss:0.00628165206031892\n",
            "train loss:0.005201086612945883\n",
            "train loss:0.022880677841259228\n",
            "train loss:0.0029257096540652805\n",
            "train loss:0.028157450171400762\n",
            "train loss:0.026781150101730607\n",
            "train loss:0.0070106537756027335\n",
            "train loss:0.010919254852062016\n",
            "train loss:0.005935554655266104\n",
            "=== epoch:189, train acc:0.994625, test acc:0.923 ===\n",
            "train loss:0.002007664435697855\n",
            "train loss:0.001696087055244694\n",
            "train loss:0.005441456694725138\n",
            "train loss:0.0038976545080345064\n",
            "train loss:0.012820971737002158\n",
            "train loss:0.0264897816570401\n",
            "train loss:0.006616740981503059\n",
            "train loss:0.012569568532464626\n",
            "train loss:0.01727312890419294\n",
            "train loss:0.009294366550355242\n",
            "train loss:0.008231936920284004\n",
            "train loss:0.008371256984965873\n",
            "train loss:0.015378139414880318\n",
            "train loss:0.008156642003129344\n",
            "train loss:0.003254621527102053\n",
            "train loss:0.013977100146592319\n",
            "train loss:0.023001144927913338\n",
            "train loss:0.05302628238991141\n",
            "train loss:0.0075543105570580705\n",
            "train loss:0.005092585248696938\n",
            "train loss:0.027970480486022003\n",
            "train loss:0.0030408670582079407\n",
            "train loss:0.06471465402030356\n",
            "train loss:0.009738250318029244\n",
            "train loss:0.002602716720625047\n",
            "train loss:0.010857135603465084\n",
            "train loss:0.020931488775678683\n",
            "train loss:0.011431016150226696\n",
            "train loss:0.013339155584004073\n",
            "train loss:0.003435717083828253\n",
            "train loss:0.00805522825755023\n",
            "train loss:0.019889042166936818\n",
            "train loss:0.008735475961942447\n",
            "train loss:0.008263397996969395\n",
            "train loss:0.030768984161886295\n",
            "train loss:0.0037131863674125486\n",
            "train loss:0.0018960129413306468\n",
            "train loss:0.05447837025453024\n",
            "train loss:0.004958311881273602\n",
            "train loss:0.005696002683004384\n",
            "train loss:0.005237590297069379\n",
            "train loss:0.006941759102480184\n",
            "train loss:0.01886967697154958\n",
            "train loss:0.034753600098174076\n",
            "train loss:0.0022691288080618354\n",
            "train loss:0.0015894871900252294\n",
            "train loss:0.018092957715246754\n",
            "train loss:0.001377432455216326\n",
            "train loss:0.011594590152666017\n",
            "train loss:0.008104557197289735\n",
            "train loss:0.009958317824447623\n",
            "train loss:0.002650854983179383\n",
            "train loss:0.0078081348924351705\n",
            "train loss:0.02511487789411583\n",
            "train loss:0.017977133743789928\n",
            "train loss:0.022220441627351466\n",
            "train loss:0.005510362006057539\n",
            "train loss:0.05250278531257674\n",
            "train loss:0.011572921414819444\n",
            "train loss:0.0032607867136831797\n",
            "train loss:0.008012934844568253\n",
            "train loss:0.003465428302146987\n",
            "train loss:0.019613071049243702\n",
            "train loss:0.04268560261803949\n",
            "train loss:0.021198177414528913\n",
            "train loss:0.008639864979234697\n",
            "train loss:0.008419688175637754\n",
            "train loss:0.023988784377255885\n",
            "train loss:0.0035461844769656735\n",
            "train loss:0.0025486860457483752\n",
            "train loss:0.009609822543947677\n",
            "train loss:0.007070741284777316\n",
            "train loss:0.03664417612091753\n",
            "train loss:0.02350032282565086\n",
            "train loss:0.04560756377109715\n",
            "train loss:0.0037387161918205213\n",
            "train loss:0.006967163013265075\n",
            "train loss:0.044183903258071124\n",
            "train loss:0.027522532106586263\n",
            "train loss:0.022295337310748974\n",
            "=== epoch:190, train acc:0.994, test acc:0.92 ===\n",
            "train loss:0.004560138750770672\n",
            "train loss:0.00423916605552754\n",
            "train loss:0.006938886880826092\n",
            "train loss:0.0450134753877579\n",
            "train loss:0.015743741639287406\n",
            "train loss:0.007269850039764035\n",
            "train loss:0.009058529135805935\n",
            "train loss:0.005241709024490122\n",
            "train loss:0.033539562046040985\n",
            "train loss:0.007108022622484994\n",
            "train loss:0.010390875071605767\n",
            "train loss:0.013837331220211675\n",
            "train loss:0.01845518061761574\n",
            "train loss:0.01390154898603392\n",
            "train loss:0.025124022588866923\n",
            "train loss:0.003721936633083448\n",
            "train loss:0.01550374491866014\n",
            "train loss:0.007587995165665011\n",
            "train loss:0.005406307338281418\n",
            "train loss:0.0019975346987498657\n",
            "train loss:0.019030065740633192\n",
            "train loss:0.004756664560663608\n",
            "train loss:0.012193060107530957\n",
            "train loss:0.0019867361436743803\n",
            "train loss:0.00521187227190367\n",
            "train loss:0.0034341178288656822\n",
            "train loss:0.007025042796979447\n",
            "train loss:0.014955342908089446\n",
            "train loss:0.06202575040384712\n",
            "train loss:0.019735732630765626\n",
            "train loss:0.023129415295718703\n",
            "train loss:0.002999631231094156\n",
            "train loss:0.025107759295587603\n",
            "train loss:0.010769978647544018\n",
            "train loss:0.06179413160267746\n",
            "train loss:0.0074800219847332115\n",
            "train loss:0.00791975435165337\n",
            "train loss:0.008683687889159556\n",
            "train loss:0.006701830051007057\n",
            "train loss:0.017972894686377604\n",
            "train loss:0.004453150603433358\n",
            "train loss:0.002332658090246815\n",
            "train loss:0.009529500398986583\n",
            "train loss:0.058610687234081735\n",
            "train loss:0.0101752868308188\n",
            "train loss:0.05099867255895893\n",
            "train loss:0.006230206502146686\n",
            "train loss:0.004789738928515158\n",
            "train loss:0.0025339704446845136\n",
            "train loss:0.05147971643696637\n",
            "train loss:0.0018437228409577005\n",
            "train loss:0.03293257038307836\n",
            "train loss:0.019037039385328938\n",
            "train loss:0.029217351000998548\n",
            "train loss:0.020718511756087573\n",
            "train loss:0.051910781917544994\n",
            "train loss:0.03608688319112053\n",
            "train loss:0.03266297544588154\n",
            "train loss:0.006384988793119213\n",
            "train loss:0.07928766678497329\n",
            "train loss:0.010304161100454425\n",
            "train loss:0.008410716766211707\n",
            "train loss:0.021628913945284167\n",
            "train loss:0.004997397094255853\n",
            "train loss:0.00213108469879674\n",
            "train loss:0.07749817862803739\n",
            "train loss:0.01171860453289149\n",
            "train loss:0.0019800379232771167\n",
            "train loss:0.004389946197008998\n",
            "train loss:0.011934523338392834\n",
            "train loss:0.004034578754352174\n",
            "train loss:0.006252426961852036\n",
            "train loss:0.017547410088421395\n",
            "train loss:0.011694627234503012\n",
            "train loss:0.03311111367935966\n",
            "train loss:0.0031370258418674806\n",
            "train loss:0.04026305230292168\n",
            "train loss:0.011689964484585586\n",
            "train loss:0.004148705174753754\n",
            "train loss:0.008424267692341938\n",
            "=== epoch:191, train acc:0.995, test acc:0.9215 ===\n",
            "train loss:0.01681113201374187\n",
            "train loss:0.001782373275389072\n",
            "train loss:0.004871455721605557\n",
            "train loss:0.0034028669574968627\n",
            "train loss:0.006844629319971632\n",
            "train loss:0.02462210527312527\n",
            "train loss:0.006900426362369396\n",
            "train loss:0.0041035652469284456\n",
            "train loss:0.04343627433081288\n",
            "train loss:0.051248677004096556\n",
            "train loss:0.005665758365904846\n",
            "train loss:0.0025498170067382825\n",
            "train loss:0.00989089277655477\n",
            "train loss:0.0035891958689658627\n",
            "train loss:0.007214418149472733\n",
            "train loss:0.013084057540258136\n",
            "train loss:0.006242837428244885\n",
            "train loss:0.01490368175951668\n",
            "train loss:0.057681465780318755\n",
            "train loss:0.006610034295535682\n",
            "train loss:0.02227402110209661\n",
            "train loss:0.008976144969186207\n",
            "train loss:0.015437813692739394\n",
            "train loss:0.03201223479402792\n",
            "train loss:0.002377727147707767\n",
            "train loss:0.0018633475937791866\n",
            "train loss:0.007440718014203298\n",
            "train loss:0.0034634986683541517\n",
            "train loss:0.003049968335573077\n",
            "train loss:0.009964265479215952\n",
            "train loss:0.013230328990577678\n",
            "train loss:0.02445503521377369\n",
            "train loss:0.007371140575884743\n",
            "train loss:0.0025026919501607335\n",
            "train loss:0.0065204667562152215\n",
            "train loss:0.049503411452908874\n",
            "train loss:0.004150413745063817\n",
            "train loss:0.003354611221197807\n",
            "train loss:0.002399068449013665\n",
            "train loss:0.003903208409916198\n",
            "train loss:0.015441680799116559\n",
            "train loss:0.024094127836004633\n",
            "train loss:0.0016823099747061202\n",
            "train loss:0.047277110375106936\n",
            "train loss:0.03319618191361734\n",
            "train loss:0.034319537748690725\n",
            "train loss:0.08008698904485474\n",
            "train loss:0.03777601075174638\n",
            "train loss:0.007594719805148195\n",
            "train loss:0.006327916991356177\n",
            "train loss:0.005071841346520072\n",
            "train loss:0.002377268533572591\n",
            "train loss:0.005072339036274478\n",
            "train loss:0.0035466563231263226\n",
            "train loss:0.011447980829763236\n",
            "train loss:0.004238890486138053\n",
            "train loss:0.014778538540032835\n",
            "train loss:0.009445362456931452\n",
            "train loss:0.008341438732309588\n",
            "train loss:0.01416094698897031\n",
            "train loss:0.0033908842095516966\n",
            "train loss:0.0027581340520953547\n",
            "train loss:0.08055500849618971\n",
            "train loss:0.017921750355317102\n",
            "train loss:0.011409012958769282\n",
            "train loss:0.004739901102816336\n",
            "train loss:0.002755086636352164\n",
            "train loss:0.009164080256757372\n",
            "train loss:0.00553768014943841\n",
            "train loss:0.0034757560460333802\n",
            "train loss:0.023103467709510953\n",
            "train loss:0.03466588389412382\n",
            "train loss:0.011919741135023327\n",
            "train loss:0.009183238554574601\n",
            "train loss:0.025259461238362015\n",
            "train loss:0.007325144546663122\n",
            "train loss:0.004529831523553097\n",
            "train loss:0.00696781229235055\n",
            "train loss:0.003488370140062289\n",
            "train loss:0.002188357862686065\n",
            "=== epoch:192, train acc:0.99525, test acc:0.925 ===\n",
            "train loss:0.026247852159259524\n",
            "train loss:0.0025844383601573728\n",
            "train loss:0.014236907461274075\n",
            "train loss:0.04667508878823631\n",
            "train loss:0.037177694000328596\n",
            "train loss:0.017925535623357334\n",
            "train loss:0.003253301521051427\n",
            "train loss:0.0064585160479750025\n",
            "train loss:0.003227564280719018\n",
            "train loss:0.005611940416366141\n",
            "train loss:0.008573588101855186\n",
            "train loss:0.002498247086238585\n",
            "train loss:0.003491370800765967\n",
            "train loss:0.019477653075641047\n",
            "train loss:0.004946267483258458\n",
            "train loss:0.05018286690252852\n",
            "train loss:0.011172530871818324\n",
            "train loss:0.0053656541777078424\n",
            "train loss:0.006286961991898659\n",
            "train loss:0.019744383900834432\n",
            "train loss:0.004766738329374017\n",
            "train loss:0.003981816749665913\n",
            "train loss:0.06679258734458783\n",
            "train loss:0.004359926617833078\n",
            "train loss:0.002856560955553156\n",
            "train loss:0.0019906876101321138\n",
            "train loss:0.003796917032109849\n",
            "train loss:0.002191420066923539\n",
            "train loss:0.004840068755464241\n",
            "train loss:0.00372438888177059\n",
            "train loss:0.016577253728419835\n",
            "train loss:0.005956649824216381\n",
            "train loss:0.003353973668080188\n",
            "train loss:0.06454917253351064\n",
            "train loss:0.008445687671235389\n",
            "train loss:0.009212154722635583\n",
            "train loss:0.01667542525133712\n",
            "train loss:0.009260992137386037\n",
            "train loss:0.04583297230161869\n",
            "train loss:0.019224076145471575\n",
            "train loss:0.08122706304577068\n",
            "train loss:0.006738910733164069\n",
            "train loss:0.004020196489228114\n",
            "train loss:0.002734976006258844\n",
            "train loss:0.00390985600140575\n",
            "train loss:0.02784513346886179\n",
            "train loss:0.004551863567131387\n",
            "train loss:0.012563206222609419\n",
            "train loss:0.012766042566746247\n",
            "train loss:0.008596985799158671\n",
            "train loss:0.014782287568060642\n",
            "train loss:0.003029108120300911\n",
            "train loss:0.013107693495038661\n",
            "train loss:0.002349367883840787\n",
            "train loss:0.0026467645849360412\n",
            "train loss:0.08584485895962249\n",
            "train loss:0.025006364974904015\n",
            "train loss:0.05801718731117452\n",
            "train loss:0.014447408514335642\n",
            "train loss:0.01610766865553923\n",
            "train loss:0.006717308178992073\n",
            "train loss:0.0027164269739577396\n",
            "train loss:0.008915468144329742\n",
            "train loss:0.04580439139595266\n",
            "train loss:0.004605423835745864\n",
            "train loss:0.0051350277719801754\n",
            "train loss:0.017782048251532713\n",
            "train loss:0.005176665322430627\n",
            "train loss:0.003161584462456725\n",
            "train loss:0.003026721088871288\n",
            "train loss:0.04011663445177374\n",
            "train loss:0.0054036065281930015\n",
            "train loss:0.015391742682650967\n",
            "train loss:0.010832998531251335\n",
            "train loss:0.005575796593159814\n",
            "train loss:0.006337042617862052\n",
            "train loss:0.024944149578124453\n",
            "train loss:0.02743864598053777\n",
            "train loss:0.0030931219782607986\n",
            "train loss:0.05318924450337117\n",
            "=== epoch:193, train acc:0.994375, test acc:0.922 ===\n",
            "train loss:0.013921598764679497\n",
            "train loss:0.0025884545288050735\n",
            "train loss:0.005738929117499602\n",
            "train loss:0.006558219719068416\n",
            "train loss:0.012890395874979688\n",
            "train loss:0.010223923005437941\n",
            "train loss:0.009454841805898076\n",
            "train loss:0.011607701792827885\n",
            "train loss:0.003496944714839208\n",
            "train loss:0.009111289261323971\n",
            "train loss:0.012052458304944584\n",
            "train loss:0.002296328097315099\n",
            "train loss:0.008052407438816446\n",
            "train loss:0.0033670510464119795\n",
            "train loss:0.00421976547752413\n",
            "train loss:0.03400127245159342\n",
            "train loss:0.012200179434749795\n",
            "train loss:0.025020235302334597\n",
            "train loss:0.002423680587992929\n",
            "train loss:0.006461497899659918\n",
            "train loss:0.0289718077064112\n",
            "train loss:0.015256176175309433\n",
            "train loss:0.04887703627870719\n",
            "train loss:0.007228257789181533\n",
            "train loss:0.03515327932521832\n",
            "train loss:0.004315619604020254\n",
            "train loss:0.01637588590554584\n",
            "train loss:0.004580803630981185\n",
            "train loss:0.021597595386993768\n",
            "train loss:0.008054928295677808\n",
            "train loss:0.002283437929305879\n",
            "train loss:0.004594360383349697\n",
            "train loss:0.00553471751711749\n",
            "train loss:0.004114248360252719\n",
            "train loss:0.004272915601222948\n",
            "train loss:0.07574705434361884\n",
            "train loss:0.004812637547943678\n",
            "train loss:0.014413250003905276\n",
            "train loss:0.0038777832871275926\n",
            "train loss:0.005545673186204013\n",
            "train loss:0.033128706951505056\n",
            "train loss:0.01845934408625373\n",
            "train loss:0.002274720238597451\n",
            "train loss:0.03181145078765623\n",
            "train loss:0.005923066840896194\n",
            "train loss:0.008290460480146596\n",
            "train loss:0.005997171484426359\n",
            "train loss:0.0023166238405579365\n",
            "train loss:0.00790878616880343\n",
            "train loss:0.00436129288517525\n",
            "train loss:0.009162887746825362\n",
            "train loss:0.03431239319980381\n",
            "train loss:0.020816434652016002\n",
            "train loss:0.0046155384068712075\n",
            "train loss:0.0064049117497075635\n",
            "train loss:0.007646528395159006\n",
            "train loss:0.003674486106413186\n",
            "train loss:0.00440416352338068\n",
            "train loss:0.006136916260794227\n",
            "train loss:0.004658174415793755\n",
            "train loss:0.010777484190936628\n",
            "train loss:0.003481511624926154\n",
            "train loss:0.030771147091644205\n",
            "train loss:0.010698388567502099\n",
            "train loss:0.08840017486237428\n",
            "train loss:0.011451565948903444\n",
            "train loss:0.05278260335057043\n",
            "train loss:0.019983802711317594\n",
            "train loss:0.07386543688274676\n",
            "train loss:0.006310352330293079\n",
            "train loss:0.004005881018741933\n",
            "train loss:0.013709796019133559\n",
            "train loss:0.004644071100262735\n",
            "train loss:0.03563587985684163\n",
            "train loss:0.018831456445467316\n",
            "train loss:0.02046838610835035\n",
            "train loss:0.004438680329586986\n",
            "train loss:0.025303093003585405\n",
            "train loss:0.011090875557221415\n",
            "train loss:0.007013066297510914\n",
            "=== epoch:194, train acc:0.995125, test acc:0.924 ===\n",
            "train loss:0.005307562689600864\n",
            "train loss:0.007682227247992873\n",
            "train loss:0.0026788545047183275\n",
            "train loss:0.004339091699466303\n",
            "train loss:0.0035354992927344887\n",
            "train loss:0.04773456273324362\n",
            "train loss:0.03446064815827043\n",
            "train loss:0.028030229063483614\n",
            "train loss:0.030785952021489527\n",
            "train loss:0.02641236706122141\n",
            "train loss:0.015791689268047426\n",
            "train loss:0.010938370580881011\n",
            "train loss:0.05591322548082574\n",
            "train loss:0.05934116460264427\n",
            "train loss:0.034344365820395954\n",
            "train loss:0.0024443479429620755\n",
            "train loss:0.013431208852495413\n",
            "train loss:0.008425732514184183\n",
            "train loss:0.00493400897768612\n",
            "train loss:0.010821264904435352\n",
            "train loss:0.012547601152354424\n",
            "train loss:0.012788741889764688\n",
            "train loss:0.0016972918015852095\n",
            "train loss:0.002489676830960384\n",
            "train loss:0.013895183610545415\n",
            "train loss:0.005964842899644479\n",
            "train loss:0.007829414480091102\n",
            "train loss:0.009350090415460624\n",
            "train loss:0.004908604346240494\n",
            "train loss:0.028073712352314442\n",
            "train loss:0.0034562100507420164\n",
            "train loss:0.002927953839901734\n",
            "train loss:0.0037361051167264276\n",
            "train loss:0.014680746747451665\n",
            "train loss:0.0201396499360309\n",
            "train loss:0.0058583303938467175\n",
            "train loss:0.046988450793175686\n",
            "train loss:0.0013600226993221005\n",
            "train loss:0.020871194957125718\n",
            "train loss:0.008923652107828892\n",
            "train loss:0.02864421773262963\n",
            "train loss:0.0016989811488223557\n",
            "train loss:0.0019302444582745685\n",
            "train loss:0.0038242212179252703\n",
            "train loss:0.009884736541692933\n",
            "train loss:0.002617484322260541\n",
            "train loss:0.005361314422545968\n",
            "train loss:0.03149079917081449\n",
            "train loss:0.01720284612137612\n",
            "train loss:0.0014955784095939775\n",
            "train loss:0.0014928429328656776\n",
            "train loss:0.052944357109074566\n",
            "train loss:0.008259888653990211\n",
            "train loss:0.00904068133889283\n",
            "train loss:0.005637493801077764\n",
            "train loss:0.00590711434572026\n",
            "train loss:0.02894338869716797\n",
            "train loss:0.03403290286814911\n",
            "train loss:0.002124216524853589\n",
            "train loss:0.003967123835309472\n",
            "train loss:0.01496993018521989\n",
            "train loss:0.00880307325830239\n",
            "train loss:0.010024373560002315\n",
            "train loss:0.004090430356131558\n",
            "train loss:0.004552464212856414\n",
            "train loss:0.06400867829490432\n",
            "train loss:0.06339755024281302\n",
            "train loss:0.004091674728314832\n",
            "train loss:0.0016620585608429184\n",
            "train loss:0.005847789942222824\n",
            "train loss:0.004017351213828011\n",
            "train loss:0.026684003714541382\n",
            "train loss:0.005008347597871772\n",
            "train loss:0.04380383739960597\n",
            "train loss:0.008357426333947619\n",
            "train loss:0.03405921730415306\n",
            "train loss:0.0037307674320708833\n",
            "train loss:0.0033770348272116734\n",
            "train loss:0.01552993385125081\n",
            "train loss:0.0019475764012006009\n",
            "=== epoch:195, train acc:0.994875, test acc:0.923 ===\n",
            "train loss:0.006622037534975409\n",
            "train loss:0.00666130998814641\n",
            "train loss:0.002946103602934737\n",
            "train loss:0.001503618649301702\n",
            "train loss:0.0051405565345049\n",
            "train loss:0.002956605969209531\n",
            "train loss:0.0008009999180099553\n",
            "train loss:0.0013916266889066765\n",
            "train loss:0.00993549698315226\n",
            "train loss:0.008679629807391644\n",
            "train loss:0.007100272531601546\n",
            "train loss:0.010722461731142417\n",
            "train loss:0.0032237023892033507\n",
            "train loss:0.004624025972256347\n",
            "train loss:0.003971974344250561\n",
            "train loss:0.002827500316641699\n",
            "train loss:0.015834939152485093\n",
            "train loss:0.004101838186624426\n",
            "train loss:0.01113504239750422\n",
            "train loss:0.02716212180281328\n",
            "train loss:0.026919630924703505\n",
            "train loss:0.004005643253238565\n",
            "train loss:0.0412940928070505\n",
            "train loss:0.004401784584874538\n",
            "train loss:0.0035725184583027475\n",
            "train loss:0.006763434365303614\n",
            "train loss:0.03497354785737695\n",
            "train loss:0.0033992802751448975\n",
            "train loss:0.02261254852957676\n",
            "train loss:0.0023640807412096134\n",
            "train loss:0.0043539651874458015\n",
            "train loss:0.013985777717093407\n",
            "train loss:0.002369074777296131\n",
            "train loss:0.004058127234771599\n",
            "train loss:0.030830692050264532\n",
            "train loss:0.017820549367057285\n",
            "train loss:0.0017063306427041371\n",
            "train loss:0.0037519945914275526\n",
            "train loss:0.009816110292575859\n",
            "train loss:0.018929831765138804\n",
            "train loss:0.02737282136852037\n",
            "train loss:0.008007717811459565\n",
            "train loss:0.020875706451490558\n",
            "train loss:0.0023372902253236136\n",
            "train loss:0.016558465039815676\n",
            "train loss:0.004766233364228487\n",
            "train loss:0.04387868730385132\n",
            "train loss:0.0024796792124236453\n",
            "train loss:0.010118371274411467\n",
            "train loss:0.0022967466523695426\n",
            "train loss:0.033633389728136794\n",
            "train loss:0.007359410846607315\n",
            "train loss:0.03369880949568857\n",
            "train loss:0.0029015942992983153\n",
            "train loss:0.0036295704641828488\n",
            "train loss:0.05952774422531551\n",
            "train loss:0.0036804172027072694\n",
            "train loss:0.0471644625778778\n",
            "train loss:0.013292849802469626\n",
            "train loss:0.004179008234235437\n",
            "train loss:0.0017774355933818096\n",
            "train loss:0.046899068085423726\n",
            "train loss:0.005053785618609233\n",
            "train loss:0.026563809281795532\n",
            "train loss:0.01676155342123811\n",
            "train loss:0.0786900087953626\n",
            "train loss:0.014997861200373578\n",
            "train loss:0.00823592088813273\n",
            "train loss:0.004587445158792047\n",
            "train loss:0.0025578082438840123\n",
            "train loss:0.02660536483825737\n",
            "train loss:0.009640106104563282\n",
            "train loss:0.0013920101958441978\n",
            "train loss:0.01587258268606594\n",
            "train loss:0.029138706412127246\n",
            "train loss:0.04750161311546722\n",
            "train loss:0.03126250375575372\n",
            "train loss:0.0283890164254686\n",
            "train loss:0.05436848571312617\n",
            "train loss:0.0028103202305329446\n",
            "=== epoch:196, train acc:0.995125, test acc:0.9225 ===\n",
            "train loss:0.01053642401500007\n",
            "train loss:0.0033791905664446197\n",
            "train loss:0.0046842279695098\n",
            "train loss:0.005711650481048496\n",
            "train loss:0.010595368648217116\n",
            "train loss:0.004412632514569998\n",
            "train loss:0.09874584035215266\n",
            "train loss:0.034249603875348526\n",
            "train loss:0.0032645411688507382\n",
            "train loss:0.0047254003914933935\n",
            "train loss:0.03487655014360342\n",
            "train loss:0.0022955173095619737\n",
            "train loss:0.00592584702658506\n",
            "train loss:0.005604519645385454\n",
            "train loss:0.02579126214895679\n",
            "train loss:0.043930879735456375\n",
            "train loss:0.0031337468899019115\n",
            "train loss:0.006059622305409557\n",
            "train loss:0.015606957062966722\n",
            "train loss:0.0016038924891034168\n",
            "train loss:0.009131034999087718\n",
            "train loss:0.014766790622119663\n",
            "train loss:0.03056542387174605\n",
            "train loss:0.008679111701099935\n",
            "train loss:0.007223622538740454\n",
            "train loss:0.007693313035801424\n",
            "train loss:0.017771892275314727\n",
            "train loss:0.007866839594524897\n",
            "train loss:0.07789875140371151\n",
            "train loss:0.008107977178586238\n",
            "train loss:0.017076153765385377\n",
            "train loss:0.013694304697304547\n",
            "train loss:0.004174827414473163\n",
            "train loss:0.003095085329083172\n",
            "train loss:0.003744470748480662\n",
            "train loss:0.046669476264767834\n",
            "train loss:0.0051420038458605665\n",
            "train loss:0.00244947364267371\n",
            "train loss:0.0033787061382037496\n",
            "train loss:0.0019623278158886356\n",
            "train loss:0.0078271541032511\n",
            "train loss:0.004604431275470556\n",
            "train loss:0.0023227009042670944\n",
            "train loss:0.0048476780468084\n",
            "train loss:0.019427255018418837\n",
            "train loss:0.00553901477607252\n",
            "train loss:0.014441611904488887\n",
            "train loss:0.048160208704835485\n",
            "train loss:0.0027917743968549325\n",
            "train loss:0.00721153512784628\n",
            "train loss:0.005031844658657435\n",
            "train loss:0.0036850865051884455\n",
            "train loss:0.016048773616544994\n",
            "train loss:0.003933478495978436\n",
            "train loss:0.00479291076911298\n",
            "train loss:0.022860502491988777\n",
            "train loss:0.07112591734811242\n",
            "train loss:0.024035444299039206\n",
            "train loss:0.0024804581527851687\n",
            "train loss:0.002322526788235386\n",
            "train loss:0.0043542538544676926\n",
            "train loss:0.01697362408602113\n",
            "train loss:0.004221726470316542\n",
            "train loss:0.005275744757342009\n",
            "train loss:0.03354603148160049\n",
            "train loss:0.003074822494770435\n",
            "train loss:0.033073868464899785\n",
            "train loss:0.0035048312604690685\n",
            "train loss:0.005486643348889941\n",
            "train loss:0.006049214110715463\n",
            "train loss:0.022749962722681864\n",
            "train loss:0.02861908744777033\n",
            "train loss:0.0028232785836083125\n",
            "train loss:0.005507943635080971\n",
            "train loss:0.002823281975733194\n",
            "train loss:0.0034819064332583943\n",
            "train loss:0.005670596949721921\n",
            "train loss:0.0027902871798309377\n",
            "train loss:0.013152624431597384\n",
            "train loss:0.029366734878979953\n",
            "=== epoch:197, train acc:0.99525, test acc:0.921 ===\n",
            "train loss:0.004374918813945639\n",
            "train loss:0.08763334042602584\n",
            "train loss:0.0017326750995172737\n",
            "train loss:0.026956562810691836\n",
            "train loss:0.004994914859248214\n",
            "train loss:0.01142362129624646\n",
            "train loss:0.009907561862502683\n",
            "train loss:0.0017618314210899272\n",
            "train loss:0.034871231371027836\n",
            "train loss:0.002926374409378057\n",
            "train loss:0.009104393575823154\n",
            "train loss:0.0025726195759439283\n",
            "train loss:0.006268735165908697\n",
            "train loss:0.038116866247136706\n",
            "train loss:0.00172702482527588\n",
            "train loss:0.01190506368628002\n",
            "train loss:0.005961001139392681\n",
            "train loss:0.004095226102143396\n",
            "train loss:0.00242099368031656\n",
            "train loss:0.003261540123616868\n",
            "train loss:0.005098418518466489\n",
            "train loss:0.0232460730770707\n",
            "train loss:0.018049638874550578\n",
            "train loss:0.021183048057067964\n",
            "train loss:0.0039850399766057695\n",
            "train loss:0.0077874825763136\n",
            "train loss:0.0038397066454610067\n",
            "train loss:0.0662442447866562\n",
            "train loss:0.002497486688150546\n",
            "train loss:0.005884176435252979\n",
            "train loss:0.03291101515857409\n",
            "train loss:0.003102189223705593\n",
            "train loss:0.024496103221505465\n",
            "train loss:0.0016811809716512289\n",
            "train loss:0.006971295873176994\n",
            "train loss:0.004679034633666802\n",
            "train loss:0.005545099964521595\n",
            "train loss:0.004447621517768964\n",
            "train loss:0.020874579536144248\n",
            "train loss:0.009794497786757584\n",
            "train loss:0.0048962503644822755\n",
            "train loss:0.006741059874268002\n",
            "train loss:0.013438837367346907\n",
            "train loss:0.009801628641412074\n",
            "train loss:0.013097636740281882\n",
            "train loss:0.004547712564396757\n",
            "train loss:0.015846325122452275\n",
            "train loss:0.001248930574606442\n",
            "train loss:0.02221928517451194\n",
            "train loss:0.010464299657370231\n",
            "train loss:0.01987330568388968\n",
            "train loss:0.0017968845137998398\n",
            "train loss:0.011586198428817031\n",
            "train loss:0.048328279190938996\n",
            "train loss:0.06413542682561289\n",
            "train loss:0.05247032083504257\n",
            "train loss:0.0043303942219067295\n",
            "train loss:0.003212107380686558\n",
            "train loss:0.03581725679803499\n",
            "train loss:0.00428869069499199\n",
            "train loss:0.008080019310911811\n",
            "train loss:0.04248535608487686\n",
            "train loss:0.004190479896158788\n",
            "train loss:0.0030471911513540906\n",
            "train loss:0.013427695192484492\n",
            "train loss:0.008054076587747958\n",
            "train loss:0.003034766112589609\n",
            "train loss:0.005794116298657915\n",
            "train loss:0.005486347063033734\n",
            "train loss:0.052434459903869204\n",
            "train loss:0.002163506832221729\n",
            "train loss:0.02914254303176529\n",
            "train loss:0.001786260609349563\n",
            "train loss:0.0240166158466055\n",
            "train loss:0.0020551141663527187\n",
            "train loss:0.0017267880600731202\n",
            "train loss:0.013479629237667245\n",
            "train loss:0.002448676597223709\n",
            "train loss:0.0029962092237790384\n",
            "train loss:0.006137406882914634\n",
            "=== epoch:198, train acc:0.99475, test acc:0.921 ===\n",
            "train loss:0.0031070299671718217\n",
            "train loss:0.020490490077956492\n",
            "train loss:0.0026126475193930573\n",
            "train loss:0.007064595564508985\n",
            "train loss:0.017340283186829814\n",
            "train loss:0.004676996779639964\n",
            "train loss:0.0660908328306776\n",
            "train loss:0.010071981013925858\n",
            "train loss:0.05041157993003484\n",
            "train loss:0.010738390989430371\n",
            "train loss:0.02603279747115079\n",
            "train loss:0.00573537128364056\n",
            "train loss:0.009962542444711384\n",
            "train loss:0.0027043267797732464\n",
            "train loss:0.002867489846413972\n",
            "train loss:0.004843549472349911\n",
            "train loss:0.0020160808063868344\n",
            "train loss:0.00984894065832902\n",
            "train loss:0.0033044254993467607\n",
            "train loss:0.003989577739617638\n",
            "train loss:0.002454896501539695\n",
            "train loss:0.03126447135008551\n",
            "train loss:0.0602906189585419\n",
            "train loss:0.0025522482282936475\n",
            "train loss:0.024825345493886625\n",
            "train loss:0.039253969012271406\n",
            "train loss:0.027286098302052007\n",
            "train loss:0.004734985353843908\n",
            "train loss:0.0015060204898284149\n",
            "train loss:0.032530889667434254\n",
            "train loss:0.002889258314188237\n",
            "train loss:0.012305390193769177\n",
            "train loss:0.025212698546740686\n",
            "train loss:0.0031196120132797756\n",
            "train loss:0.020189530090646057\n",
            "train loss:0.0050752206771404185\n",
            "train loss:0.0024109076356455456\n",
            "train loss:0.0035233528755808947\n",
            "train loss:0.0011148994699931786\n",
            "train loss:0.030780607633546054\n",
            "train loss:0.09267861787342013\n",
            "train loss:0.003595195280541819\n",
            "train loss:0.07180356251639569\n",
            "train loss:0.01659995433404924\n",
            "train loss:0.0015376556793387191\n",
            "train loss:0.03128837044206221\n",
            "train loss:0.012663425981919782\n",
            "train loss:0.0028585227211960717\n",
            "train loss:0.010234336386435748\n",
            "train loss:0.013643580649151903\n",
            "train loss:0.004017720916773689\n",
            "train loss:0.004248080186244615\n",
            "train loss:0.0017533100796322063\n",
            "train loss:0.01424713986932256\n",
            "train loss:0.05735895902169634\n",
            "train loss:0.02339812285245253\n",
            "train loss:0.04638668849883115\n",
            "train loss:0.0009311021911055394\n",
            "train loss:0.06691735398933693\n",
            "train loss:0.0028816583126144617\n",
            "train loss:0.0036990857686659408\n",
            "train loss:0.05052491527655154\n",
            "train loss:0.010953476386855248\n",
            "train loss:0.002775777167742823\n",
            "train loss:0.025153134098456507\n",
            "train loss:0.002893278592968584\n",
            "train loss:0.01024378842177371\n",
            "train loss:0.02829745227910997\n",
            "train loss:0.009059143942056468\n",
            "train loss:0.004041424330414015\n",
            "train loss:0.0030039043665687475\n",
            "train loss:0.035889910721320195\n",
            "train loss:0.027202810748613256\n",
            "train loss:0.002531772791474607\n",
            "train loss:0.011156821974354424\n",
            "train loss:0.0017899696035472668\n",
            "train loss:0.0021534470521614004\n",
            "train loss:0.042945840260690996\n",
            "train loss:0.004335273702323576\n",
            "train loss:0.0017174296828415337\n",
            "=== epoch:199, train acc:0.99475, test acc:0.923 ===\n",
            "train loss:0.005102926923653055\n",
            "train loss:0.002640163077452381\n",
            "train loss:0.005775393488343851\n",
            "train loss:0.02455703709260226\n",
            "train loss:0.010510976604339447\n",
            "train loss:0.03564459595849741\n",
            "train loss:0.002536296314839881\n",
            "train loss:0.0012643294543677917\n",
            "train loss:0.0025229785161750605\n",
            "train loss:0.05042892156436116\n",
            "train loss:0.032551025513982076\n",
            "train loss:0.003850421432640728\n",
            "train loss:0.007138528462848633\n",
            "train loss:0.00397253738964837\n",
            "train loss:0.026590801839462407\n",
            "train loss:0.008026055371709218\n",
            "train loss:0.007451875966030077\n",
            "train loss:0.004313217172106995\n",
            "train loss:0.012007529288710829\n",
            "train loss:0.0041765024713761425\n",
            "train loss:0.0027523029047305226\n",
            "train loss:0.0032176956913192185\n",
            "train loss:0.003951522779181245\n",
            "train loss:0.0019473686152489986\n",
            "train loss:0.03362556368928245\n",
            "train loss:0.02959728405209746\n",
            "train loss:0.05252552178716992\n",
            "train loss:0.043459701957405256\n",
            "train loss:0.0021242576564479846\n",
            "train loss:0.017489554929364503\n",
            "train loss:0.024144539284416632\n",
            "train loss:0.0034526225658486915\n",
            "train loss:0.08451493605468123\n",
            "train loss:0.029698479252813183\n",
            "train loss:0.003873119107263625\n",
            "train loss:0.027769127443036664\n",
            "train loss:0.006970368718851192\n",
            "train loss:0.03720321448926501\n",
            "train loss:0.031853689029879034\n",
            "train loss:0.006582680055728042\n",
            "train loss:0.0029555054194376974\n",
            "train loss:0.006039771602060944\n",
            "train loss:0.005207096046898108\n",
            "train loss:0.05130038844460165\n",
            "train loss:0.03962250918274677\n",
            "train loss:0.00753393703340861\n",
            "train loss:0.0034179634978677366\n",
            "train loss:0.012479899713346699\n",
            "train loss:0.020705007174980913\n",
            "train loss:0.0018524372071733203\n",
            "train loss:0.003361480279906396\n",
            "train loss:0.01605576221729642\n",
            "train loss:0.03214140217086628\n",
            "train loss:0.024544974502743223\n",
            "train loss:0.005464813647182568\n",
            "train loss:0.04735501550183591\n",
            "train loss:0.050019309076662605\n",
            "train loss:0.06820784357304162\n",
            "train loss:0.008163268996283854\n",
            "train loss:0.009672047952596824\n",
            "train loss:0.027928144871281046\n",
            "train loss:0.004106282440168944\n",
            "train loss:0.0031374605155679064\n",
            "train loss:0.005843158028440946\n",
            "train loss:0.016691965255944807\n",
            "train loss:0.010239747238332429\n",
            "train loss:0.06941770371674391\n",
            "train loss:0.012815662754880087\n",
            "train loss:0.014821641828351488\n",
            "train loss:0.005587214388621027\n",
            "train loss:0.010398074628312859\n",
            "train loss:0.003355188338108864\n",
            "train loss:0.016964121121343517\n",
            "train loss:0.00734334894502258\n",
            "train loss:0.0011753646792586694\n",
            "train loss:0.030832749742404703\n",
            "train loss:0.0031258248127413895\n",
            "train loss:0.08101635598456816\n",
            "train loss:0.0035559513297697587\n",
            "train loss:0.006116361259785201\n",
            "=== epoch:200, train acc:0.99575, test acc:0.923 ===\n",
            "train loss:0.004075035198477264\n",
            "train loss:0.007438047334175527\n",
            "train loss:0.02115790375567623\n",
            "train loss:0.006900548335218316\n",
            "train loss:0.01018895644756111\n",
            "train loss:0.015405212361369167\n",
            "train loss:0.004747374740234651\n",
            "train loss:0.02659506867710724\n",
            "train loss:0.003109211718975796\n",
            "train loss:0.014461060145860063\n",
            "train loss:0.0017162149512954777\n",
            "train loss:0.03265901107279625\n",
            "train loss:0.0031989879466179844\n",
            "train loss:0.016621645123472396\n",
            "train loss:0.04860392603675399\n",
            "train loss:0.006264746841098221\n",
            "train loss:0.015581870742634472\n",
            "train loss:0.006381739921383621\n",
            "train loss:0.009958388736969921\n",
            "train loss:0.014366430150369253\n",
            "train loss:0.0033689785451991826\n",
            "train loss:0.022584392970304134\n",
            "train loss:0.08156352099581038\n",
            "train loss:0.007820832659810779\n",
            "train loss:0.006926150850944187\n",
            "train loss:0.01957744944710087\n",
            "train loss:0.003107634375316799\n",
            "train loss:0.042641261871047546\n",
            "train loss:0.015494934585898961\n",
            "train loss:0.002734826906465329\n",
            "train loss:0.008444187913223882\n",
            "train loss:0.006042957160099434\n",
            "train loss:0.003754355655987012\n",
            "train loss:0.003275358464023366\n",
            "train loss:0.0038507582776179405\n",
            "train loss:0.00832444038273549\n",
            "train loss:0.0036635424464503885\n",
            "train loss:0.0057043892141057685\n",
            "train loss:0.029735369863884583\n",
            "train loss:0.02508472180019643\n",
            "train loss:0.006478036920899896\n",
            "train loss:0.0016557372181869804\n",
            "train loss:0.0038216163526959075\n",
            "train loss:0.02011647796247153\n",
            "train loss:0.006051241424580024\n",
            "train loss:0.005604965901835828\n",
            "train loss:0.008483699538353602\n",
            "train loss:0.06718200341810328\n",
            "train loss:0.004948324555691161\n",
            "train loss:0.004052430068748737\n",
            "train loss:0.006520594823619527\n",
            "train loss:0.0049801729402554635\n",
            "train loss:0.0050438315925345825\n",
            "train loss:0.008351260542373798\n",
            "train loss:0.03357896186107465\n",
            "train loss:0.00447244128039009\n",
            "train loss:0.01530176016690612\n",
            "train loss:0.032062559010527944\n",
            "train loss:0.005226850500390913\n",
            "train loss:0.01696007296204828\n",
            "train loss:0.016130211831983012\n",
            "train loss:0.002568327376085268\n",
            "train loss:0.03535494750561729\n",
            "train loss:0.008787555681325073\n",
            "train loss:0.01678761363039346\n",
            "train loss:0.04352184615891337\n",
            "train loss:0.003978972411837448\n",
            "train loss:0.003641825061574753\n",
            "train loss:0.011786866202005954\n",
            "train loss:0.0037010932316992894\n",
            "train loss:0.0037019517368510474\n",
            "train loss:0.010037195363304652\n",
            "train loss:0.0330563208082414\n",
            "train loss:0.0037586516974181137\n",
            "train loss:0.008066429923638017\n",
            "train loss:0.004578137396158295\n",
            "train loss:0.03964218672419375\n",
            "train loss:0.002453938721158218\n",
            "train loss:0.00810036866074993\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9205\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XY6Gg__qwi1K"
      },
      "source": [
        "## Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "sqUaNVBIwk-Q",
        "outputId": "98f2e1b8-06f5-4e45-b531-819304ed01ae"
      },
      "source": [
        "network = MultiLayerNetExtend(input_size=784, hidden_size_list=[100, 100, 100, 100, 100, 100, 100, 100],\r\n",
        "                              output_size=10, use_dropout=True, dropout_ration=0.2, use_batchnorm=True)\r\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\r\n",
        "                  epochs=200, mini_batch_size=100,\r\n",
        "                  optimizer='adam', optimizer_param={'lr': 0.001}, verbose=True)\r\n",
        "trainer.train()\r\n",
        "\r\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\r\n",
        "\r\n",
        "markers = {'train': 'o', 'test': 's'}\r\n",
        "x = np.arange(len(train_acc_list))\r\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\r\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\r\n",
        "plt.xlabel(\"epochs\")\r\n",
        "plt.ylabel(\"accuracy\")\r\n",
        "plt.ylim(0, 1.0)\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43m스트리밍 출력 내용이 길어서 마지막 5000줄이 삭제되었습니다.\u001b[0m\n",
            "train loss:0.00011773647148180615\n",
            "train loss:0.00017661062087243696\n",
            "train loss:0.00015563246323645124\n",
            "train loss:0.00011073249440537112\n",
            "train loss:0.0001494025440040362\n",
            "train loss:0.002322265130672009\n",
            "train loss:0.00024005502814272405\n",
            "train loss:0.000299013172383635\n",
            "train loss:0.00018236077379321622\n",
            "train loss:6.741577830471625e-05\n",
            "train loss:0.00013390798025965421\n",
            "train loss:0.00014923317266438122\n",
            "train loss:0.00014711906452378532\n",
            "train loss:0.0001429350782590975\n",
            "train loss:0.00021511444205277524\n",
            "train loss:0.00014976237378839385\n",
            "train loss:0.00017944555494318467\n",
            "train loss:0.0003189234077716715\n",
            "train loss:0.00019269345738979086\n",
            "train loss:0.00016478483936961885\n",
            "train loss:0.00014550478590494325\n",
            "train loss:0.0002977057416103542\n",
            "train loss:0.0002493011073607161\n",
            "train loss:0.00018121676067558393\n",
            "train loss:0.00023808409613384256\n",
            "train loss:0.0003584254529269811\n",
            "train loss:0.00015463644967261903\n",
            "train loss:0.00011214653913913058\n",
            "train loss:0.007941480004412867\n",
            "train loss:0.00024336531734307228\n",
            "train loss:0.005623854651215612\n",
            "train loss:0.00018974198396610537\n",
            "train loss:0.0001872065592601801\n",
            "train loss:0.00021661448346140167\n",
            "train loss:0.00025706908119467714\n",
            "train loss:0.00018738317272064993\n",
            "train loss:0.0001359031913034613\n",
            "train loss:0.00017595563318754105\n",
            "train loss:0.0001350418901516837\n",
            "train loss:0.0009868329888740696\n",
            "train loss:0.000197475670224744\n",
            "train loss:0.00017678662297859437\n",
            "train loss:0.0015725559246938632\n",
            "train loss:0.0001500931829719511\n",
            "train loss:0.00019012032962312052\n",
            "train loss:0.004774041411139193\n",
            "train loss:0.0003519817960687498\n",
            "train loss:0.0002527546819270213\n",
            "train loss:0.0002514123950574267\n",
            "train loss:0.0002959448065539566\n",
            "train loss:0.00019048176124196033\n",
            "train loss:0.0003364579842301136\n",
            "train loss:0.00043065126145851743\n",
            "train loss:0.0001974440901349541\n",
            "train loss:0.00016107316268553122\n",
            "train loss:0.00020038428208881437\n",
            "train loss:0.00035725937744261234\n",
            "train loss:0.0002161920019140241\n",
            "=== epoch:140, train acc:0.999875, test acc:0.951 ===\n",
            "train loss:0.0001594482129107611\n",
            "train loss:0.00021984619359538698\n",
            "train loss:0.00032291382488831464\n",
            "train loss:0.0001890034505284524\n",
            "train loss:0.0003755182984794904\n",
            "train loss:0.00021822967969068518\n",
            "train loss:0.00021837939625091299\n",
            "train loss:0.00023989909326442395\n",
            "train loss:0.00021808589154646924\n",
            "train loss:0.0002612394966694305\n",
            "train loss:0.0003514176806666061\n",
            "train loss:0.00014470342889406578\n",
            "train loss:0.000261147381200926\n",
            "train loss:0.00013680669575328214\n",
            "train loss:0.011385878932130113\n",
            "train loss:0.0021059373548357953\n",
            "train loss:0.000445494580507008\n",
            "train loss:0.0002598182426359612\n",
            "train loss:0.00012020942474666984\n",
            "train loss:0.00044112242620502256\n",
            "train loss:0.00022831740844205626\n",
            "train loss:0.00013455306081588454\n",
            "train loss:0.0001860433542514577\n",
            "train loss:0.0003319606761995789\n",
            "train loss:0.00015755485316246208\n",
            "train loss:0.00018925428769992004\n",
            "train loss:0.00013287111106832818\n",
            "train loss:0.00027969794793620097\n",
            "train loss:0.00020068087021807987\n",
            "train loss:0.00010726427528918652\n",
            "train loss:0.00016651335885799093\n",
            "train loss:0.0010501373250512503\n",
            "train loss:0.00015198713602560568\n",
            "train loss:0.0001494732663043114\n",
            "train loss:0.0001739332211642601\n",
            "train loss:0.0002165489394613871\n",
            "train loss:0.0005765928713471327\n",
            "train loss:0.0002425400032993761\n",
            "train loss:0.00025091972832532355\n",
            "train loss:0.00016298378970988067\n",
            "train loss:0.00023646813284888807\n",
            "train loss:0.0001659267969463118\n",
            "train loss:0.00015839876053136553\n",
            "train loss:0.00027450903315448375\n",
            "train loss:0.00028698019964345145\n",
            "train loss:0.00012705354709564802\n",
            "train loss:0.00012145642889492162\n",
            "train loss:0.0002346965737672623\n",
            "train loss:0.00015193474453474454\n",
            "train loss:0.0005895029376967068\n",
            "train loss:0.00021588194575083197\n",
            "train loss:0.00019222976248293866\n",
            "train loss:0.00023756575643265633\n",
            "train loss:0.0001402173837831105\n",
            "train loss:0.00020774379399047336\n",
            "train loss:0.00010180460063354251\n",
            "train loss:0.00015253320369924253\n",
            "train loss:0.0002818129671760918\n",
            "train loss:0.00021831557932298784\n",
            "train loss:0.00024003096095829839\n",
            "train loss:0.00014267449518025574\n",
            "train loss:0.0001629112741183701\n",
            "train loss:0.00016733909171477213\n",
            "train loss:0.00013388623280525397\n",
            "train loss:0.0001949302051159719\n",
            "train loss:0.0002834566646248919\n",
            "train loss:0.0001836644792296041\n",
            "train loss:0.0001689086289275845\n",
            "train loss:0.00014334440275050442\n",
            "train loss:0.0002585951338959114\n",
            "train loss:0.00015337617073930463\n",
            "train loss:0.00015888413764938487\n",
            "train loss:0.00020160621862413584\n",
            "train loss:0.00027630803411464124\n",
            "train loss:0.0001483001867801802\n",
            "train loss:0.00017735617042329778\n",
            "train loss:0.0003005689061014236\n",
            "train loss:0.00015179718908891993\n",
            "train loss:0.0003017779698925054\n",
            "train loss:0.00012822724380961339\n",
            "=== epoch:141, train acc:1.0, test acc:0.948 ===\n",
            "train loss:0.005128409265166694\n",
            "train loss:0.0003865094545652672\n",
            "train loss:0.0005499472829994285\n",
            "train loss:0.00021177398515069033\n",
            "train loss:0.00029196028478316923\n",
            "train loss:0.00028990954996868407\n",
            "train loss:0.00028686577286846706\n",
            "train loss:0.0002699997319753926\n",
            "train loss:0.0002445826111236452\n",
            "train loss:0.003958850904601848\n",
            "train loss:0.00040034403347748507\n",
            "train loss:0.00014834265982712662\n",
            "train loss:0.00033485339785626527\n",
            "train loss:0.00024493466096888925\n",
            "train loss:0.00019742319015930094\n",
            "train loss:0.0003675936470139633\n",
            "train loss:0.0002424574883884396\n",
            "train loss:0.00012447645748290867\n",
            "train loss:0.00022241276442649808\n",
            "train loss:0.00017195007202173962\n",
            "train loss:0.0006541447075436511\n",
            "train loss:0.00022314126362483583\n",
            "train loss:0.0002177391002805333\n",
            "train loss:0.00020184879403358356\n",
            "train loss:0.0001920773809880336\n",
            "train loss:0.00017413880460598316\n",
            "train loss:0.00010695252711765851\n",
            "train loss:0.0006392913246055367\n",
            "train loss:0.0002192023537177059\n",
            "train loss:0.00013795614079013352\n",
            "train loss:0.00039018338993025725\n",
            "train loss:0.00015244376521977924\n",
            "train loss:0.00011728525115357621\n",
            "train loss:0.0005159121430871196\n",
            "train loss:0.00019042969190895248\n",
            "train loss:0.0002530666326180499\n",
            "train loss:0.0004177179740556682\n",
            "train loss:0.00012010662052613573\n",
            "train loss:0.0005194049258410081\n",
            "train loss:0.00023529224540297229\n",
            "train loss:0.00016500076784184943\n",
            "train loss:0.00025279492528197065\n",
            "train loss:0.00016529813790307934\n",
            "train loss:0.0001342073441393651\n",
            "train loss:0.00018748758766618504\n",
            "train loss:0.00014475454655890724\n",
            "train loss:0.00012883727330102564\n",
            "train loss:0.00019788402322382437\n",
            "train loss:0.00020582706742998996\n",
            "train loss:0.0002726286407941486\n",
            "train loss:0.0003531974085075711\n",
            "train loss:0.00015071446527680364\n",
            "train loss:0.00016085446170004163\n",
            "train loss:0.0002026332772822033\n",
            "train loss:0.00022424057611231094\n",
            "train loss:0.00013476089905459048\n",
            "train loss:0.0001296654850613724\n",
            "train loss:0.0002764754445042663\n",
            "train loss:0.00031265343522878837\n",
            "train loss:0.00017685798682030884\n",
            "train loss:0.0005939993257186161\n",
            "train loss:0.00015232020532131752\n",
            "train loss:0.00013982940651849213\n",
            "train loss:0.0001457465559809905\n",
            "train loss:0.00016071769761636886\n",
            "train loss:0.00013006711739989554\n",
            "train loss:0.00022419259837458503\n",
            "train loss:0.00013001041631760193\n",
            "train loss:0.0005624529698337183\n",
            "train loss:0.00013031420083227494\n",
            "train loss:0.0002347282654251152\n",
            "train loss:0.0004419219239976782\n",
            "train loss:0.00018889739105243557\n",
            "train loss:0.00016289574866674912\n",
            "train loss:0.00020062009086896845\n",
            "train loss:0.00021073240188616626\n",
            "train loss:0.0002659425266652964\n",
            "train loss:0.00023384615000359612\n",
            "train loss:0.00012207879115328036\n",
            "train loss:0.00015770279430626607\n",
            "=== epoch:142, train acc:1.0, test acc:0.948 ===\n",
            "train loss:0.00016353796115416342\n",
            "train loss:0.001287214185714532\n",
            "train loss:0.00013103922767381313\n",
            "train loss:0.00020360186061911566\n",
            "train loss:0.0002581262698655432\n",
            "train loss:9.405499399034464e-05\n",
            "train loss:0.00015551133139895873\n",
            "train loss:0.00020135638921735976\n",
            "train loss:0.00024675006928492535\n",
            "train loss:0.00020030088943883025\n",
            "train loss:0.0002479362566250415\n",
            "train loss:0.00033923916105424895\n",
            "train loss:0.00025884819491365054\n",
            "train loss:0.00019199743831319874\n",
            "train loss:0.00040335182992517956\n",
            "train loss:0.00022974692967916294\n",
            "train loss:0.0002751071393050643\n",
            "train loss:0.00040673351968603834\n",
            "train loss:0.00024270775372744704\n",
            "train loss:0.00025186418003629677\n",
            "train loss:0.0004650909409289292\n",
            "train loss:0.00015400565899656565\n",
            "train loss:0.00024231222912077306\n",
            "train loss:0.00014934718842749388\n",
            "train loss:0.00011926154731006734\n",
            "train loss:0.0004653588545267841\n",
            "train loss:0.00027567455836980846\n",
            "train loss:9.746759934849247e-05\n",
            "train loss:0.00018696928301620245\n",
            "train loss:0.00016073813594182527\n",
            "train loss:0.00012743183114472139\n",
            "train loss:0.00011148580558848413\n",
            "train loss:0.00023302684198805987\n",
            "train loss:0.0002494980169087024\n",
            "train loss:0.0002021860160127289\n",
            "train loss:0.0001774884666475388\n",
            "train loss:0.0001849987287650515\n",
            "train loss:0.00027490011017636983\n",
            "train loss:0.0002123271761804758\n",
            "train loss:0.0003488964201535097\n",
            "train loss:0.0001658869707394339\n",
            "train loss:0.0002963859620027667\n",
            "train loss:0.00017202842951663633\n",
            "train loss:0.0002898908409010011\n",
            "train loss:0.00012573077221469668\n",
            "train loss:0.00019673124465412377\n",
            "train loss:0.00016454964440180782\n",
            "train loss:0.00024582615343529386\n",
            "train loss:0.0003089075425670437\n",
            "train loss:0.00024604234798090627\n",
            "train loss:0.00018794953855526578\n",
            "train loss:0.00024109126528452511\n",
            "train loss:0.00020715878523824106\n",
            "train loss:0.000157326221902911\n",
            "train loss:0.0001947695204542775\n",
            "train loss:0.0001799249495312365\n",
            "train loss:0.000519985659300848\n",
            "train loss:0.0002174074082527899\n",
            "train loss:0.0003129632965456309\n",
            "train loss:0.00014707972242864538\n",
            "train loss:0.00010374640169602061\n",
            "train loss:0.00014589179115704194\n",
            "train loss:0.0003257728153137485\n",
            "train loss:0.00026217013531771324\n",
            "train loss:0.000248196031153452\n",
            "train loss:0.00018051125593698328\n",
            "train loss:0.0002528359757613989\n",
            "train loss:0.00026787467968502113\n",
            "train loss:0.0005142119616341492\n",
            "train loss:0.00020590161767588443\n",
            "train loss:0.00021178384894291746\n",
            "train loss:0.00014052225846807013\n",
            "train loss:0.0016083467814698867\n",
            "train loss:0.00024899504321095544\n",
            "train loss:0.0001815999947582779\n",
            "train loss:0.0002514253464198632\n",
            "train loss:0.00011765597829319516\n",
            "train loss:0.0001714733018777161\n",
            "train loss:0.0003284578452557651\n",
            "train loss:0.00012630453806935528\n",
            "=== epoch:143, train acc:0.99975, test acc:0.949 ===\n",
            "train loss:0.00023639719818176145\n",
            "train loss:0.00011166213946940865\n",
            "train loss:0.00013231872278888723\n",
            "train loss:0.00030596365397813946\n",
            "train loss:0.0004256350776819019\n",
            "train loss:0.00012076224694289415\n",
            "train loss:0.0002610272146954757\n",
            "train loss:0.0001396897654470198\n",
            "train loss:0.00012235232195184208\n",
            "train loss:0.00015619964703501475\n",
            "train loss:0.00014456809064334873\n",
            "train loss:0.0318062180608975\n",
            "train loss:0.027038241157377872\n",
            "train loss:0.00016387091573443347\n",
            "train loss:9.209536213729604e-05\n",
            "train loss:0.00016056224162190586\n",
            "train loss:0.00016241070541902152\n",
            "train loss:0.0003163212432324553\n",
            "train loss:9.141877451825557e-05\n",
            "train loss:0.0001810043197107104\n",
            "train loss:0.00018750953233626498\n",
            "train loss:0.00013160255615778777\n",
            "train loss:0.00014797346831183866\n",
            "train loss:0.00012262146907945825\n",
            "train loss:0.00010580257792719067\n",
            "train loss:0.005374005380828011\n",
            "train loss:0.0001529131582756017\n",
            "train loss:0.00019505720545709562\n",
            "train loss:0.00015432825548562272\n",
            "train loss:0.00013818891418283238\n",
            "train loss:0.00014923242499624063\n",
            "train loss:0.00022336124238521527\n",
            "train loss:0.00017476575084350902\n",
            "train loss:0.0010780397943907514\n",
            "train loss:0.0001864414549502594\n",
            "train loss:0.00015611885852941223\n",
            "train loss:0.00013767765929501821\n",
            "train loss:0.00011131841521678121\n",
            "train loss:0.00014131788162411536\n",
            "train loss:9.068097828076632e-05\n",
            "train loss:0.0001951622845640955\n",
            "train loss:0.00011210814680890373\n",
            "train loss:0.00012158777493119515\n",
            "train loss:0.00017796138223068073\n",
            "train loss:0.0001132301192334784\n",
            "train loss:9.420092030699902e-05\n",
            "train loss:0.00014382528574266576\n",
            "train loss:0.00013396574147764447\n",
            "train loss:8.653931046045104e-05\n",
            "train loss:0.00014854836537704093\n",
            "train loss:0.00010923608319026157\n",
            "train loss:0.000682239920794933\n",
            "train loss:0.00010680771379525345\n",
            "train loss:9.852340846495391e-05\n",
            "train loss:0.001843791083951414\n",
            "train loss:7.191808291906709e-05\n",
            "train loss:0.00011571278633000502\n",
            "train loss:9.9345491175889e-05\n",
            "train loss:0.00024129278299897625\n",
            "train loss:0.00014407699220448327\n",
            "train loss:0.00017978505494375111\n",
            "train loss:0.0001206980918143386\n",
            "train loss:0.00019519823279260613\n",
            "train loss:0.00010111187406163439\n",
            "train loss:0.0002480655065356426\n",
            "train loss:0.00016611270112030518\n",
            "train loss:0.00018705630008552357\n",
            "train loss:0.0001364129693430324\n",
            "train loss:0.00017343180725927932\n",
            "train loss:0.0001823624285302017\n",
            "train loss:0.00014170802762588752\n",
            "train loss:9.078584691561281e-05\n",
            "train loss:0.00018731046259851903\n",
            "train loss:0.00015722247324298524\n",
            "train loss:0.00024747907556821406\n",
            "train loss:0.0027826294890615135\n",
            "train loss:9.956004548706986e-05\n",
            "train loss:0.000511026969717767\n",
            "train loss:0.00025755732443821176\n",
            "train loss:0.00020791196607089258\n",
            "=== epoch:144, train acc:0.999875, test acc:0.9415 ===\n",
            "train loss:0.00013883396473721548\n",
            "train loss:0.0002427357848048397\n",
            "train loss:0.00021957921021581326\n",
            "train loss:0.0002952457724330934\n",
            "train loss:0.00033691178765636105\n",
            "train loss:0.00019833243871156985\n",
            "train loss:0.00010714360988347808\n",
            "train loss:0.0001048920725922731\n",
            "train loss:0.00015942787489806758\n",
            "train loss:9.82121655658412e-05\n",
            "train loss:0.0001282157642664384\n",
            "train loss:0.0001224433691476746\n",
            "train loss:0.00011120264808627274\n",
            "train loss:0.0001093891003654944\n",
            "train loss:8.456369461814852e-05\n",
            "train loss:0.00016155597799266083\n",
            "train loss:9.567470696409976e-05\n",
            "train loss:0.0005578217173937382\n",
            "train loss:0.0003294960094541589\n",
            "train loss:0.00018617213239748988\n",
            "train loss:0.0001709497190200428\n",
            "train loss:0.00018185970177790161\n",
            "train loss:0.00012964093618200663\n",
            "train loss:0.00014891507484073081\n",
            "train loss:0.00022524241811759763\n",
            "train loss:0.00016358168902695278\n",
            "train loss:0.00013467356648841984\n",
            "train loss:8.609338699784506e-05\n",
            "train loss:9.52963595355152e-05\n",
            "train loss:0.00011083002558339584\n",
            "train loss:8.966219353281057e-05\n",
            "train loss:0.00013989627810871306\n",
            "train loss:0.00014410797739390592\n",
            "train loss:0.000122604079726093\n",
            "train loss:0.00021689399494389066\n",
            "train loss:0.00016326167945183583\n",
            "train loss:0.0002003471612494958\n",
            "train loss:0.00018733445144445915\n",
            "train loss:0.0001833033697586599\n",
            "train loss:9.414390909746343e-05\n",
            "train loss:0.00013461607220004125\n",
            "train loss:8.103243200928636e-05\n",
            "train loss:0.00011672455110155492\n",
            "train loss:0.00012223556724165566\n",
            "train loss:0.00019340389903419768\n",
            "train loss:8.75587949324017e-05\n",
            "train loss:0.00012228917100630255\n",
            "train loss:0.0003464295120166138\n",
            "train loss:0.0026148034076150482\n",
            "train loss:0.00011941251481396712\n",
            "train loss:0.00018589430773608057\n",
            "train loss:0.0026093231318220157\n",
            "train loss:0.000121004481151269\n",
            "train loss:0.00013256286256222179\n",
            "train loss:0.00010780085027370248\n",
            "train loss:0.00015518423028177262\n",
            "train loss:0.0001372727057891745\n",
            "train loss:0.00010655219052262423\n",
            "train loss:0.00013081055301878637\n",
            "train loss:0.00017567887376386936\n",
            "train loss:9.196578550677122e-05\n",
            "train loss:0.00021791008094605043\n",
            "train loss:9.754500206145789e-05\n",
            "train loss:0.00011315597424796425\n",
            "train loss:0.00014644388922695255\n",
            "train loss:0.00015153065499180164\n",
            "train loss:0.00012800347295428797\n",
            "train loss:0.000293786552865188\n",
            "train loss:0.00012776369420087983\n",
            "train loss:0.00014721245603518848\n",
            "train loss:0.0010170851920779764\n",
            "train loss:5.297698316317044e-05\n",
            "train loss:0.0003013558444968147\n",
            "train loss:0.00015000215549985124\n",
            "train loss:0.00010321178165001296\n",
            "train loss:8.090291811203339e-05\n",
            "train loss:0.00024376371221654012\n",
            "train loss:0.00018179806207267195\n",
            "train loss:0.00017371500354193374\n",
            "train loss:0.0023197560599785057\n",
            "=== epoch:145, train acc:0.999875, test acc:0.9455 ===\n",
            "train loss:0.00038463528283761523\n",
            "train loss:0.00015076110560356503\n",
            "train loss:0.00012162235191259784\n",
            "train loss:0.00010986984682794562\n",
            "train loss:0.00021221993857349077\n",
            "train loss:0.0001432739383249905\n",
            "train loss:0.00012183174779073602\n",
            "train loss:0.0017836374890682779\n",
            "train loss:9.980951251842147e-05\n",
            "train loss:0.00013564226841033763\n",
            "train loss:0.0002186715982166006\n",
            "train loss:0.00013950960904528625\n",
            "train loss:0.0003365400964294784\n",
            "train loss:0.00010295480885158992\n",
            "train loss:0.00015927946447354235\n",
            "train loss:8.967949968440352e-05\n",
            "train loss:0.00013952135880496605\n",
            "train loss:0.00019970768851661607\n",
            "train loss:9.255408830200321e-05\n",
            "train loss:0.00019595282485767036\n",
            "train loss:0.00015606493446871595\n",
            "train loss:0.00013242062298184497\n",
            "train loss:0.00022848160827730352\n",
            "train loss:8.726669904404494e-05\n",
            "train loss:0.00013098469386921594\n",
            "train loss:0.00014661310699642772\n",
            "train loss:0.0001515498690217745\n",
            "train loss:0.0010946031036096439\n",
            "train loss:0.00015785298986419595\n",
            "train loss:0.00022990907792381593\n",
            "train loss:0.00012736227497344983\n",
            "train loss:0.00016562390012134876\n",
            "train loss:0.0006744973234740046\n",
            "train loss:0.0002493267600303472\n",
            "train loss:7.803853029300116e-05\n",
            "train loss:7.715317743101216e-05\n",
            "train loss:0.0001625455803768315\n",
            "train loss:9.951250662188361e-05\n",
            "train loss:0.00013956820136142557\n",
            "train loss:0.0003140616883717534\n",
            "train loss:0.00021976160901918213\n",
            "train loss:0.00010808187605413575\n",
            "train loss:0.00015216569447058754\n",
            "train loss:0.00023013277987741834\n",
            "train loss:0.00020813829136770917\n",
            "train loss:0.00016117324921790998\n",
            "train loss:0.00019218248909186057\n",
            "train loss:0.0001757894814753256\n",
            "train loss:0.00019431844720485251\n",
            "train loss:0.00021469544262413268\n",
            "train loss:0.0002277952400991501\n",
            "train loss:0.0001518049186815597\n",
            "train loss:0.00010606985986278569\n",
            "train loss:0.00016645462395933872\n",
            "train loss:0.0001543707498265993\n",
            "train loss:0.00011150632908128342\n",
            "train loss:0.00020934493693763602\n",
            "train loss:0.00010164548725360547\n",
            "train loss:0.00014355061491723377\n",
            "train loss:0.0001924026354336904\n",
            "train loss:0.00011488459088189542\n",
            "train loss:0.00013043451069769064\n",
            "train loss:0.000113717478673251\n",
            "train loss:0.00012469347731504236\n",
            "train loss:0.00020942505044130858\n",
            "train loss:0.00015642527792035233\n",
            "train loss:0.00018397859863153047\n",
            "train loss:0.00010293416008504365\n",
            "train loss:0.00020462989021114567\n",
            "train loss:0.00015222498061730498\n",
            "train loss:0.00011948090026228672\n",
            "train loss:0.0002543896234043424\n",
            "train loss:0.00011758292451491235\n",
            "train loss:0.00021598609032304637\n",
            "train loss:0.00023328036210436252\n",
            "train loss:0.00016329544772041837\n",
            "train loss:0.0001991919465545695\n",
            "train loss:7.354909346650125e-05\n",
            "train loss:0.00010760833622120554\n",
            "train loss:0.0002556899301341645\n",
            "=== epoch:146, train acc:1.0, test acc:0.9445 ===\n",
            "train loss:0.00018214579603176764\n",
            "train loss:0.00012027036830585329\n",
            "train loss:0.00012218472129244941\n",
            "train loss:0.00019509109436739283\n",
            "train loss:0.00015594558911921259\n",
            "train loss:0.0002455751051920473\n",
            "train loss:0.0001402962608033227\n",
            "train loss:0.00012168135378865247\n",
            "train loss:0.00015735103287305445\n",
            "train loss:0.00011416359486601044\n",
            "train loss:0.0002083714912163618\n",
            "train loss:0.0002088183714607359\n",
            "train loss:0.00015700809851667275\n",
            "train loss:0.00020128838944124327\n",
            "train loss:0.00019741948204366082\n",
            "train loss:0.0003219354156170478\n",
            "train loss:0.00015716753657619238\n",
            "train loss:0.0002662496114927683\n",
            "train loss:0.0003693372859430772\n",
            "train loss:0.00017851454100508058\n",
            "train loss:0.00021993509677234666\n",
            "train loss:0.0001595411107681078\n",
            "train loss:0.0003625824774081578\n",
            "train loss:0.0001492960853283231\n",
            "train loss:7.83635024537251e-05\n",
            "train loss:0.0001352924220676519\n",
            "train loss:0.00017475962344257838\n",
            "train loss:0.00013969177643691132\n",
            "train loss:0.00017839205865956154\n",
            "train loss:0.00013542759332330079\n",
            "train loss:0.00015325325413297986\n",
            "train loss:0.00010870137877979876\n",
            "train loss:0.0002129118153622663\n",
            "train loss:0.00022238734396464298\n",
            "train loss:0.0002564599276450125\n",
            "train loss:0.00015219577927589426\n",
            "train loss:0.00016041432387273424\n",
            "train loss:0.00014593978584842945\n",
            "train loss:0.00020517763951130932\n",
            "train loss:0.0002047015883193187\n",
            "train loss:0.00012436958187252354\n",
            "train loss:0.00016588248592557623\n",
            "train loss:0.00015877743064906648\n",
            "train loss:0.00013407897208108296\n",
            "train loss:0.00021296089269995562\n",
            "train loss:0.0001240660773861985\n",
            "train loss:0.00033690957247877926\n",
            "train loss:0.00019600467923842769\n",
            "train loss:0.0001356016875928928\n",
            "train loss:0.00019929703459027303\n",
            "train loss:0.00012599807685707498\n",
            "train loss:0.0002513317363353098\n",
            "train loss:0.00014299225956785025\n",
            "train loss:0.00016603950655409212\n",
            "train loss:0.00013180589334744702\n",
            "train loss:9.908381013824697e-05\n",
            "train loss:0.00022447323830021028\n",
            "train loss:0.0002364745775179645\n",
            "train loss:0.0001568496388278388\n",
            "train loss:0.00018373773803163592\n",
            "train loss:0.0001160672249519518\n",
            "train loss:0.00015337122289025762\n",
            "train loss:0.0002762502180285351\n",
            "train loss:0.00016643757245590765\n",
            "train loss:0.0002869135672585615\n",
            "train loss:0.00016005681624473386\n",
            "train loss:0.0002238867004878203\n",
            "train loss:0.00016599202265890362\n",
            "train loss:0.00013222569670666982\n",
            "train loss:0.00016827605050985604\n",
            "train loss:0.00014237494484435972\n",
            "train loss:0.00018662551805810252\n",
            "train loss:0.0002981222910218418\n",
            "train loss:0.00028647631914994267\n",
            "train loss:0.00020346271960094606\n",
            "train loss:0.0001906277796475581\n",
            "train loss:0.000209628894599301\n",
            "train loss:0.0009321401725315186\n",
            "train loss:0.00019279261229999262\n",
            "train loss:0.0002106149463072804\n",
            "=== epoch:147, train acc:1.0, test acc:0.9395 ===\n",
            "train loss:0.0001223648903558136\n",
            "train loss:0.00016600284141141613\n",
            "train loss:0.00014769405911004623\n",
            "train loss:0.00015647486935341164\n",
            "train loss:0.00012459255911319596\n",
            "train loss:0.00020031009394072896\n",
            "train loss:0.00014435371562640585\n",
            "train loss:0.00016380689483897122\n",
            "train loss:0.000262353896254414\n",
            "train loss:0.00903718006209509\n",
            "train loss:0.0001707053691810999\n",
            "train loss:0.00012881100402610855\n",
            "train loss:0.00012687969873568196\n",
            "train loss:0.00038863354809440845\n",
            "train loss:0.00017122154470365452\n",
            "train loss:0.00024220976853635536\n",
            "train loss:0.0001863473216759057\n",
            "train loss:9.72329121805763e-05\n",
            "train loss:0.0003497009973974908\n",
            "train loss:0.00012840597458808938\n",
            "train loss:0.00040516875832089747\n",
            "train loss:0.00018383460770961336\n",
            "train loss:0.0002620524512281189\n",
            "train loss:0.00023598087270448463\n",
            "train loss:0.00019725255572094742\n",
            "train loss:0.0001470610809322525\n",
            "train loss:0.00012686544471807352\n",
            "train loss:0.0002506496024818031\n",
            "train loss:0.00014832877941749965\n",
            "train loss:0.00011904398290199007\n",
            "train loss:0.0004429481957381691\n",
            "train loss:0.00017447555828585906\n",
            "train loss:0.00017744675882285253\n",
            "train loss:0.00012170601138702136\n",
            "train loss:0.0002042143118918622\n",
            "train loss:0.0005278449017462832\n",
            "train loss:0.00024070214975928324\n",
            "train loss:0.00014682532105667395\n",
            "train loss:0.0004792260548719001\n",
            "train loss:0.0003658998876476425\n",
            "train loss:0.00043369151216726763\n",
            "train loss:0.0002304662102660438\n",
            "train loss:0.00024814198478467146\n",
            "train loss:0.0001854916281346709\n",
            "train loss:0.00014268699829920981\n",
            "train loss:0.00016803606467355394\n",
            "train loss:0.00030036881464223623\n",
            "train loss:0.00021989765605478118\n",
            "train loss:0.00033988991432439166\n",
            "train loss:0.00040376308215513373\n",
            "train loss:0.00012535053991360155\n",
            "train loss:0.00026126531919156847\n",
            "train loss:0.0003099104286598399\n",
            "train loss:0.00023210696875210874\n",
            "train loss:0.0002018738093728869\n",
            "train loss:0.0002806323052174144\n",
            "train loss:0.0004309567690067452\n",
            "train loss:0.00023053712267669918\n",
            "train loss:0.0005161673757445587\n",
            "train loss:0.00017171175090777006\n",
            "train loss:0.00028902265756150416\n",
            "train loss:0.0003012623746159671\n",
            "train loss:0.0005823759324078126\n",
            "train loss:0.00036285162084158975\n",
            "train loss:0.00028889108011350784\n",
            "train loss:0.00037413279498855605\n",
            "train loss:0.0005827363378433193\n",
            "train loss:0.0006154752111522764\n",
            "train loss:0.00019644678438697618\n",
            "train loss:0.0004494106504378034\n",
            "train loss:0.00019139807896830253\n",
            "train loss:0.0003106919423334826\n",
            "train loss:0.00022644137630350888\n",
            "train loss:0.00035644151501334446\n",
            "train loss:0.00020891142351935041\n",
            "train loss:0.0002967213972622928\n",
            "train loss:0.00027683199472833703\n",
            "train loss:0.00017480837365409734\n",
            "train loss:0.0002511652935952346\n",
            "train loss:0.00041700127570693023\n",
            "=== epoch:148, train acc:0.999875, test acc:0.9425 ===\n",
            "train loss:0.00017839336229386342\n",
            "train loss:0.00021587220766471147\n",
            "train loss:0.00024466425867905085\n",
            "train loss:0.00023308638386509043\n",
            "train loss:0.0002681923248574304\n",
            "train loss:0.00020827470564906383\n",
            "train loss:0.0002125255182182846\n",
            "train loss:0.0002446952142047328\n",
            "train loss:0.00021066572280870874\n",
            "train loss:0.0003252611540112507\n",
            "train loss:0.00015672784556040544\n",
            "train loss:0.00019871123559508095\n",
            "train loss:0.0001700918548200198\n",
            "train loss:0.00018531152403382094\n",
            "train loss:0.00019894546632155614\n",
            "train loss:0.0001429614530637463\n",
            "train loss:0.00021894371099855932\n",
            "train loss:0.00026149896215872095\n",
            "train loss:0.00020903670023398943\n",
            "train loss:0.00018907772621805784\n",
            "train loss:0.00014163908525356545\n",
            "train loss:0.00037692023381460666\n",
            "train loss:0.00016980410383559902\n",
            "train loss:0.00011624377553934945\n",
            "train loss:0.0003707655614444383\n",
            "train loss:0.00021943440220434772\n",
            "train loss:0.00023740266837997488\n",
            "train loss:0.0003298448581832173\n",
            "train loss:0.00017922177712549034\n",
            "train loss:0.0001335224824251638\n",
            "train loss:0.00033268507483462606\n",
            "train loss:0.00020350495818271158\n",
            "train loss:0.0001628401081344412\n",
            "train loss:0.00014510777207973885\n",
            "train loss:0.00014564045005337903\n",
            "train loss:0.00019726032942245358\n",
            "train loss:0.00012466240143100567\n",
            "train loss:0.00024534903335442134\n",
            "train loss:0.0001522093866048424\n",
            "train loss:0.00022195043860783538\n",
            "train loss:0.000166808407196518\n",
            "train loss:0.00017105602883679075\n",
            "train loss:0.00013455237826432388\n",
            "train loss:0.00014629936426651603\n",
            "train loss:0.0002004418738407335\n",
            "train loss:0.00019725896295815827\n",
            "train loss:0.00011749585086006837\n",
            "train loss:0.00019077144239194158\n",
            "train loss:0.0001830267308725121\n",
            "train loss:0.00018849791855225242\n",
            "train loss:0.00012522506676248326\n",
            "train loss:0.00010157387709479005\n",
            "train loss:0.00013533821311159175\n",
            "train loss:0.00010297085397505227\n",
            "train loss:0.0003029642187720748\n",
            "train loss:0.00011955316951460451\n",
            "train loss:0.00014022492899473633\n",
            "train loss:0.00011133825116652686\n",
            "train loss:0.0001261668789284982\n",
            "train loss:0.0001270270723579557\n",
            "train loss:0.0001949809552357375\n",
            "train loss:0.0001546275070035091\n",
            "train loss:0.00015985910221390367\n",
            "train loss:0.008850977065441325\n",
            "train loss:0.0001374414232396512\n",
            "train loss:0.00015065172509349798\n",
            "train loss:0.0014064763599338146\n",
            "train loss:0.00018916179433751303\n",
            "train loss:0.0001508343696396894\n",
            "train loss:0.00018314073244533467\n",
            "train loss:0.00012683611782409783\n",
            "train loss:0.00021020174378135742\n",
            "train loss:0.00038242107305257146\n",
            "train loss:0.00011233833286876315\n",
            "train loss:9.919488267933408e-05\n",
            "train loss:0.00011287038512881677\n",
            "train loss:0.0001207966755601635\n",
            "train loss:0.0003482941148997913\n",
            "train loss:0.00010823400739136177\n",
            "train loss:0.0001821117807621721\n",
            "=== epoch:149, train acc:1.0, test acc:0.9415 ===\n",
            "train loss:8.244793783597623e-05\n",
            "train loss:0.00016284072366442844\n",
            "train loss:0.00011240045600832608\n",
            "train loss:0.00102384805539497\n",
            "train loss:0.00023583029346918902\n",
            "train loss:9.657480323606573e-05\n",
            "train loss:0.00028348127016189934\n",
            "train loss:0.0001923318744198016\n",
            "train loss:0.00016252392191502035\n",
            "train loss:0.00012423483509867056\n",
            "train loss:0.00022061022811154882\n",
            "train loss:0.00038820029794335966\n",
            "train loss:0.00025619680156097735\n",
            "train loss:0.0001817479790910685\n",
            "train loss:0.00017810118998222988\n",
            "train loss:0.00032171214088788224\n",
            "train loss:0.00012229338154055245\n",
            "train loss:0.00010778597479513617\n",
            "train loss:0.00011509770471398218\n",
            "train loss:0.00013188760604176386\n",
            "train loss:0.00018153541417323897\n",
            "train loss:0.000136760099740688\n",
            "train loss:0.00014037090104549488\n",
            "train loss:0.00016827217740961767\n",
            "train loss:0.0001070449575499728\n",
            "train loss:0.00021251269911015392\n",
            "train loss:0.00023650662243360709\n",
            "train loss:0.00023602004102427407\n",
            "train loss:0.00017718889810719029\n",
            "train loss:0.00020876483467565168\n",
            "train loss:0.00018384700232049395\n",
            "train loss:0.00017174493102372024\n",
            "train loss:0.00025652819851081554\n",
            "train loss:0.00022230558760884692\n",
            "train loss:0.00018444399165723821\n",
            "train loss:0.00012606038121796653\n",
            "train loss:0.0001829183654655891\n",
            "train loss:0.00021864868384015732\n",
            "train loss:0.0001634701369406529\n",
            "train loss:0.0001383156877501899\n",
            "train loss:0.00016560042918956476\n",
            "train loss:0.00019078463244267974\n",
            "train loss:0.00022136541078485045\n",
            "train loss:0.00012262182109301788\n",
            "train loss:0.0005660643656734502\n",
            "train loss:0.00012557128004751902\n",
            "train loss:0.00011250681795728164\n",
            "train loss:0.00019410217467125677\n",
            "train loss:0.00019539379125571427\n",
            "train loss:0.0001743031916802902\n",
            "train loss:0.00011790644315553109\n",
            "train loss:0.0029560017186824896\n",
            "train loss:0.00018181149573973383\n",
            "train loss:0.000251380057447934\n",
            "train loss:0.00013868782314095112\n",
            "train loss:0.0003111706663297418\n",
            "train loss:0.0001639876737738693\n",
            "train loss:0.00023761233241603028\n",
            "train loss:0.00018259391313492918\n",
            "train loss:0.00018528578855020407\n",
            "train loss:0.0002517906605928569\n",
            "train loss:0.00022091412632925208\n",
            "train loss:0.0003195597539932435\n",
            "train loss:0.0022693893529791916\n",
            "train loss:0.00017383581173310473\n",
            "train loss:0.00028169858679560997\n",
            "train loss:0.0003639069265527985\n",
            "train loss:0.00010080032501333218\n",
            "train loss:0.00026177177481895395\n",
            "train loss:0.00017783704553944415\n",
            "train loss:0.0002684650515571849\n",
            "train loss:0.00040497522122040876\n",
            "train loss:0.00016596742727419106\n",
            "train loss:0.00014528456641332821\n",
            "train loss:0.00028149537734627547\n",
            "train loss:0.00014898110957959224\n",
            "train loss:0.00013046097553532838\n",
            "train loss:0.0001109612394482681\n",
            "train loss:0.0007813864282660576\n",
            "train loss:0.0002039664852715836\n",
            "=== epoch:150, train acc:1.0, test acc:0.944 ===\n",
            "train loss:0.00012599597135523228\n",
            "train loss:0.00015559244423758313\n",
            "train loss:0.00011457478553524008\n",
            "train loss:0.00011353557100270077\n",
            "train loss:0.00019003451214622016\n",
            "train loss:0.00045716170901254213\n",
            "train loss:0.00014183766670228406\n",
            "train loss:0.00013333160284348584\n",
            "train loss:0.0001485437569629785\n",
            "train loss:0.0003711436325604696\n",
            "train loss:0.00010257418139981929\n",
            "train loss:0.00013814862765241392\n",
            "train loss:9.612138726759243e-05\n",
            "train loss:0.00014408108715008346\n",
            "train loss:0.00026831929702966333\n",
            "train loss:0.00032370455719205006\n",
            "train loss:0.00013749874869342004\n",
            "train loss:0.00011710493083369763\n",
            "train loss:0.0003599353994685556\n",
            "train loss:0.0001928377403315554\n",
            "train loss:0.00021149198953267535\n",
            "train loss:0.00025014238207378566\n",
            "train loss:0.00039926367120837156\n",
            "train loss:0.0002145244232950741\n",
            "train loss:0.0001946516445003001\n",
            "train loss:0.00023303445461980905\n",
            "train loss:0.00021206314243437538\n",
            "train loss:0.00013949920015136473\n",
            "train loss:0.0001999667061535834\n",
            "train loss:0.00014085952129172904\n",
            "train loss:0.0005592971735996765\n",
            "train loss:0.00018904399169280853\n",
            "train loss:0.00028874238642542495\n",
            "train loss:0.00030192358234355543\n",
            "train loss:0.00026133296620437196\n",
            "train loss:0.0002136626863634147\n",
            "train loss:0.0001815203740780027\n",
            "train loss:0.00016904253584687002\n",
            "train loss:0.000133277438969622\n",
            "train loss:0.00013740813651039925\n",
            "train loss:0.00019534067794517204\n",
            "train loss:0.00027299264084032856\n",
            "train loss:0.0002118146476414415\n",
            "train loss:0.00013379014279314708\n",
            "train loss:0.00014807794486043383\n",
            "train loss:0.00027493880204538797\n",
            "train loss:0.00013790658399984827\n",
            "train loss:0.000219636956469129\n",
            "train loss:0.00014289312290506552\n",
            "train loss:0.00015846310613253613\n",
            "train loss:0.00015266513978775645\n",
            "train loss:0.00010999643495785039\n",
            "train loss:0.00016693499428327983\n",
            "train loss:0.00015653347157678328\n",
            "train loss:0.0002805937048144215\n",
            "train loss:0.00015571330711681842\n",
            "train loss:0.0009534700729687882\n",
            "train loss:0.0001222218979208269\n",
            "train loss:0.00017831312137496022\n",
            "train loss:0.00039021366638266534\n",
            "train loss:0.0004422476411725052\n",
            "train loss:0.00014727725070231852\n",
            "train loss:0.0001357467402277805\n",
            "train loss:0.0001452643741558854\n",
            "train loss:0.00014783638404859722\n",
            "train loss:0.0002749616461758678\n",
            "train loss:0.00015530950662754172\n",
            "train loss:0.0001683074460180116\n",
            "train loss:0.0002552355921995271\n",
            "train loss:0.0001685567511990703\n",
            "train loss:0.00015211636390374177\n",
            "train loss:0.0003899279607704386\n",
            "train loss:0.00012263616320590126\n",
            "train loss:0.0001185485894817777\n",
            "train loss:0.000297012249125834\n",
            "train loss:0.00011509685329920323\n",
            "train loss:0.00013288933450083997\n",
            "train loss:0.00019345185804870273\n",
            "train loss:0.0001574994176001725\n",
            "train loss:0.0002594565292220173\n",
            "=== epoch:151, train acc:1.0, test acc:0.945 ===\n",
            "train loss:0.00019039483107284407\n",
            "train loss:0.0003187820359805814\n",
            "train loss:0.00013160941078628618\n",
            "train loss:0.00037576131629693364\n",
            "train loss:0.00013304127704541135\n",
            "train loss:9.778031248854633e-05\n",
            "train loss:0.0001396693951791213\n",
            "train loss:0.0001898282035453747\n",
            "train loss:0.00013008388017020296\n",
            "train loss:0.00015564936112379517\n",
            "train loss:0.0001473305584365723\n",
            "train loss:0.00016479972724163145\n",
            "train loss:0.00018037222013688104\n",
            "train loss:0.0001383762325151783\n",
            "train loss:0.00021068415616139327\n",
            "train loss:0.0003348028498097222\n",
            "train loss:0.00014335454683823542\n",
            "train loss:0.0001563830299266469\n",
            "train loss:0.00016025066578724553\n",
            "train loss:0.0004278338416401997\n",
            "train loss:0.0002660238707019398\n",
            "train loss:0.0001432927820517708\n",
            "train loss:0.00012690307219366312\n",
            "train loss:0.00019470001993070353\n",
            "train loss:0.00026752244754840834\n",
            "train loss:0.0002376959556049463\n",
            "train loss:0.00023491712809857283\n",
            "train loss:0.00014037484066904276\n",
            "train loss:0.00020989333149236817\n",
            "train loss:0.00022142883738130437\n",
            "train loss:0.00034951427801930375\n",
            "train loss:0.00027114362175902876\n",
            "train loss:0.00020915523523640854\n",
            "train loss:0.00022823200077506795\n",
            "train loss:0.00015846432914305513\n",
            "train loss:0.00019136278401332362\n",
            "train loss:0.00017323291729370502\n",
            "train loss:0.00027442946473532425\n",
            "train loss:0.00025152780273795595\n",
            "train loss:0.00018039916615205853\n",
            "train loss:0.0001714484848360741\n",
            "train loss:0.00015459584907090353\n",
            "train loss:0.0001455002139474177\n",
            "train loss:0.0001579134827683248\n",
            "train loss:0.00011990797001540573\n",
            "train loss:0.00018126947796546667\n",
            "train loss:9.461516219436617e-05\n",
            "train loss:0.00016252470276391706\n",
            "train loss:9.323974087087175e-05\n",
            "train loss:0.00014797539885070784\n",
            "train loss:0.0001705679649995217\n",
            "train loss:0.00016121857640947628\n",
            "train loss:0.00014521770227751975\n",
            "train loss:0.0001289762150641285\n",
            "train loss:0.00022927807532872088\n",
            "train loss:0.0001749869622371799\n",
            "train loss:0.00017153560995540826\n",
            "train loss:0.00014356392913110957\n",
            "train loss:0.00020271906199536123\n",
            "train loss:0.00024505855161387446\n",
            "train loss:0.00017196131891986547\n",
            "train loss:0.00023301890917113322\n",
            "train loss:0.00016546686540871546\n",
            "train loss:0.00018288526240720416\n",
            "train loss:0.00015929992037746693\n",
            "train loss:0.0001961957843742415\n",
            "train loss:0.00015819259889267873\n",
            "train loss:0.00011812968037769144\n",
            "train loss:0.0003667686697171609\n",
            "train loss:0.00021494895278358406\n",
            "train loss:0.00015389376022087838\n",
            "train loss:0.00020856474999820314\n",
            "train loss:0.0002213287162187328\n",
            "train loss:0.0002558049849737709\n",
            "train loss:0.00027769906109998506\n",
            "train loss:0.00011167737632574015\n",
            "train loss:0.00015970436811049254\n",
            "train loss:0.00013050915371328346\n",
            "train loss:0.00014792293215520248\n",
            "train loss:0.00015229478418396405\n",
            "=== epoch:152, train acc:0.999875, test acc:0.9425 ===\n",
            "train loss:0.00010637201948162557\n",
            "train loss:0.00012767773563186635\n",
            "train loss:0.0002988667418616714\n",
            "train loss:0.00015814945921050004\n",
            "train loss:0.00036930242168039927\n",
            "train loss:0.00016356627337027905\n",
            "train loss:0.00012765909524816926\n",
            "train loss:0.000164320891086491\n",
            "train loss:0.00015753097934990902\n",
            "train loss:0.00033682637451776514\n",
            "train loss:0.0008474906491095756\n",
            "train loss:0.00017199285699277152\n",
            "train loss:0.00012814768862568946\n",
            "train loss:0.00016414238426310933\n",
            "train loss:0.0001424471093108845\n",
            "train loss:0.0001076595116705888\n",
            "train loss:0.00021362812426979573\n",
            "train loss:0.00023755721369803285\n",
            "train loss:0.0003989300514486379\n",
            "train loss:0.00015850528836686698\n",
            "train loss:0.00039280062726723485\n",
            "train loss:0.00014916337625219876\n",
            "train loss:0.00016652726628331384\n",
            "train loss:0.00016688531462696768\n",
            "train loss:0.00022769085066080635\n",
            "train loss:0.00012141626054632646\n",
            "train loss:0.00026557845039588116\n",
            "train loss:0.00021547849175814634\n",
            "train loss:0.000157897542731936\n",
            "train loss:0.0003328491248349223\n",
            "train loss:0.0002864861045507832\n",
            "train loss:0.0001481304418523223\n",
            "train loss:0.00016598633160546373\n",
            "train loss:0.00017473521735296253\n",
            "train loss:0.00014145418947442192\n",
            "train loss:0.00019761759554252288\n",
            "train loss:0.00017892937988321436\n",
            "train loss:0.00015372300418532023\n",
            "train loss:0.00014064269749241657\n",
            "train loss:0.00015378451539268566\n",
            "train loss:0.00014961510688445408\n",
            "train loss:0.00012884335174211497\n",
            "train loss:0.00013756242995831812\n",
            "train loss:0.00015020987354157832\n",
            "train loss:0.0002093838837166078\n",
            "train loss:0.0001473753038318823\n",
            "train loss:0.00017464295954395718\n",
            "train loss:0.00010231980001290764\n",
            "train loss:0.0001596502322192647\n",
            "train loss:0.00020860781989778321\n",
            "train loss:0.00016880175058433245\n",
            "train loss:0.0001784895977589012\n",
            "train loss:0.00017681575742873102\n",
            "train loss:0.00013365722585977443\n",
            "train loss:0.00030131878349098294\n",
            "train loss:0.00021367084275728164\n",
            "train loss:0.0001909415131890715\n",
            "train loss:0.00019269786104904131\n",
            "train loss:0.00025588333488535663\n",
            "train loss:0.00016961011381942678\n",
            "train loss:0.00015058089027091958\n",
            "train loss:0.0003708652009559378\n",
            "train loss:0.0001655053294066594\n",
            "train loss:0.0001868355477343783\n",
            "train loss:0.00020131458481979021\n",
            "train loss:0.000164280633347904\n",
            "train loss:0.00013917252322595355\n",
            "train loss:0.0001418413072686681\n",
            "train loss:9.39935274005109e-05\n",
            "train loss:0.00016464366955968628\n",
            "train loss:0.0001039056500633638\n",
            "train loss:0.00014226518727346064\n",
            "train loss:0.00011655895959224675\n",
            "train loss:0.0001676983206486132\n",
            "train loss:0.00018680251454906088\n",
            "train loss:0.0007527120390945949\n",
            "train loss:0.00011399438117514136\n",
            "train loss:0.0001307723970249713\n",
            "train loss:0.0001312729659913074\n",
            "train loss:9.913475916799641e-05\n",
            "=== epoch:153, train acc:0.999875, test acc:0.9405 ===\n",
            "train loss:9.692160105478479e-05\n",
            "train loss:0.000237345489379343\n",
            "train loss:0.00013190393886495892\n",
            "train loss:0.0001422831327291454\n",
            "train loss:0.00019985450293476248\n",
            "train loss:0.00015530812448482018\n",
            "train loss:0.00014267448397396935\n",
            "train loss:0.00012132236190985151\n",
            "train loss:0.00013093307789475178\n",
            "train loss:0.00020384646119895618\n",
            "train loss:0.00010129956247920578\n",
            "train loss:0.0001581727828909812\n",
            "train loss:0.00013069244057625187\n",
            "train loss:0.0001815607593207454\n",
            "train loss:0.00019368078140085987\n",
            "train loss:0.0001767331583808969\n",
            "train loss:0.00013131495521079243\n",
            "train loss:0.0002204728640720627\n",
            "train loss:0.00015415568140022832\n",
            "train loss:0.00012728072579474263\n",
            "train loss:0.00012938543802520162\n",
            "train loss:0.00016498338476318898\n",
            "train loss:7.667630031116456e-05\n",
            "train loss:0.00011458769706615543\n",
            "train loss:0.00046897542620186417\n",
            "train loss:0.00018455905632031974\n",
            "train loss:0.0001466804163179804\n",
            "train loss:0.00014064242508392055\n",
            "train loss:0.00011873732339794427\n",
            "train loss:9.413369597292449e-05\n",
            "train loss:0.0003125151661248485\n",
            "train loss:0.0001738715251020986\n",
            "train loss:0.00017515500845280836\n",
            "train loss:0.00032981385182981473\n",
            "train loss:0.0001283067954880227\n",
            "train loss:0.00012316548599381762\n",
            "train loss:0.00014829614335107314\n",
            "train loss:0.00015613690515415383\n",
            "train loss:0.0001846583029768228\n",
            "train loss:0.0001228557420755707\n",
            "train loss:0.00017165228709641056\n",
            "train loss:0.00012133315657520655\n",
            "train loss:0.00014646327443443828\n",
            "train loss:0.0001505499676329758\n",
            "train loss:0.00019974313350929924\n",
            "train loss:0.00010480195030407554\n",
            "train loss:0.00013871198970192403\n",
            "train loss:0.00023831111773573856\n",
            "train loss:0.00018415238658982833\n",
            "train loss:0.00022590923233155297\n",
            "train loss:0.0001486912376833346\n",
            "train loss:0.00012322978090530164\n",
            "train loss:0.00011114151202609279\n",
            "train loss:0.0001422810814851428\n",
            "train loss:0.00016452984871919606\n",
            "train loss:0.00012670179817423317\n",
            "train loss:0.0001350296220105317\n",
            "train loss:0.00011658143327873817\n",
            "train loss:9.375062107446049e-05\n",
            "train loss:0.00016521155508859647\n",
            "train loss:0.0003093849450490021\n",
            "train loss:0.0001534451550526576\n",
            "train loss:0.0002768622114370553\n",
            "train loss:0.00011716958186733287\n",
            "train loss:0.00010885372445408667\n",
            "train loss:0.00014955599949270074\n",
            "train loss:0.0001488971265147187\n",
            "train loss:9.693671781742474e-05\n",
            "train loss:0.00010036541375774163\n",
            "train loss:0.00017472183823739055\n",
            "train loss:0.00010264366002907511\n",
            "train loss:0.00011779244214729477\n",
            "train loss:0.00029370211726732277\n",
            "train loss:0.00012939868531157653\n",
            "train loss:0.000162447985071192\n",
            "train loss:7.617542946480159e-05\n",
            "train loss:9.864612678266041e-05\n",
            "train loss:0.00013770395620752357\n",
            "train loss:0.00021368933884509882\n",
            "train loss:0.00011881155820062662\n",
            "=== epoch:154, train acc:1.0, test acc:0.943 ===\n",
            "train loss:0.00011693027140455311\n",
            "train loss:0.0013373139014077823\n",
            "train loss:0.0001173637571570046\n",
            "train loss:0.00011037684601310277\n",
            "train loss:0.00010883560425055073\n",
            "train loss:0.0002129169121402398\n",
            "train loss:9.206024524035333e-05\n",
            "train loss:0.00011301981184162448\n",
            "train loss:9.05198523202896e-05\n",
            "train loss:0.00010973187204224124\n",
            "train loss:0.00014358188837078467\n",
            "train loss:0.00013836480402571754\n",
            "train loss:0.0001594940535298161\n",
            "train loss:0.00019536326747273924\n",
            "train loss:0.00012522260979093612\n",
            "train loss:0.0001543822333281994\n",
            "train loss:0.0002115814092806041\n",
            "train loss:0.00019795833799422975\n",
            "train loss:0.0007384083700883172\n",
            "train loss:0.00018086755370236854\n",
            "train loss:0.00016314107103932194\n",
            "train loss:0.0001769639029684862\n",
            "train loss:0.00011456456335219429\n",
            "train loss:0.00024528011035245066\n",
            "train loss:0.00018926490279976734\n",
            "train loss:0.0002544756702459055\n",
            "train loss:0.00012516842962118684\n",
            "train loss:0.0001972885190687589\n",
            "train loss:0.00014768937484940903\n",
            "train loss:0.00023029984473173587\n",
            "train loss:0.00022693054249995408\n",
            "train loss:0.0003906650905294696\n",
            "train loss:0.0004332938142768315\n",
            "train loss:0.00015559682901235917\n",
            "train loss:0.0003763712503177225\n",
            "train loss:0.00016304163546462477\n",
            "train loss:0.00016476831951059694\n",
            "train loss:0.00035609561150790276\n",
            "train loss:0.00014130385306482303\n",
            "train loss:0.00020821657614985322\n",
            "train loss:0.0003303124942381691\n",
            "train loss:0.0002012626979707336\n",
            "train loss:0.00016271816388288335\n",
            "train loss:0.0003421988292337218\n",
            "train loss:0.0001259949555463975\n",
            "train loss:0.00022487798375481517\n",
            "train loss:0.00013351283480730438\n",
            "train loss:0.00020703863434099512\n",
            "train loss:0.0002901445418743531\n",
            "train loss:0.00024152940486751683\n",
            "train loss:0.0002366157262446311\n",
            "train loss:0.00014824997010955726\n",
            "train loss:0.00015174406153960194\n",
            "train loss:0.0007123119959371305\n",
            "train loss:0.00020360352212933075\n",
            "train loss:0.0001250525961732277\n",
            "train loss:0.00015721639952355273\n",
            "train loss:0.0002063799531034355\n",
            "train loss:0.0001998061286467007\n",
            "train loss:0.00017148667124035794\n",
            "train loss:0.0003096938417993546\n",
            "train loss:0.00012315869046974605\n",
            "train loss:0.0001208051827747679\n",
            "train loss:0.0037407999351274745\n",
            "train loss:0.00014145524491404073\n",
            "train loss:0.0003434896731455816\n",
            "train loss:0.0001151240372162752\n",
            "train loss:9.334180306246673e-05\n",
            "train loss:0.0006404720142123669\n",
            "train loss:0.0001542289145823954\n",
            "train loss:0.0001294522858315879\n",
            "train loss:0.00015134088080335324\n",
            "train loss:0.00021964692550832454\n",
            "train loss:0.00022432237488646283\n",
            "train loss:0.0002468182507688507\n",
            "train loss:0.00010396664282832181\n",
            "train loss:0.00012517622778776162\n",
            "train loss:0.0003051607731918364\n",
            "train loss:0.00014373972471905933\n",
            "train loss:0.00014311213972338963\n",
            "=== epoch:155, train acc:1.0, test acc:0.943 ===\n",
            "train loss:0.00012497486986226615\n",
            "train loss:0.00012105123621137265\n",
            "train loss:0.000282245120826695\n",
            "train loss:0.00010447893079601498\n",
            "train loss:0.00018001916685886094\n",
            "train loss:9.964181857502545e-05\n",
            "train loss:0.0001212631266665771\n",
            "train loss:0.00010887785676831796\n",
            "train loss:0.00014871505008520468\n",
            "train loss:0.000181058052408097\n",
            "train loss:0.00011248769164769532\n",
            "train loss:0.00011638339265229279\n",
            "train loss:9.390258400691463e-05\n",
            "train loss:6.94995859883442e-05\n",
            "train loss:0.00010482061639861577\n",
            "train loss:0.00012338895176152765\n",
            "train loss:0.00010718580214764838\n",
            "train loss:0.0001101265158000995\n",
            "train loss:0.0001403827407074497\n",
            "train loss:7.47053953702046e-05\n",
            "train loss:0.0003017554792956362\n",
            "train loss:8.43497259719785e-05\n",
            "train loss:0.0004019074051423381\n",
            "train loss:0.0001329314867107047\n",
            "train loss:0.00034758422817653475\n",
            "train loss:0.0002959739597937796\n",
            "train loss:0.00020339594127139184\n",
            "train loss:0.0002718347032547759\n",
            "train loss:0.00011504474622469517\n",
            "train loss:0.0002773194183506467\n",
            "train loss:0.00014940828423321278\n",
            "train loss:0.00012417245181142636\n",
            "train loss:0.00017475995581867402\n",
            "train loss:0.00011684120494344214\n",
            "train loss:0.00034127043599873957\n",
            "train loss:0.0002353962806776186\n",
            "train loss:0.00011037736152161037\n",
            "train loss:0.00018530908526229035\n",
            "train loss:0.00014855121320303847\n",
            "train loss:0.00014677604635554228\n",
            "train loss:0.00021744798405952056\n",
            "train loss:0.00017210866078580018\n",
            "train loss:0.00034132946216835006\n",
            "train loss:0.0001353986055032722\n",
            "train loss:0.0003787680996696689\n",
            "train loss:0.00013141982042785342\n",
            "train loss:0.0003544951991620642\n",
            "train loss:0.0002079023531016226\n",
            "train loss:0.00023695704536668278\n",
            "train loss:0.00024410520923417048\n",
            "train loss:0.00017335467998103927\n",
            "train loss:0.00010312399287780959\n",
            "train loss:9.015128256509408e-05\n",
            "train loss:0.00014902676252171671\n",
            "train loss:0.00011262333049300485\n",
            "train loss:0.00011009215649952032\n",
            "train loss:0.0001404710592916451\n",
            "train loss:0.0016972374493782662\n",
            "train loss:0.0012279902478603687\n",
            "train loss:0.00014560125899241782\n",
            "train loss:0.00012415976012830935\n",
            "train loss:0.0001008076759475003\n",
            "train loss:0.00012855865340153166\n",
            "train loss:0.00012588087570908376\n",
            "train loss:0.00014518698656048974\n",
            "train loss:0.0002321055287276272\n",
            "train loss:8.859481324706934e-05\n",
            "train loss:0.0005123117171339723\n",
            "train loss:0.00022371962518831476\n",
            "train loss:0.0001619027659868698\n",
            "train loss:0.00022561591463888774\n",
            "train loss:0.000330127058121075\n",
            "train loss:0.00013912633428350013\n",
            "train loss:0.00016226753875569005\n",
            "train loss:0.00013926596506549655\n",
            "train loss:0.00028367789122671523\n",
            "train loss:0.0001468664902419385\n",
            "train loss:0.00014467285020849355\n",
            "train loss:0.00022170449454319833\n",
            "train loss:0.00015113491292144277\n",
            "=== epoch:156, train acc:1.0, test acc:0.9445 ===\n",
            "train loss:0.0005211665730951806\n",
            "train loss:0.00020850681670799318\n",
            "train loss:0.00017787757408254892\n",
            "train loss:0.0001681809648582353\n",
            "train loss:0.0002122736058504283\n",
            "train loss:0.00027457892074240993\n",
            "train loss:0.00017535561706080228\n",
            "train loss:0.0004259407844606677\n",
            "train loss:0.00020916689460357893\n",
            "train loss:9.860473431092681e-05\n",
            "train loss:6.660818314630098e-05\n",
            "train loss:0.00023478301169670862\n",
            "train loss:0.00010771812364281554\n",
            "train loss:0.00016160247000923265\n",
            "train loss:9.144407115900451e-05\n",
            "train loss:0.00019039480435750585\n",
            "train loss:0.00011143520351650215\n",
            "train loss:0.00020047941739718343\n",
            "train loss:0.0001781809172925978\n",
            "train loss:0.00013735210263456934\n",
            "train loss:9.844156127996257e-05\n",
            "train loss:0.00015865536616882815\n",
            "train loss:0.0002051260159286497\n",
            "train loss:0.00013876894725078993\n",
            "train loss:0.00015848831574835585\n",
            "train loss:0.00028833467550673937\n",
            "train loss:0.0001424124248475898\n",
            "train loss:0.00017614211744846272\n",
            "train loss:0.0002655429815516003\n",
            "train loss:0.00024728401816216596\n",
            "train loss:0.00023555806879535662\n",
            "train loss:0.0002365248126213601\n",
            "train loss:0.00013838191075071728\n",
            "train loss:0.00019145080488809855\n",
            "train loss:0.00020202085266753683\n",
            "train loss:0.00019726406345629951\n",
            "train loss:0.00029466134724100883\n",
            "train loss:0.0002484168135466205\n",
            "train loss:0.00017314760567940974\n",
            "train loss:0.00011082422864538326\n",
            "train loss:0.0005077460661461921\n",
            "train loss:0.00014613115515230417\n",
            "train loss:0.0002387094420992501\n",
            "train loss:0.00019106778721011614\n",
            "train loss:0.00024345492911683863\n",
            "train loss:0.0002555543412812343\n",
            "train loss:0.00018573196901644512\n",
            "train loss:0.00015684458427140774\n",
            "train loss:0.0006028273740131055\n",
            "train loss:0.0004174573070107573\n",
            "train loss:0.00015984685778123745\n",
            "train loss:0.00023596220258884418\n",
            "train loss:0.0003140440543982411\n",
            "train loss:0.00024077219583665168\n",
            "train loss:0.00016781166210810093\n",
            "train loss:0.00013533241358148882\n",
            "train loss:9.453342971291041e-05\n",
            "train loss:0.00019837466290567216\n",
            "train loss:0.00015540389384298848\n",
            "train loss:9.140695448999283e-05\n",
            "train loss:0.00016063154605146912\n",
            "train loss:0.00020212068337763258\n",
            "train loss:0.0003025459185784876\n",
            "train loss:0.0001574293773837761\n",
            "train loss:0.00014120711748880736\n",
            "train loss:0.00013599297448301258\n",
            "train loss:0.0005662334852190715\n",
            "train loss:0.0002249913189831569\n",
            "train loss:0.0001137260224500609\n",
            "train loss:0.00021211880462021553\n",
            "train loss:0.0001524951776615629\n",
            "train loss:0.00011860927923596515\n",
            "train loss:0.00016819644432298047\n",
            "train loss:0.00033226698250665774\n",
            "train loss:0.00019135232223122181\n",
            "train loss:0.00017500264063843158\n",
            "train loss:0.0002615305332621212\n",
            "train loss:0.0002163533519082933\n",
            "train loss:0.00046324635870127295\n",
            "train loss:0.00026234086180625604\n",
            "=== epoch:157, train acc:1.0, test acc:0.9395 ===\n",
            "train loss:0.00034065622319872475\n",
            "train loss:0.00043793996614646083\n",
            "train loss:0.0002307837717075398\n",
            "train loss:0.0003263130364913632\n",
            "train loss:9.857570670033978e-05\n",
            "train loss:0.0001097071105222785\n",
            "train loss:0.00010159662328121154\n",
            "train loss:0.00018405363734486611\n",
            "train loss:9.317498388687917e-05\n",
            "train loss:0.0001565071694695305\n",
            "train loss:0.00011334094488364636\n",
            "train loss:0.0001229332589682139\n",
            "train loss:7.900382958429713e-05\n",
            "train loss:0.00025749567475450244\n",
            "train loss:7.846886660134843e-05\n",
            "train loss:0.00016075895723236872\n",
            "train loss:0.0001983731879556245\n",
            "train loss:7.536096525276157e-05\n",
            "train loss:0.00016583687460580124\n",
            "train loss:0.00010331770704192472\n",
            "train loss:0.0001514312091955439\n",
            "train loss:0.00014452386396287046\n",
            "train loss:0.0002574397546444588\n",
            "train loss:0.00010922972565277645\n",
            "train loss:0.00022073217817952787\n",
            "train loss:0.00016118617820606428\n",
            "train loss:0.00013373540855896474\n",
            "train loss:0.00038620765894950316\n",
            "train loss:0.00011900120218985914\n",
            "train loss:0.00033464459458925337\n",
            "train loss:0.0001233515004298418\n",
            "train loss:0.00017018570226833867\n",
            "train loss:0.00012352614349548434\n",
            "train loss:0.00014674551505056529\n",
            "train loss:0.00012036431345974662\n",
            "train loss:0.0001510967463440964\n",
            "train loss:0.00016806640034650996\n",
            "train loss:0.00013263310251036863\n",
            "train loss:0.00015875761724269815\n",
            "train loss:0.00019584150658188\n",
            "train loss:0.00012148062301057834\n",
            "train loss:0.0001696625406093602\n",
            "train loss:0.00021284508259390326\n",
            "train loss:0.00019908297207422719\n",
            "train loss:0.00010737696468296725\n",
            "train loss:0.0013788316390629407\n",
            "train loss:0.0001466727740154914\n",
            "train loss:0.0002401795357004537\n",
            "train loss:0.00017937168509341443\n",
            "train loss:0.0007547451264599697\n",
            "train loss:0.0003090487319276889\n",
            "train loss:0.00011394370708198258\n",
            "train loss:0.00010065936499295353\n",
            "train loss:0.00019798917283351633\n",
            "train loss:0.00032979883096302025\n",
            "train loss:0.000212139459668453\n",
            "train loss:0.00019096239483829984\n",
            "train loss:0.00017110279486104243\n",
            "train loss:0.00014639667822857177\n",
            "train loss:0.0002858252457057822\n",
            "train loss:0.0002045105512076324\n",
            "train loss:0.00018224840150728075\n",
            "train loss:0.00019004221222319534\n",
            "train loss:0.0006048507196301916\n",
            "train loss:0.00016475443850997025\n",
            "train loss:0.00025739706924225026\n",
            "train loss:0.0001033425945413953\n",
            "train loss:0.0002008130329679463\n",
            "train loss:0.00021043791188345422\n",
            "train loss:0.0001934593641135703\n",
            "train loss:0.0001737896451099277\n",
            "train loss:0.00010284316312960746\n",
            "train loss:0.00011769347705296065\n",
            "train loss:0.00014365325979956057\n",
            "train loss:0.00011592287385457027\n",
            "train loss:0.000162643260443248\n",
            "train loss:0.00022661406621806107\n",
            "train loss:0.00015767263364238886\n",
            "train loss:0.00013708296942747407\n",
            "train loss:0.00013276600219767383\n",
            "=== epoch:158, train acc:1.0, test acc:0.9445 ===\n",
            "train loss:0.00010393230884178947\n",
            "train loss:0.0001729790747907631\n",
            "train loss:0.00020753239005585685\n",
            "train loss:0.0001094931122143943\n",
            "train loss:0.0002485199835047557\n",
            "train loss:0.00014262532244037156\n",
            "train loss:0.00022309113598775756\n",
            "train loss:0.00018040045950637817\n",
            "train loss:0.00010579989994211292\n",
            "train loss:0.00016190096556495037\n",
            "train loss:0.0001094485058343023\n",
            "train loss:0.00017792857122127966\n",
            "train loss:0.00019095617236574293\n",
            "train loss:0.0001461496141043959\n",
            "train loss:0.0005976653434285406\n",
            "train loss:0.0001798626165938832\n",
            "train loss:0.0005040322402284455\n",
            "train loss:0.0016128409329972456\n",
            "train loss:0.00012326828903495965\n",
            "train loss:0.00013534472526735835\n",
            "train loss:0.00015375902566933282\n",
            "train loss:0.00023281953395371918\n",
            "train loss:0.0001676884341727194\n",
            "train loss:0.00010131887703303925\n",
            "train loss:0.00023741074187565094\n",
            "train loss:0.00010156765594764413\n",
            "train loss:0.00010596180824573434\n",
            "train loss:0.00016608075924904388\n",
            "train loss:0.00020574694204323867\n",
            "train loss:0.0001076408795713848\n",
            "train loss:0.00011224854537970566\n",
            "train loss:0.00019062987467250144\n",
            "train loss:0.0001747610567550297\n",
            "train loss:0.00012331788597830094\n",
            "train loss:0.00015214333359813938\n",
            "train loss:0.00029086250733289685\n",
            "train loss:0.0001674171005931865\n",
            "train loss:0.00019338950508876262\n",
            "train loss:0.0003416771655000078\n",
            "train loss:0.004435263974033111\n",
            "train loss:0.00017043558950656534\n",
            "train loss:0.00019168735170707233\n",
            "train loss:0.00015419027801832056\n",
            "train loss:0.0001606087539754678\n",
            "train loss:0.000182595898671922\n",
            "train loss:0.0003423949992433764\n",
            "train loss:0.00013267435303508605\n",
            "train loss:0.00011761810630527914\n",
            "train loss:0.00016701828264527736\n",
            "train loss:0.00019586682918964384\n",
            "train loss:0.00013794406008487098\n",
            "train loss:0.0004728521649407585\n",
            "train loss:0.00012585000104382794\n",
            "train loss:0.0002452314938274547\n",
            "train loss:0.00012405720188652582\n",
            "train loss:0.00010311014789998364\n",
            "train loss:0.00013463202779993776\n",
            "train loss:0.00012496845481227962\n",
            "train loss:9.182581487519678e-05\n",
            "train loss:0.00018334175778230103\n",
            "train loss:0.000450903162107101\n",
            "train loss:0.00013676262390525744\n",
            "train loss:0.0001293446155980724\n",
            "train loss:0.00012432024715255854\n",
            "train loss:0.00012479577084306364\n",
            "train loss:0.00012068876292183037\n",
            "train loss:0.00010057462030150947\n",
            "train loss:0.00019991823539802588\n",
            "train loss:8.197697506025492e-05\n",
            "train loss:0.0001684428892600319\n",
            "train loss:0.00016763249809852545\n",
            "train loss:0.00017359819861772024\n",
            "train loss:0.00010923513874301045\n",
            "train loss:0.00010102128824325267\n",
            "train loss:0.00032077384711050343\n",
            "train loss:0.0001279698855482619\n",
            "train loss:8.773868599137959e-05\n",
            "train loss:0.00014412316755797307\n",
            "train loss:0.00013291219059831052\n",
            "train loss:0.00011256846442054001\n",
            "=== epoch:159, train acc:1.0, test acc:0.9395 ===\n",
            "train loss:0.0001290764582576693\n",
            "train loss:0.00011354797214111877\n",
            "train loss:0.00012102107466580662\n",
            "train loss:6.973208851512241e-05\n",
            "train loss:0.00010145407223383383\n",
            "train loss:8.32456284167209e-05\n",
            "train loss:0.00012839819741745063\n",
            "train loss:8.439697463881743e-05\n",
            "train loss:8.334367762704987e-05\n",
            "train loss:6.544824171181017e-05\n",
            "train loss:0.00010171494686282764\n",
            "train loss:7.806588060220463e-05\n",
            "train loss:9.823789792068322e-05\n",
            "train loss:7.504446055811104e-05\n",
            "train loss:6.880313163037487e-05\n",
            "train loss:7.594285992756558e-05\n",
            "train loss:0.00010163315057085152\n",
            "train loss:0.0001033997437344898\n",
            "train loss:9.66898132777738e-05\n",
            "train loss:0.00012305067073612952\n",
            "train loss:0.00016576412923946117\n",
            "train loss:9.500623552245383e-05\n",
            "train loss:0.00018214333272026145\n",
            "train loss:0.00013174489672131522\n",
            "train loss:0.00016892813619711123\n",
            "train loss:0.00019049270883595253\n",
            "train loss:0.00015772805608021251\n",
            "train loss:0.0002398089022575328\n",
            "train loss:0.0001961661254870587\n",
            "train loss:0.00044463643100973234\n",
            "train loss:0.0001273061575719665\n",
            "train loss:0.000164965131008001\n",
            "train loss:0.00012941880194525848\n",
            "train loss:0.00026135655687247123\n",
            "train loss:0.000125071057988708\n",
            "train loss:0.00015645278319565426\n",
            "train loss:0.0001958892440625872\n",
            "train loss:0.000156686456920125\n",
            "train loss:0.0002637232331996774\n",
            "train loss:0.00010171822183593945\n",
            "train loss:0.00016958549137365767\n",
            "train loss:0.00015421394597072655\n",
            "train loss:0.00011263203261594636\n",
            "train loss:0.0006682254454308165\n",
            "train loss:0.0001012806520590261\n",
            "train loss:9.451331406135949e-05\n",
            "train loss:0.0001191832571061718\n",
            "train loss:0.00015798811977520908\n",
            "train loss:0.00018893831612103932\n",
            "train loss:0.00015590922770991576\n",
            "train loss:0.00014219759426503886\n",
            "train loss:0.0001634914947218756\n",
            "train loss:0.0002180976147737796\n",
            "train loss:9.030567554148813e-05\n",
            "train loss:0.00019325446981429436\n",
            "train loss:0.00014860094772103168\n",
            "train loss:0.000117637793813219\n",
            "train loss:0.00015776489929534578\n",
            "train loss:0.00011150287282833237\n",
            "train loss:0.00013791970550291126\n",
            "train loss:0.0002328302020529131\n",
            "train loss:0.00011005512178521779\n",
            "train loss:0.00014191574720277377\n",
            "train loss:0.00011077317701305006\n",
            "train loss:9.558021908787382e-05\n",
            "train loss:0.0001681463277548825\n",
            "train loss:0.00010380648360180188\n",
            "train loss:0.00010066560045156835\n",
            "train loss:0.0001405885742713592\n",
            "train loss:9.824494366576297e-05\n",
            "train loss:8.590693720717366e-05\n",
            "train loss:0.00015618278776147223\n",
            "train loss:0.00011670837709580853\n",
            "train loss:0.00022329141236458543\n",
            "train loss:0.0001401960651373758\n",
            "train loss:0.00012178383737063944\n",
            "train loss:0.0001358029518455801\n",
            "train loss:7.967435870246675e-05\n",
            "train loss:0.00015415104546547345\n",
            "train loss:0.000157611138087152\n",
            "=== epoch:160, train acc:1.0, test acc:0.946 ===\n",
            "train loss:0.00014660516494517808\n",
            "train loss:0.000144638396358791\n",
            "train loss:0.00014717983660551546\n",
            "train loss:0.00015563987109950376\n",
            "train loss:9.700244503115938e-05\n",
            "train loss:0.00014552598994343267\n",
            "train loss:0.00013224110392802952\n",
            "train loss:0.00024825528909314635\n",
            "train loss:0.00010121134549183355\n",
            "train loss:0.00015487610943228466\n",
            "train loss:0.00010819591142335319\n",
            "train loss:9.006847268878647e-05\n",
            "train loss:8.017802152470024e-05\n",
            "train loss:9.919065924583952e-05\n",
            "train loss:0.00014609913937328736\n",
            "train loss:0.00010538918475851803\n",
            "train loss:9.300054013515433e-05\n",
            "train loss:7.521642066118355e-05\n",
            "train loss:0.00010707439547409284\n",
            "train loss:0.00012785244967948142\n",
            "train loss:0.0003374625813856528\n",
            "train loss:7.946901840639142e-05\n",
            "train loss:0.00012802267309286426\n",
            "train loss:0.00018800574539003703\n",
            "train loss:0.0001533569371908663\n",
            "train loss:9.638836923675533e-05\n",
            "train loss:9.413954609136027e-05\n",
            "train loss:0.00018271225371079805\n",
            "train loss:0.0001398286368825576\n",
            "train loss:9.411373511593182e-05\n",
            "train loss:9.908660472459884e-05\n",
            "train loss:0.00012198272696371054\n",
            "train loss:0.00012883543293002998\n",
            "train loss:0.0001671728293838442\n",
            "train loss:9.766314122188982e-05\n",
            "train loss:0.00015718403750674862\n",
            "train loss:0.00014990704283786562\n",
            "train loss:0.00019537514253030488\n",
            "train loss:9.058457325822038e-05\n",
            "train loss:0.00011996962720966939\n",
            "train loss:9.634700275456787e-05\n",
            "train loss:8.891713454752529e-05\n",
            "train loss:0.0001475482277951912\n",
            "train loss:0.00012377930746853506\n",
            "train loss:0.00013375348562481352\n",
            "train loss:0.00013285374462140525\n",
            "train loss:0.00017815722705525147\n",
            "train loss:0.00013230470357340163\n",
            "train loss:0.00012964433180669136\n",
            "train loss:0.0001815797737873415\n",
            "train loss:7.266034040805747e-05\n",
            "train loss:0.00010009518667205486\n",
            "train loss:9.551156317617831e-05\n",
            "train loss:0.00011104288066814866\n",
            "train loss:0.0002938736234928986\n",
            "train loss:0.0002524478551857557\n",
            "train loss:0.00013346236952119553\n",
            "train loss:7.620858064002517e-05\n",
            "train loss:8.270154920299415e-05\n",
            "train loss:9.836437445923032e-05\n",
            "train loss:8.647651290013894e-05\n",
            "train loss:7.592648444660184e-05\n",
            "train loss:0.0006304720461144728\n",
            "train loss:8.423672264490301e-05\n",
            "train loss:0.0001241880340806316\n",
            "train loss:0.00010997003480658343\n",
            "train loss:0.00011667239355630099\n",
            "train loss:0.0001071811672629311\n",
            "train loss:0.000749175194100831\n",
            "train loss:0.0001237209147932988\n",
            "train loss:8.978207402082303e-05\n",
            "train loss:0.00011006355235102499\n",
            "train loss:0.00025428311689871337\n",
            "train loss:0.0003781236851258845\n",
            "train loss:9.157159961774999e-05\n",
            "train loss:0.00011804819379275682\n",
            "train loss:0.000110535806938116\n",
            "train loss:0.00029508053935564595\n",
            "train loss:0.0001289076763338145\n",
            "train loss:8.043827169948022e-05\n",
            "=== epoch:161, train acc:1.0, test acc:0.9485 ===\n",
            "train loss:0.0001907179961977422\n",
            "train loss:9.301575490949485e-05\n",
            "train loss:6.979255549496238e-05\n",
            "train loss:0.00011901462365962132\n",
            "train loss:0.00021275903074235362\n",
            "train loss:8.636220986106843e-05\n",
            "train loss:0.00012076993797909944\n",
            "train loss:9.199898256272463e-05\n",
            "train loss:0.00011391036897561734\n",
            "train loss:7.68747711931157e-05\n",
            "train loss:0.0001236286823329543\n",
            "train loss:8.777026946264154e-05\n",
            "train loss:0.00011802865390265045\n",
            "train loss:9.58640975254655e-05\n",
            "train loss:0.00024636051082969673\n",
            "train loss:0.0001071939076254797\n",
            "train loss:0.0001265689442241992\n",
            "train loss:0.00011872839099197396\n",
            "train loss:8.513210129041847e-05\n",
            "train loss:5.992236298077498e-05\n",
            "train loss:7.361636521441894e-05\n",
            "train loss:0.00029155078392650434\n",
            "train loss:0.0001224523524520648\n",
            "train loss:0.00012355625448456028\n",
            "train loss:8.857463881703286e-05\n",
            "train loss:0.00011559561367783817\n",
            "train loss:9.340471948174757e-05\n",
            "train loss:0.00012126088602253312\n",
            "train loss:7.424937243795126e-05\n",
            "train loss:0.00016573450992251197\n",
            "train loss:0.00019835270833866285\n",
            "train loss:0.00011891803397823387\n",
            "train loss:8.583310368728824e-05\n",
            "train loss:8.690094096348779e-05\n",
            "train loss:8.3043104517614e-05\n",
            "train loss:0.00018171078249397103\n",
            "train loss:9.670351306088772e-05\n",
            "train loss:0.0003985096177795354\n",
            "train loss:0.00013350450548571936\n",
            "train loss:0.00012343222132388696\n",
            "train loss:0.00018117646886927334\n",
            "train loss:9.585173044068775e-05\n",
            "train loss:0.00014097201394047286\n",
            "train loss:0.00012623474059250033\n",
            "train loss:0.00017354461846252726\n",
            "train loss:0.00014639702720326323\n",
            "train loss:0.00012784933468536712\n",
            "train loss:8.689738839669498e-05\n",
            "train loss:0.00010602254954877921\n",
            "train loss:0.00012258012064680427\n",
            "train loss:9.61519956328582e-05\n",
            "train loss:0.00011240549972756627\n",
            "train loss:0.00012282908669260101\n",
            "train loss:0.00020020098295321024\n",
            "train loss:9.601149099529703e-05\n",
            "train loss:0.00018378339870760472\n",
            "train loss:0.00011640242223538034\n",
            "train loss:9.193564425913682e-05\n",
            "train loss:9.92634882676945e-05\n",
            "train loss:0.00017276709210907695\n",
            "train loss:0.0001089877108882749\n",
            "train loss:9.184104910517738e-05\n",
            "train loss:0.00026686258182804547\n",
            "train loss:0.0001342290828694506\n",
            "train loss:0.00011493152694669662\n",
            "train loss:0.0001557254557241326\n",
            "train loss:0.00021073574019682547\n",
            "train loss:0.00010391550429178685\n",
            "train loss:0.00021448397360340806\n",
            "train loss:0.0001570255574208773\n",
            "train loss:0.00027576439119749693\n",
            "train loss:0.00012164021688897124\n",
            "train loss:0.0002069226328128763\n",
            "train loss:8.4065200134836e-05\n",
            "train loss:0.00020999847675885806\n",
            "train loss:0.00013992254629477442\n",
            "train loss:0.00024774965330029197\n",
            "train loss:0.00013288360847851126\n",
            "train loss:0.0001498926730628523\n",
            "train loss:6.50141081209379e-05\n",
            "=== epoch:162, train acc:1.0, test acc:0.947 ===\n",
            "train loss:0.00010229951560227972\n",
            "train loss:0.00012983698042899825\n",
            "train loss:0.00017257667685078787\n",
            "train loss:0.00012439476159416805\n",
            "train loss:0.00020381029303592944\n",
            "train loss:0.0001383897542331501\n",
            "train loss:0.00013570706213341155\n",
            "train loss:0.00013356996109473575\n",
            "train loss:7.186817573612887e-05\n",
            "train loss:0.00013738461454204328\n",
            "train loss:0.00010839387395239232\n",
            "train loss:8.41476533961919e-05\n",
            "train loss:0.00013431008531928702\n",
            "train loss:0.00010114045702411493\n",
            "train loss:0.00014148249061450143\n",
            "train loss:0.0001666257295708682\n",
            "train loss:0.00011336046452447491\n",
            "train loss:9.238164551542044e-05\n",
            "train loss:0.00013241436570704247\n",
            "train loss:0.00017401016118893256\n",
            "train loss:0.0003160448726472069\n",
            "train loss:0.00010165348347841286\n",
            "train loss:0.00010244905618348889\n",
            "train loss:0.00013279049637560312\n",
            "train loss:0.00012288308483600905\n",
            "train loss:0.00012465100473094113\n",
            "train loss:0.00026571562283079223\n",
            "train loss:0.0001543046306125807\n",
            "train loss:0.00011642647484751266\n",
            "train loss:0.00024455403943288417\n",
            "train loss:0.00023812678840580404\n",
            "train loss:0.00012104706637998869\n",
            "train loss:0.00011136337948724828\n",
            "train loss:0.0004252675870932107\n",
            "train loss:0.0006573383827644211\n",
            "train loss:0.00012015319341629211\n",
            "train loss:0.00024587942301226384\n",
            "train loss:0.00021090039694730841\n",
            "train loss:0.000119109484688068\n",
            "train loss:0.0002507558570691721\n",
            "train loss:0.000219193357812746\n",
            "train loss:0.0006534242899151815\n",
            "train loss:0.0006242879911872213\n",
            "train loss:0.00022550213738717334\n",
            "train loss:0.00018781495530274334\n",
            "train loss:0.00025140002636817786\n",
            "train loss:0.00024992303115852117\n",
            "train loss:0.00026606029724973767\n",
            "train loss:0.00029100016990429265\n",
            "train loss:0.0012702484590510236\n",
            "train loss:0.00022795862676913976\n",
            "train loss:0.00023606469933862753\n",
            "train loss:0.00013919941936657544\n",
            "train loss:0.0003283439048424366\n",
            "train loss:0.00011700560853845405\n",
            "train loss:0.00019170100192259588\n",
            "train loss:0.0002411489504286286\n",
            "train loss:0.00016550994418203634\n",
            "train loss:0.00016420724485521977\n",
            "train loss:0.00023923433596404663\n",
            "train loss:0.0002082400499601641\n",
            "train loss:0.00026081853963486355\n",
            "train loss:0.00039118467248805704\n",
            "train loss:0.0001522658117606135\n",
            "train loss:0.00016445906058397853\n",
            "train loss:0.00013805687341654366\n",
            "train loss:0.00011593500680072193\n",
            "train loss:0.0001288630677811999\n",
            "train loss:0.00022321408093539116\n",
            "train loss:0.0001893220552202472\n",
            "train loss:0.00012054556606546476\n",
            "train loss:0.00013732391572732628\n",
            "train loss:0.00019678137462851895\n",
            "train loss:0.00014700304443353144\n",
            "train loss:0.00014136580239728714\n",
            "train loss:0.00018703375149538255\n",
            "train loss:0.0001572932331329776\n",
            "train loss:0.00014114496638452406\n",
            "train loss:0.00011122211437999132\n",
            "train loss:9.290055943350323e-05\n",
            "=== epoch:163, train acc:1.0, test acc:0.9435 ===\n",
            "train loss:8.7298521507677e-05\n",
            "train loss:0.00010119896483843273\n",
            "train loss:9.646202109974536e-05\n",
            "train loss:0.00012853095653083004\n",
            "train loss:0.00014355542273154174\n",
            "train loss:9.540208515604554e-05\n",
            "train loss:0.00016399985616225344\n",
            "train loss:0.00011206410865959715\n",
            "train loss:0.00010319258156097036\n",
            "train loss:0.0001416379066827282\n",
            "train loss:0.00015197133811606416\n",
            "train loss:0.00033561114386894636\n",
            "train loss:0.00012144831187310946\n",
            "train loss:0.00011092229690682318\n",
            "train loss:8.687229134321091e-05\n",
            "train loss:0.00010725400118204765\n",
            "train loss:0.0001601010954818477\n",
            "train loss:8.075743488068714e-05\n",
            "train loss:0.00012809078229008002\n",
            "train loss:0.0001593259031616499\n",
            "train loss:0.00020928758629347243\n",
            "train loss:0.00018185504193451174\n",
            "train loss:0.0003439004500474397\n",
            "train loss:0.00020053096814989427\n",
            "train loss:0.00010782068737118629\n",
            "train loss:0.0001589561741770488\n",
            "train loss:0.00011947441957915125\n",
            "train loss:0.00011915331961562268\n",
            "train loss:6.107965352607842e-05\n",
            "train loss:0.00015368880992839155\n",
            "train loss:9.64843485920233e-05\n",
            "train loss:0.00010250015037892754\n",
            "train loss:0.00036129261131609853\n",
            "train loss:0.000214165821267009\n",
            "train loss:9.319348304680266e-05\n",
            "train loss:7.513692305835869e-05\n",
            "train loss:0.00012884620327247872\n",
            "train loss:0.0001374177035493791\n",
            "train loss:0.00018727488251917904\n",
            "train loss:0.00015959407617651252\n",
            "train loss:0.0001159566308322904\n",
            "train loss:0.00014812250633433092\n",
            "train loss:0.00014721427818411396\n",
            "train loss:0.0002757622620186419\n",
            "train loss:0.00013592664066783153\n",
            "train loss:0.00013984010176796736\n",
            "train loss:0.0005771834963784269\n",
            "train loss:0.0010537161938551302\n",
            "train loss:0.00011099080209747494\n",
            "train loss:0.000172781496061007\n",
            "train loss:0.00016016649302854724\n",
            "train loss:0.00026700595174498654\n",
            "train loss:0.00013028695955378175\n",
            "train loss:0.00014085051890246918\n",
            "train loss:0.00022133175567641748\n",
            "train loss:0.00012750910666268537\n",
            "train loss:0.00010496426440535882\n",
            "train loss:0.00012719317317600668\n",
            "train loss:0.0001843435120451566\n",
            "train loss:8.856087654493782e-05\n",
            "train loss:0.00017430243567962797\n",
            "train loss:0.00014809820299033812\n",
            "train loss:0.00013601660206359452\n",
            "train loss:0.00011639455175266723\n",
            "train loss:0.00012808599025119255\n",
            "train loss:0.00014089753334632022\n",
            "train loss:0.0001253051939341292\n",
            "train loss:0.00015410348390917063\n",
            "train loss:9.059722185510269e-05\n",
            "train loss:0.00010651681158046636\n",
            "train loss:9.727573352893434e-05\n",
            "train loss:0.0001142578242910544\n",
            "train loss:0.00014168091961628494\n",
            "train loss:0.00013197890568325806\n",
            "train loss:0.0001377170753568801\n",
            "train loss:0.0001374001962465121\n",
            "train loss:0.00012343867855776504\n",
            "train loss:0.0003268951697166828\n",
            "train loss:9.602306934115188e-05\n",
            "train loss:0.00013034736790001097\n",
            "=== epoch:164, train acc:1.0, test acc:0.947 ===\n",
            "train loss:0.00010969487083372379\n",
            "train loss:0.00017799916863161822\n",
            "train loss:0.0001279375800959036\n",
            "train loss:0.00023582039762488463\n",
            "train loss:0.0001362364490944828\n",
            "train loss:0.00011539348911754044\n",
            "train loss:0.0001312262257608225\n",
            "train loss:0.00010532944437182682\n",
            "train loss:7.352530151162856e-05\n",
            "train loss:0.00011161686931334137\n",
            "train loss:0.00012798340127077661\n",
            "train loss:0.00022073413333608408\n",
            "train loss:8.327916010989549e-05\n",
            "train loss:8.196506144819772e-05\n",
            "train loss:0.00012426962231890917\n",
            "train loss:6.679446463694244e-05\n",
            "train loss:0.0001476807799285459\n",
            "train loss:0.0001710490743059671\n",
            "train loss:0.00014121065747951263\n",
            "train loss:0.00015860792848586692\n",
            "train loss:8.114333351020572e-05\n",
            "train loss:7.986632624508569e-05\n",
            "train loss:0.00019973723042426652\n",
            "train loss:7.894023699914797e-05\n",
            "train loss:7.617572382877035e-05\n",
            "train loss:0.0002736320145732913\n",
            "train loss:6.912280067499762e-05\n",
            "train loss:7.06139477734783e-05\n",
            "train loss:0.00010002782666908164\n",
            "train loss:0.00016550242216783407\n",
            "train loss:7.610583778056561e-05\n",
            "train loss:0.00017236615203339748\n",
            "train loss:8.519143581111195e-05\n",
            "train loss:7.974766924808419e-05\n",
            "train loss:0.00012516788258717617\n",
            "train loss:0.0001268752428968357\n",
            "train loss:7.619336055385409e-05\n",
            "train loss:6.491834807929514e-05\n",
            "train loss:0.0001002361442811784\n",
            "train loss:9.204639901296776e-05\n",
            "train loss:9.371668975899819e-05\n",
            "train loss:7.421777847209893e-05\n",
            "train loss:4.034126194611821e-05\n",
            "train loss:6.826257045153221e-05\n",
            "train loss:0.00010784450561647166\n",
            "train loss:7.001385241125522e-05\n",
            "train loss:8.135134961315482e-05\n",
            "train loss:0.00016672763851116774\n",
            "train loss:0.00010081277956836677\n",
            "train loss:0.0001696524779311372\n",
            "train loss:7.338114020209126e-05\n",
            "train loss:7.369596809567432e-05\n",
            "train loss:0.00013187988622621842\n",
            "train loss:0.0001401272764245622\n",
            "train loss:8.24444632247364e-05\n",
            "train loss:5.715305598679293e-05\n",
            "train loss:0.00010217036957268464\n",
            "train loss:0.00012508384413349434\n",
            "train loss:7.721049791052925e-05\n",
            "train loss:7.686290281606804e-05\n",
            "train loss:0.00010289042325864074\n",
            "train loss:0.00016052109402394263\n",
            "train loss:0.0001102526060375138\n",
            "train loss:0.00013343804508586437\n",
            "train loss:0.00012921861135730387\n",
            "train loss:7.12398533810707e-05\n",
            "train loss:8.44172819174116e-05\n",
            "train loss:6.910901257324059e-05\n",
            "train loss:6.894084941391925e-05\n",
            "train loss:6.356462453890505e-05\n",
            "train loss:0.00011731385901887877\n",
            "train loss:7.274269736504896e-05\n",
            "train loss:0.00012300176868369907\n",
            "train loss:6.631379346418852e-05\n",
            "train loss:0.0001373875762223444\n",
            "train loss:0.00023816888870890847\n",
            "train loss:5.7398399335278503e-05\n",
            "train loss:0.00010804958688828864\n",
            "train loss:7.677235110164223e-05\n",
            "train loss:0.00012494919612994325\n",
            "=== epoch:165, train acc:1.0, test acc:0.943 ===\n",
            "train loss:6.98450172731175e-05\n",
            "train loss:8.824051807043561e-05\n",
            "train loss:0.00019030552513276585\n",
            "train loss:9.005151973383028e-05\n",
            "train loss:8.589971340183874e-05\n",
            "train loss:9.061687508400094e-05\n",
            "train loss:0.0001547106800780987\n",
            "train loss:6.784352264847618e-05\n",
            "train loss:9.008412891506086e-05\n",
            "train loss:0.000175851174782063\n",
            "train loss:9.531149910222726e-05\n",
            "train loss:0.00047455168884566056\n",
            "train loss:0.00011959246766062656\n",
            "train loss:7.363068322967018e-05\n",
            "train loss:7.527105370329167e-05\n",
            "train loss:0.000178903168629371\n",
            "train loss:0.00010216040602484693\n",
            "train loss:8.912355500125648e-05\n",
            "train loss:0.00011177045424140193\n",
            "train loss:0.00020239434345153574\n",
            "train loss:6.710554285874567e-05\n",
            "train loss:9.226611563187433e-05\n",
            "train loss:0.000838813654858119\n",
            "train loss:8.71457154186192e-05\n",
            "train loss:0.00021952664700106692\n",
            "train loss:8.884042107540966e-05\n",
            "train loss:7.941175597333083e-05\n",
            "train loss:0.00010432729938590964\n",
            "train loss:7.664068551479769e-05\n",
            "train loss:0.00010514958543730056\n",
            "train loss:9.195385644209193e-05\n",
            "train loss:0.00018898158264061372\n",
            "train loss:0.00018567261172235683\n",
            "train loss:6.398360053631359e-05\n",
            "train loss:0.00016637855518956832\n",
            "train loss:0.000210692991751152\n",
            "train loss:0.00012475453411379934\n",
            "train loss:0.00022792414967566431\n",
            "train loss:0.00011513246317461732\n",
            "train loss:0.00011974819746187483\n",
            "train loss:8.757754593954664e-05\n",
            "train loss:7.779338090001916e-05\n",
            "train loss:0.0001709766674926864\n",
            "train loss:0.00014845170477682074\n",
            "train loss:0.00016132848968041283\n",
            "train loss:0.0002457139278530218\n",
            "train loss:0.0001391198103350437\n",
            "train loss:0.00011467193560776696\n",
            "train loss:9.641035879286881e-05\n",
            "train loss:9.989330442650585e-05\n",
            "train loss:0.00010023294625269805\n",
            "train loss:7.51977211216137e-05\n",
            "train loss:0.0003213384784478944\n",
            "train loss:6.073117174314977e-05\n",
            "train loss:0.00010152657199363418\n",
            "train loss:0.00010244549701827671\n",
            "train loss:0.00020274343587785464\n",
            "train loss:0.00014472885197491008\n",
            "train loss:0.00016315775457922813\n",
            "train loss:0.0008501511218418384\n",
            "train loss:0.00019645920098264823\n",
            "train loss:0.0001217867932832125\n",
            "train loss:0.00010424546328629022\n",
            "train loss:0.00011438657105974566\n",
            "train loss:8.968093606103407e-05\n",
            "train loss:0.0001681844179424076\n",
            "train loss:0.00019709619783536865\n",
            "train loss:8.021550783431303e-05\n",
            "train loss:0.00012567716309953776\n",
            "train loss:0.00021324588414866938\n",
            "train loss:6.97261992753115e-05\n",
            "train loss:0.0001314898108979372\n",
            "train loss:0.00010288614301539382\n",
            "train loss:0.00018432420584223955\n",
            "train loss:6.925449341623449e-05\n",
            "train loss:0.00010722028743071133\n",
            "train loss:7.591368304697307e-05\n",
            "train loss:0.0001247601434947268\n",
            "train loss:0.00011455454979644432\n",
            "train loss:8.798507589362734e-05\n",
            "=== epoch:166, train acc:1.0, test acc:0.9435 ===\n",
            "train loss:0.00013749918293659463\n",
            "train loss:6.992679841340702e-05\n",
            "train loss:7.260700388306835e-05\n",
            "train loss:9.78438960943245e-05\n",
            "train loss:0.0001493246764880477\n",
            "train loss:0.00017600366047190356\n",
            "train loss:0.002102503390089583\n",
            "train loss:9.441679579815066e-05\n",
            "train loss:9.764958298631889e-05\n",
            "train loss:9.351021360295645e-05\n",
            "train loss:0.00011327944538728568\n",
            "train loss:7.63953505278027e-05\n",
            "train loss:8.598747933533457e-05\n",
            "train loss:0.00017166694828878843\n",
            "train loss:5.5725388785503126e-05\n",
            "train loss:9.452409492897938e-05\n",
            "train loss:6.152854166707938e-05\n",
            "train loss:7.033411724929287e-05\n",
            "train loss:0.0001529163160669491\n",
            "train loss:0.00014971808928769846\n",
            "train loss:9.861837798617895e-05\n",
            "train loss:7.645517767628898e-05\n",
            "train loss:8.21784208882078e-05\n",
            "train loss:0.000162692410628211\n",
            "train loss:0.00027377798532960166\n",
            "train loss:9.994628214979682e-05\n",
            "train loss:0.00010180048019342659\n",
            "train loss:0.00018241955862327369\n",
            "train loss:0.00011938723759550925\n",
            "train loss:0.0001364476497760574\n",
            "train loss:9.558186458939222e-05\n",
            "train loss:0.00013226306496247337\n",
            "train loss:0.000260020601443885\n",
            "train loss:0.00030810226387633846\n",
            "train loss:0.0001243482941350564\n",
            "train loss:0.0002051043887199782\n",
            "train loss:0.00015615052281893094\n",
            "train loss:0.00017977750355078454\n",
            "train loss:0.00017893858147872827\n",
            "train loss:0.0001091531623976353\n",
            "train loss:0.00010301865931300002\n",
            "train loss:0.0001303816283242501\n",
            "train loss:0.00013580260385724936\n",
            "train loss:0.00012228003273252356\n",
            "train loss:0.00011233866511850822\n",
            "train loss:0.00018498711937203657\n",
            "train loss:0.0001876631452228209\n",
            "train loss:7.691176652211456e-05\n",
            "train loss:6.079695267727185e-05\n",
            "train loss:0.00010303372739825246\n",
            "train loss:8.903950101825257e-05\n",
            "train loss:7.127036648831748e-05\n",
            "train loss:7.964945234446791e-05\n",
            "train loss:0.00010362320460717358\n",
            "train loss:7.299458872912753e-05\n",
            "train loss:0.0001270360111809358\n",
            "train loss:7.308420886504121e-05\n",
            "train loss:8.74201478873206e-05\n",
            "train loss:5.691467215758382e-05\n",
            "train loss:8.488003523205039e-05\n",
            "train loss:8.027108697728086e-05\n",
            "train loss:8.39571569986051e-05\n",
            "train loss:0.00010154527788968257\n",
            "train loss:6.825081776161841e-05\n",
            "train loss:0.00012330304197461373\n",
            "train loss:0.0001181171915005405\n",
            "train loss:0.00019432311989838472\n",
            "train loss:8.88593274881853e-05\n",
            "train loss:0.00019128976423875768\n",
            "train loss:0.00015484111804684646\n",
            "train loss:0.00026185635181507765\n",
            "train loss:0.0014051556630227254\n",
            "train loss:0.00018958230332272944\n",
            "train loss:0.00017485248356597643\n",
            "train loss:0.00010711250851656185\n",
            "train loss:0.006974130760722892\n",
            "train loss:0.00011714900030281264\n",
            "train loss:0.00011397269088663906\n",
            "train loss:0.0001567475471189208\n",
            "train loss:0.00010846587436670428\n",
            "=== epoch:167, train acc:0.999875, test acc:0.9415 ===\n",
            "train loss:0.00012870839716907581\n",
            "train loss:0.0004134267330984785\n",
            "train loss:8.792102421723815e-05\n",
            "train loss:0.004100183757856589\n",
            "train loss:0.0012387609721990036\n",
            "train loss:0.0003585195537071657\n",
            "train loss:0.0001760577675090734\n",
            "train loss:8.744845304086029e-05\n",
            "train loss:9.524287738036153e-05\n",
            "train loss:0.00013310313275757078\n",
            "train loss:0.00013174460168119585\n",
            "train loss:0.0001366211519633534\n",
            "train loss:0.0001232445300822011\n",
            "train loss:0.00020139796717641195\n",
            "train loss:0.0004325908134650858\n",
            "train loss:0.00013668284347782752\n",
            "train loss:0.0001306236609244885\n",
            "train loss:0.00015028644502465338\n",
            "train loss:0.00010800335065095756\n",
            "train loss:0.00014992821033648065\n",
            "train loss:0.00017026345950311831\n",
            "train loss:0.0002686043701945844\n",
            "train loss:0.00012295281571828433\n",
            "train loss:0.00014419782494094616\n",
            "train loss:0.00022646817098674846\n",
            "train loss:0.0001817357354091996\n",
            "train loss:0.00014341482431674466\n",
            "train loss:0.00024473249092738723\n",
            "train loss:0.00013422937564019758\n",
            "train loss:0.00017443920834932635\n",
            "train loss:0.00012003748867691304\n",
            "train loss:0.00019514033068513163\n",
            "train loss:0.00016014128360920818\n",
            "train loss:0.0001584763396938348\n",
            "train loss:8.380973089868054e-05\n",
            "train loss:0.00011226634941159723\n",
            "train loss:0.0002015099801009706\n",
            "train loss:0.0001340899259931266\n",
            "train loss:0.00013332608333424252\n",
            "train loss:0.00015867718208846393\n",
            "train loss:0.0024824196587892563\n",
            "train loss:0.00016349680552778714\n",
            "train loss:0.0003253427543410531\n",
            "train loss:0.00015999284548148798\n",
            "train loss:0.00013065291091040115\n",
            "train loss:0.00012758846598547448\n",
            "train loss:0.00015506894574404253\n",
            "train loss:0.00018490988323768612\n",
            "train loss:0.0001581270753744719\n",
            "train loss:0.00012539915986274484\n",
            "train loss:0.0002271448672507239\n",
            "train loss:0.00018739175384424527\n",
            "train loss:8.98160183695167e-05\n",
            "train loss:0.00011458072114436703\n",
            "train loss:0.00010264664967908444\n",
            "train loss:0.0001224818469715237\n",
            "train loss:0.001064243122333535\n",
            "train loss:0.0001189280864788908\n",
            "train loss:0.00017292216984754122\n",
            "train loss:0.00012989532938312732\n",
            "train loss:0.00010652232118975586\n",
            "train loss:0.00016286074255914457\n",
            "train loss:0.00014135700320131613\n",
            "train loss:8.29397011969873e-05\n",
            "train loss:7.834318962241615e-05\n",
            "train loss:7.478041515789755e-05\n",
            "train loss:0.00011619899838369528\n",
            "train loss:7.04420212637627e-05\n",
            "train loss:0.00012526358804362228\n",
            "train loss:9.744192272429299e-05\n",
            "train loss:0.00012091164332483092\n",
            "train loss:8.684034292337812e-05\n",
            "train loss:0.00017260369163926055\n",
            "train loss:0.00011192417654653693\n",
            "train loss:0.00013166419798049602\n",
            "train loss:0.000133105492322262\n",
            "train loss:0.00016561962863921622\n",
            "train loss:0.00016852022993774527\n",
            "train loss:0.00018823388600217148\n",
            "train loss:0.0001618137478113047\n",
            "=== epoch:168, train acc:0.999875, test acc:0.9455 ===\n",
            "train loss:0.00016773473207942223\n",
            "train loss:0.0003097845596437033\n",
            "train loss:0.00012647637202053635\n",
            "train loss:0.0001287443127529312\n",
            "train loss:0.00011843283106469081\n",
            "train loss:0.0001493501785880088\n",
            "train loss:0.00013897493487816912\n",
            "train loss:9.243178767554847e-05\n",
            "train loss:0.00017534676847610466\n",
            "train loss:0.00028116172836979643\n",
            "train loss:0.00014035512446827888\n",
            "train loss:0.0001870048508443869\n",
            "train loss:0.00020868736683898152\n",
            "train loss:0.00031758579810442527\n",
            "train loss:0.00016488301192039707\n",
            "train loss:0.00018757633934575715\n",
            "train loss:0.000262338063608149\n",
            "train loss:0.00022765503771087024\n",
            "train loss:0.0007086772463605822\n",
            "train loss:0.00018378352259052443\n",
            "train loss:0.00019804207755205888\n",
            "train loss:0.0002842160179974535\n",
            "train loss:0.00025611115779822355\n",
            "train loss:0.00017107357898850188\n",
            "train loss:0.00019473529762695623\n",
            "train loss:0.00023360570284626945\n",
            "train loss:0.00012984276964041162\n",
            "train loss:0.00015174171504197357\n",
            "train loss:0.00019309567265767712\n",
            "train loss:0.00011780577230042126\n",
            "train loss:0.0001062663059007314\n",
            "train loss:0.00010622743093724587\n",
            "train loss:0.0001704133458823624\n",
            "train loss:8.57344795839016e-05\n",
            "train loss:0.00013765943864464582\n",
            "train loss:0.00015796453980163335\n",
            "train loss:0.00016115762956082886\n",
            "train loss:0.000208104373944297\n",
            "train loss:0.00011713345995981402\n",
            "train loss:0.0001372780180525041\n",
            "train loss:9.8412518506994e-05\n",
            "train loss:0.00012049771997129509\n",
            "train loss:9.727722078202883e-05\n",
            "train loss:0.00022007295024022047\n",
            "train loss:0.00013722026391445942\n",
            "train loss:0.0002271590710398199\n",
            "train loss:0.00011753376980916052\n",
            "train loss:9.77544053482785e-05\n",
            "train loss:0.00015437661174172652\n",
            "train loss:0.00010639916391591464\n",
            "train loss:0.00017483062119261938\n",
            "train loss:0.00013772328346927176\n",
            "train loss:0.00014044850490828401\n",
            "train loss:0.0002278199172897895\n",
            "train loss:0.0001612173231956444\n",
            "train loss:0.0001320935875294736\n",
            "train loss:0.0001317512512166778\n",
            "train loss:0.00011827219024871985\n",
            "train loss:0.0003008401281642535\n",
            "train loss:0.00015983128403407112\n",
            "train loss:0.00029489009776174056\n",
            "train loss:0.0002098834271058488\n",
            "train loss:0.00013868959078117233\n",
            "train loss:0.00014094856452923435\n",
            "train loss:0.00014186156988847578\n",
            "train loss:0.0001713290568725157\n",
            "train loss:0.00014615708508153262\n",
            "train loss:0.00016801325555997838\n",
            "train loss:0.00018867317726128818\n",
            "train loss:9.194608383101275e-05\n",
            "train loss:0.00041315053041981304\n",
            "train loss:0.00011025815686437868\n",
            "train loss:0.00019958928937842773\n",
            "train loss:0.0001917505772976082\n",
            "train loss:0.0001018000109767902\n",
            "train loss:0.00012827317951713806\n",
            "train loss:0.0001325098272995335\n",
            "train loss:0.00012561720443809605\n",
            "train loss:0.000156983301981758\n",
            "train loss:0.00014165583273140388\n",
            "=== epoch:169, train acc:1.0, test acc:0.945 ===\n",
            "train loss:0.00011054417814732228\n",
            "train loss:0.00032974454334990265\n",
            "train loss:0.0001523079393359569\n",
            "train loss:0.00022906068088953532\n",
            "train loss:0.0003225529974912196\n",
            "train loss:0.00010754326362018664\n",
            "train loss:0.0001620882634520319\n",
            "train loss:0.00013515321108536937\n",
            "train loss:9.500334189390618e-05\n",
            "train loss:0.0003734646813779368\n",
            "train loss:0.00012138788466764274\n",
            "train loss:0.0001767328493487561\n",
            "train loss:0.00019197371190837593\n",
            "train loss:0.00027243229542649\n",
            "train loss:0.00013043170647924484\n",
            "train loss:0.00021272523192037752\n",
            "train loss:0.00037469047333711284\n",
            "train loss:0.00015842080150637236\n",
            "train loss:0.00016196539415043028\n",
            "train loss:0.0001224723215703524\n",
            "train loss:0.0001469033864758747\n",
            "train loss:0.00011257409160393721\n",
            "train loss:0.00018783849155003647\n",
            "train loss:0.00015673269354134147\n",
            "train loss:0.00012292704470991196\n",
            "train loss:0.00011756166615032783\n",
            "train loss:0.00019663554264047012\n",
            "train loss:0.00022379720885491082\n",
            "train loss:0.00030084249751235597\n",
            "train loss:9.777969975171035e-05\n",
            "train loss:0.00013184384456854713\n",
            "train loss:0.00018206466444593546\n",
            "train loss:0.0002700291744940473\n",
            "train loss:0.00011506323629465694\n",
            "train loss:0.00010301292718516765\n",
            "train loss:0.00015715463077220674\n",
            "train loss:0.00014285123207071856\n",
            "train loss:0.0001382466513747849\n",
            "train loss:0.00011736708500092163\n",
            "train loss:0.0002477016844785374\n",
            "train loss:0.0001699134053949215\n",
            "train loss:0.00012254208876116393\n",
            "train loss:0.00017927146894867457\n",
            "train loss:0.00018059995536612539\n",
            "train loss:0.00026685784623103355\n",
            "train loss:0.00026869237148110565\n",
            "train loss:0.0001307126784162699\n",
            "train loss:0.00015242431812599879\n",
            "train loss:0.00017406565460593518\n",
            "train loss:0.00016123989298662738\n",
            "train loss:0.00012260781868609366\n",
            "train loss:0.00013400397205697075\n",
            "train loss:0.00010391406685606392\n",
            "train loss:0.00026007352262473325\n",
            "train loss:0.0001858422237830136\n",
            "train loss:0.00021852201409980576\n",
            "train loss:0.0012828273925730798\n",
            "train loss:0.0003530479252907865\n",
            "train loss:0.00015980735721651967\n",
            "train loss:0.00015669566793549981\n",
            "train loss:0.0001507421014904545\n",
            "train loss:0.00011374932104720377\n",
            "train loss:0.00022044929284114646\n",
            "train loss:0.00011762476115889868\n",
            "train loss:0.00014315827532328032\n",
            "train loss:0.00018893921949964226\n",
            "train loss:0.0001556584797291944\n",
            "train loss:0.0009511614877734946\n",
            "train loss:0.0003044006441073014\n",
            "train loss:0.00015113726174775897\n",
            "train loss:0.00015121235141594914\n",
            "train loss:0.00018536057090292633\n",
            "train loss:0.00012913688865312756\n",
            "train loss:8.515362270336559e-05\n",
            "train loss:0.00014442811522728664\n",
            "train loss:0.00011047443210314255\n",
            "train loss:0.0001666856384218019\n",
            "train loss:8.428201586641676e-05\n",
            "train loss:0.00013016070159587543\n",
            "train loss:9.514367508278992e-05\n",
            "=== epoch:170, train acc:1.0, test acc:0.9455 ===\n",
            "train loss:7.578433121796112e-05\n",
            "train loss:0.00012684518980960306\n",
            "train loss:0.00015254359963592774\n",
            "train loss:0.00029192917898113966\n",
            "train loss:9.181500300698239e-05\n",
            "train loss:8.55343029649601e-05\n",
            "train loss:6.238396860848967e-05\n",
            "train loss:7.92108758359742e-05\n",
            "train loss:6.550347557680131e-05\n",
            "train loss:0.00016189339055214914\n",
            "train loss:0.00011694283520658875\n",
            "train loss:0.00015833351623769528\n",
            "train loss:0.0001524867971125033\n",
            "train loss:9.002083408310255e-05\n",
            "train loss:0.00011078587724364431\n",
            "train loss:0.00011515195326763801\n",
            "train loss:0.0007716809716917327\n",
            "train loss:0.00011910799063010195\n",
            "train loss:0.00010829246251483016\n",
            "train loss:9.028577155786142e-05\n",
            "train loss:9.342177356417245e-05\n",
            "train loss:0.00013108651448408298\n",
            "train loss:9.499263795241861e-05\n",
            "train loss:0.00014558731927365748\n",
            "train loss:9.968934013234017e-05\n",
            "train loss:0.00015769083951556437\n",
            "train loss:7.688705356456481e-05\n",
            "train loss:0.00011557378617487418\n",
            "train loss:0.00019978186434351327\n",
            "train loss:8.043924903211765e-05\n",
            "train loss:8.928328891356579e-05\n",
            "train loss:9.792589796487274e-05\n",
            "train loss:0.000194257076443224\n",
            "train loss:0.00014025315468211235\n",
            "train loss:0.0001285101925852213\n",
            "train loss:0.0001264197812482605\n",
            "train loss:7.239332095522734e-05\n",
            "train loss:9.831608202272322e-05\n",
            "train loss:0.00011000047722866578\n",
            "train loss:0.00026317373880935664\n",
            "train loss:0.00013412379660294408\n",
            "train loss:0.00014243016132170887\n",
            "train loss:0.00016584215332057304\n",
            "train loss:0.0001689258793699502\n",
            "train loss:9.863137338702831e-05\n",
            "train loss:0.00014041749789036413\n",
            "train loss:0.0002859508136990087\n",
            "train loss:0.0002320580005370061\n",
            "train loss:0.00013711558025611702\n",
            "train loss:0.00010419442226986106\n",
            "train loss:0.00013607428374646404\n",
            "train loss:7.220284515816533e-05\n",
            "train loss:0.0005160463815041913\n",
            "train loss:7.625967931869226e-05\n",
            "train loss:9.267001662511057e-05\n",
            "train loss:9.335310870056951e-05\n",
            "train loss:8.4325997499234e-05\n",
            "train loss:0.00010205225701848381\n",
            "train loss:7.209344816896094e-05\n",
            "train loss:0.0001128801546082749\n",
            "train loss:0.00011000544120892942\n",
            "train loss:8.625804305562783e-05\n",
            "train loss:9.229382176057034e-05\n",
            "train loss:0.00011035275839392412\n",
            "train loss:0.00014098827424901508\n",
            "train loss:0.0001475290765557291\n",
            "train loss:9.708959499019006e-05\n",
            "train loss:0.00014377529408741453\n",
            "train loss:6.379716131976531e-05\n",
            "train loss:9.667078709554989e-05\n",
            "train loss:9.837284582565646e-05\n",
            "train loss:9.283520241675376e-05\n",
            "train loss:9.763404791018349e-05\n",
            "train loss:0.00013549156959813546\n",
            "train loss:0.0001958325615163761\n",
            "train loss:0.00023935397622607706\n",
            "train loss:8.956705979484921e-05\n",
            "train loss:0.00011041705911191378\n",
            "train loss:0.00014044945251471617\n",
            "train loss:0.00014538861934948181\n",
            "=== epoch:171, train acc:1.0, test acc:0.9475 ===\n",
            "train loss:0.00018439134195115788\n",
            "train loss:0.00010500060798047603\n",
            "train loss:8.0608146254564e-05\n",
            "train loss:0.00011788622097194876\n",
            "train loss:9.889855105049845e-05\n",
            "train loss:9.251383641551656e-05\n",
            "train loss:0.00012504568628938871\n",
            "train loss:7.463074709530917e-05\n",
            "train loss:0.00013828522802773985\n",
            "train loss:0.0001267554783311374\n",
            "train loss:0.00011579796636882654\n",
            "train loss:0.00014344324692504133\n",
            "train loss:7.303339643560658e-05\n",
            "train loss:8.730877206960263e-05\n",
            "train loss:6.837924358502938e-05\n",
            "train loss:6.661954764028782e-05\n",
            "train loss:6.0227075693759384e-05\n",
            "train loss:8.215217295269167e-05\n",
            "train loss:8.923248729663329e-05\n",
            "train loss:0.00015214903585229842\n",
            "train loss:8.030747869078883e-05\n",
            "train loss:7.921515565906839e-05\n",
            "train loss:7.231842424426847e-05\n",
            "train loss:0.00018526074819135024\n",
            "train loss:8.134439992015583e-05\n",
            "train loss:8.23076683042874e-05\n",
            "train loss:6.114295666588488e-05\n",
            "train loss:7.941569499466949e-05\n",
            "train loss:6.432104566227646e-05\n",
            "train loss:5.770856611438996e-05\n",
            "train loss:0.00011369886171526214\n",
            "train loss:0.00010610724966536875\n",
            "train loss:9.18972495258502e-05\n",
            "train loss:7.174834709508929e-05\n",
            "train loss:0.00010111413796740263\n",
            "train loss:0.00012166049159583374\n",
            "train loss:9.73185901431873e-05\n",
            "train loss:0.0001461139426280001\n",
            "train loss:8.147361330849878e-05\n",
            "train loss:0.00021554047695225784\n",
            "train loss:9.729061187659534e-05\n",
            "train loss:5.674802637991519e-05\n",
            "train loss:0.00010022260833061409\n",
            "train loss:8.941438521740346e-05\n",
            "train loss:7.854613734554926e-05\n",
            "train loss:8.960646323979678e-05\n",
            "train loss:0.0003852551022849834\n",
            "train loss:0.00011414802563360871\n",
            "train loss:0.00012042810903382702\n",
            "train loss:0.00015334905630825325\n",
            "train loss:7.935429909742375e-05\n",
            "train loss:7.961077338386185e-05\n",
            "train loss:7.756023698898056e-05\n",
            "train loss:9.266450889425277e-05\n",
            "train loss:7.091636143232378e-05\n",
            "train loss:5.773087832888792e-05\n",
            "train loss:8.129312906222229e-05\n",
            "train loss:8.547203730419551e-05\n",
            "train loss:0.00010124483818727387\n",
            "train loss:8.027548064223608e-05\n",
            "train loss:0.0002513964106928519\n",
            "train loss:0.00014091497891526668\n",
            "train loss:6.521684655603723e-05\n",
            "train loss:0.00010952662432745663\n",
            "train loss:8.584431720863555e-05\n",
            "train loss:9.719061622629209e-05\n",
            "train loss:9.998385588472145e-05\n",
            "train loss:8.083753591140313e-05\n",
            "train loss:8.278603877452765e-05\n",
            "train loss:5.803677464408869e-05\n",
            "train loss:8.01737756604247e-05\n",
            "train loss:9.266706981683963e-05\n",
            "train loss:6.945982150613376e-05\n",
            "train loss:7.822257990752551e-05\n",
            "train loss:7.140975043250724e-05\n",
            "train loss:0.00011179401574542046\n",
            "train loss:7.620642472825161e-05\n",
            "train loss:9.88150151958099e-05\n",
            "train loss:9.964523784892305e-05\n",
            "train loss:0.00010166631344500344\n",
            "=== epoch:172, train acc:1.0, test acc:0.9455 ===\n",
            "train loss:6.864840910705186e-05\n",
            "train loss:7.903674152743846e-05\n",
            "train loss:0.00013459185045691722\n",
            "train loss:0.00014893738405687533\n",
            "train loss:6.485917553591699e-05\n",
            "train loss:0.0008625512385053416\n",
            "train loss:9.976049434986018e-05\n",
            "train loss:8.557053879023337e-05\n",
            "train loss:0.00015686095380429486\n",
            "train loss:7.511107163863767e-05\n",
            "train loss:0.0001196579850559426\n",
            "train loss:9.007927161946183e-05\n",
            "train loss:7.400027161812503e-05\n",
            "train loss:8.663654406195506e-05\n",
            "train loss:0.00015294995322626457\n",
            "train loss:0.00011165989397005722\n",
            "train loss:5.9731442018810505e-05\n",
            "train loss:7.269570893268454e-05\n",
            "train loss:7.963057097276861e-05\n",
            "train loss:0.0001564536466100565\n",
            "train loss:5.691528754588451e-05\n",
            "train loss:0.00015786063045820947\n",
            "train loss:0.00011617042887391562\n",
            "train loss:0.00018125803306764322\n",
            "train loss:9.87810643923351e-05\n",
            "train loss:0.00010697196232825864\n",
            "train loss:8.504587701342646e-05\n",
            "train loss:9.36752201187054e-05\n",
            "train loss:0.00010684098455049934\n",
            "train loss:8.493113315312637e-05\n",
            "train loss:0.00012010138668322765\n",
            "train loss:0.00011235290605091002\n",
            "train loss:9.458737884271255e-05\n",
            "train loss:9.094273869912528e-05\n",
            "train loss:9.901024094562381e-05\n",
            "train loss:7.006731501220289e-05\n",
            "train loss:8.465040122798317e-05\n",
            "train loss:6.566320776693006e-05\n",
            "train loss:0.0001020464851232189\n",
            "train loss:0.0001123494468513129\n",
            "train loss:0.00011443316285527287\n",
            "train loss:3.894501839181724e-05\n",
            "train loss:0.00011396562548090093\n",
            "train loss:7.955973085591725e-05\n",
            "train loss:4.733176354297065e-05\n",
            "train loss:6.209220447508121e-05\n",
            "train loss:8.763059004097131e-05\n",
            "train loss:7.476287590002485e-05\n",
            "train loss:5.582935713127614e-05\n",
            "train loss:8.138869843307197e-05\n",
            "train loss:0.0001010259190390512\n",
            "train loss:6.743015284262606e-05\n",
            "train loss:5.566478942448369e-05\n",
            "train loss:7.846963463001116e-05\n",
            "train loss:5.3453548424382755e-05\n",
            "train loss:8.68320468201273e-05\n",
            "train loss:6.785807714663177e-05\n",
            "train loss:8.927043769833399e-05\n",
            "train loss:0.00011411382308237538\n",
            "train loss:9.38474511770287e-05\n",
            "train loss:0.00014641664453893233\n",
            "train loss:6.543511140529363e-05\n",
            "train loss:8.076723839023824e-05\n",
            "train loss:0.00010844611146107996\n",
            "train loss:0.00010316688512586765\n",
            "train loss:6.0554562979968e-05\n",
            "train loss:7.586627051983538e-05\n",
            "train loss:6.915052159664863e-05\n",
            "train loss:6.479488657027733e-05\n",
            "train loss:6.473803479378278e-05\n",
            "train loss:0.00010129946650101888\n",
            "train loss:6.682577451479206e-05\n",
            "train loss:9.405654189850957e-05\n",
            "train loss:0.00012895162679089672\n",
            "train loss:8.812768394465305e-05\n",
            "train loss:6.469871151906149e-05\n",
            "train loss:7.412501259598167e-05\n",
            "train loss:8.413804875734381e-05\n",
            "train loss:6.592996537074765e-05\n",
            "train loss:8.671320357858626e-05\n",
            "=== epoch:173, train acc:1.0, test acc:0.942 ===\n",
            "train loss:8.010144683466551e-05\n",
            "train loss:6.49942002047646e-05\n",
            "train loss:6.658097098561658e-05\n",
            "train loss:7.453607680438363e-05\n",
            "train loss:0.00012476594807859484\n",
            "train loss:6.314108611760522e-05\n",
            "train loss:0.00017748578353111538\n",
            "train loss:7.763712251661824e-05\n",
            "train loss:7.439999739463653e-05\n",
            "train loss:5.0305164191418884e-05\n",
            "train loss:0.00010406248204819038\n",
            "train loss:0.00010711202902368293\n",
            "train loss:7.178945013391323e-05\n",
            "train loss:0.00010670528574219037\n",
            "train loss:0.00020107893853356997\n",
            "train loss:6.91464756612935e-05\n",
            "train loss:5.727794944288822e-05\n",
            "train loss:0.00031410637889606955\n",
            "train loss:0.00022554668044177497\n",
            "train loss:0.00010061968863744724\n",
            "train loss:9.36394773069808e-05\n",
            "train loss:8.397932902859196e-05\n",
            "train loss:0.00022559029431030075\n",
            "train loss:7.661018615974921e-05\n",
            "train loss:9.093398691355925e-05\n",
            "train loss:0.00011995520716972331\n",
            "train loss:0.00013359307941560104\n",
            "train loss:9.780371560452246e-05\n",
            "train loss:0.00011532654317194724\n",
            "train loss:7.920289836878775e-05\n",
            "train loss:0.0001378004674834964\n",
            "train loss:0.00010760046135355581\n",
            "train loss:0.00012890613625556483\n",
            "train loss:7.66753703560876e-05\n",
            "train loss:0.00017005559959405524\n",
            "train loss:8.327806779123593e-05\n",
            "train loss:0.00011226055577818263\n",
            "train loss:0.00010138475990498791\n",
            "train loss:0.0001224784612071643\n",
            "train loss:0.00013851269127908525\n",
            "train loss:8.200146562643647e-05\n",
            "train loss:0.0001533422678473422\n",
            "train loss:0.00012040068747438325\n",
            "train loss:0.00020137337201654685\n",
            "train loss:8.093097333294484e-05\n",
            "train loss:9.400745609621045e-05\n",
            "train loss:0.00014867395161497306\n",
            "train loss:0.00011816897201142954\n",
            "train loss:9.198040536550597e-05\n",
            "train loss:0.00015650086038388077\n",
            "train loss:0.0001036435180181651\n",
            "train loss:0.0001717493348045103\n",
            "train loss:9.428081302076275e-05\n",
            "train loss:0.00010178520756347455\n",
            "train loss:0.00013870566489261921\n",
            "train loss:0.00010973984905854427\n",
            "train loss:0.00026135501826649114\n",
            "train loss:0.00014307654420687337\n",
            "train loss:0.0001229430847831698\n",
            "train loss:0.00011552389342925622\n",
            "train loss:8.887140170219177e-05\n",
            "train loss:0.00011296823367018997\n",
            "train loss:0.00010564601874352133\n",
            "train loss:0.00013487954025412596\n",
            "train loss:0.00030034051513100736\n",
            "train loss:0.0001654024150341448\n",
            "train loss:0.00019552988723574316\n",
            "train loss:8.316632747589826e-05\n",
            "train loss:0.00014098636054942819\n",
            "train loss:0.00010498056365417431\n",
            "train loss:8.133435092837579e-05\n",
            "train loss:0.00023768127247009613\n",
            "train loss:9.853786267143533e-05\n",
            "train loss:0.00010401460498758432\n",
            "train loss:8.494002112108677e-05\n",
            "train loss:6.310798810361808e-05\n",
            "train loss:6.694539679372033e-05\n",
            "train loss:0.00010982234271693706\n",
            "train loss:9.436011248449444e-05\n",
            "train loss:6.18240959927344e-05\n",
            "=== epoch:174, train acc:1.0, test acc:0.943 ===\n",
            "train loss:7.304470672834279e-05\n",
            "train loss:7.804158584849102e-05\n",
            "train loss:0.00012906053127879482\n",
            "train loss:9.749497337418453e-05\n",
            "train loss:0.0002453187897463812\n",
            "train loss:0.00026630947409564875\n",
            "train loss:9.794292127890069e-05\n",
            "train loss:0.00024055764044261013\n",
            "train loss:8.723068035167294e-05\n",
            "train loss:0.00017679479718221044\n",
            "train loss:0.00011743291461096117\n",
            "train loss:8.30954194835703e-05\n",
            "train loss:9.392212270245411e-05\n",
            "train loss:7.775231627550704e-05\n",
            "train loss:8.356799404451504e-05\n",
            "train loss:5.670667986733135e-05\n",
            "train loss:6.982792897521696e-05\n",
            "train loss:0.0001469449278062191\n",
            "train loss:0.00015209512022052653\n",
            "train loss:0.00023166119132151403\n",
            "train loss:9.313967415875819e-05\n",
            "train loss:0.0001603616220312986\n",
            "train loss:0.0001309587515378811\n",
            "train loss:8.567416678075656e-05\n",
            "train loss:0.00023470750300926338\n",
            "train loss:0.00011322825871050062\n",
            "train loss:0.0001318852842318404\n",
            "train loss:0.00021552685997554703\n",
            "train loss:0.0009936858608748562\n",
            "train loss:0.00019479128148777448\n",
            "train loss:0.0001491041288139344\n",
            "train loss:0.00012731029612254172\n",
            "train loss:0.000594141359152802\n",
            "train loss:0.00017433906320017426\n",
            "train loss:0.0007158738210276645\n",
            "train loss:0.0002356696029859547\n",
            "train loss:0.00017312897115923808\n",
            "train loss:0.00013930644598862034\n",
            "train loss:9.748150483586015e-05\n",
            "train loss:8.87527728080543e-05\n",
            "train loss:0.0001854490928487411\n",
            "train loss:0.0001564344914654044\n",
            "train loss:0.00015025360106820913\n",
            "train loss:0.00019891445109957933\n",
            "train loss:0.0002916779955153257\n",
            "train loss:0.00016663337969663165\n",
            "train loss:0.0002438799338510283\n",
            "train loss:0.00019419169954412775\n",
            "train loss:0.0005325140467665931\n",
            "train loss:0.00024246080644681656\n",
            "train loss:0.000175632672241731\n",
            "train loss:0.0002172720037501832\n",
            "train loss:0.00012268584296399422\n",
            "train loss:0.00019749032366139248\n",
            "train loss:0.00021175706751293925\n",
            "train loss:0.0002968865202157663\n",
            "train loss:0.00017260413723501375\n",
            "train loss:0.0001426499298113049\n",
            "train loss:0.0002588261932123247\n",
            "train loss:0.0001433989208073899\n",
            "train loss:0.0001539702955521057\n",
            "train loss:0.0001518481909394891\n",
            "train loss:0.00012394896739694733\n",
            "train loss:0.00014648978673388675\n",
            "train loss:0.00016647319436860486\n",
            "train loss:0.0001771510091761844\n",
            "train loss:0.000197681115893561\n",
            "train loss:0.00012042507024330945\n",
            "train loss:8.623291826721763e-05\n",
            "train loss:6.685434796303413e-05\n",
            "train loss:8.71737501246187e-05\n",
            "train loss:0.0009342329966327535\n",
            "train loss:0.00011606848688866186\n",
            "train loss:0.0004962722361044668\n",
            "train loss:0.00015407882869441039\n",
            "train loss:0.0001879149403377873\n",
            "train loss:0.00020644821878717796\n",
            "train loss:0.0001768352525484377\n",
            "train loss:0.000204663259601941\n",
            "train loss:9.986195833553719e-05\n",
            "=== epoch:175, train acc:0.999875, test acc:0.946 ===\n",
            "train loss:0.00020513715414525793\n",
            "train loss:0.00022078007691505225\n",
            "train loss:0.00012170469441571243\n",
            "train loss:0.00016105862401356044\n",
            "train loss:0.00012144017182048525\n",
            "train loss:0.00013292542959343108\n",
            "train loss:0.0002455898717679319\n",
            "train loss:0.0001414618694768352\n",
            "train loss:0.0030945924952136615\n",
            "train loss:0.00012075944735972915\n",
            "train loss:0.00014731971419340272\n",
            "train loss:0.0005384395411357346\n",
            "train loss:0.00010634175711634789\n",
            "train loss:0.00019067943049079546\n",
            "train loss:0.00027091020674892356\n",
            "train loss:0.00015366510901282116\n",
            "train loss:0.00019032628966964875\n",
            "train loss:0.00014268333788088365\n",
            "train loss:0.00011298784581323916\n",
            "train loss:0.00011466602224018168\n",
            "train loss:0.00015673846199528964\n",
            "train loss:9.316537186885097e-05\n",
            "train loss:0.00017822890394959256\n",
            "train loss:0.0001586582885297334\n",
            "train loss:0.00010622428017876442\n",
            "train loss:0.0001947830572373107\n",
            "train loss:0.00011979256995897155\n",
            "train loss:8.429440769432839e-05\n",
            "train loss:0.00013993423873789082\n",
            "train loss:0.0001122393253181142\n",
            "train loss:0.0002853267973554014\n",
            "train loss:9.093787126364854e-05\n",
            "train loss:0.0001551583172369391\n",
            "train loss:0.00020193446653524606\n",
            "train loss:0.00029928035635226917\n",
            "train loss:0.00013273114526512334\n",
            "train loss:0.00018511690740264077\n",
            "train loss:0.00015271276653714983\n",
            "train loss:0.00010918834266418714\n",
            "train loss:0.00013678935441107707\n",
            "train loss:0.00012679645743269658\n",
            "train loss:0.00015727616670496508\n",
            "train loss:0.00012807507149627072\n",
            "train loss:0.00014972471238993698\n",
            "train loss:0.00013424652273387873\n",
            "train loss:0.00015088116326212808\n",
            "train loss:0.00016490538615369913\n",
            "train loss:0.0001724192589625977\n",
            "train loss:0.00010636965553803348\n",
            "train loss:0.0003935318737042087\n",
            "train loss:0.00018217109343216694\n",
            "train loss:0.00011365906119081674\n",
            "train loss:0.00017079608639556775\n",
            "train loss:0.00016148575893024152\n",
            "train loss:0.00016388343059116041\n",
            "train loss:0.00010401700619552946\n",
            "train loss:0.00018179141561639131\n",
            "train loss:0.00015351227726760578\n",
            "train loss:0.0003218047280756922\n",
            "train loss:0.00018229785778342767\n",
            "train loss:0.00017320876149195415\n",
            "train loss:0.00013130287899735453\n",
            "train loss:0.00012246690235938361\n",
            "train loss:0.00010138499885440482\n",
            "train loss:0.00011998848931534712\n",
            "train loss:0.0002060053210741197\n",
            "train loss:8.631383550422224e-05\n",
            "train loss:8.29416925971767e-05\n",
            "train loss:0.0001300637065176747\n",
            "train loss:0.00013642198281619232\n",
            "train loss:9.748126829688704e-05\n",
            "train loss:8.911562098367003e-05\n",
            "train loss:0.00015618059294877172\n",
            "train loss:0.000204548805315089\n",
            "train loss:0.00016559274520523064\n",
            "train loss:0.0001637568509000963\n",
            "train loss:0.00015120533862870235\n",
            "train loss:0.00012471780474772188\n",
            "train loss:0.00010425343078545373\n",
            "train loss:0.00010996984457014112\n",
            "=== epoch:176, train acc:0.99975, test acc:0.9405 ===\n",
            "train loss:0.00010785679802946912\n",
            "train loss:0.00021731714241919042\n",
            "train loss:5.6617381182426935e-05\n",
            "train loss:0.0001352763270533525\n",
            "train loss:0.00011470247872849002\n",
            "train loss:0.0001564069766957729\n",
            "train loss:5.4578443251443455e-05\n",
            "train loss:8.27194247550973e-05\n",
            "train loss:0.00010854769365768873\n",
            "train loss:0.0007337016711118377\n",
            "train loss:0.0001335595209269636\n",
            "train loss:0.00012094812476698427\n",
            "train loss:0.0001207073868642073\n",
            "train loss:0.0001411774695720327\n",
            "train loss:0.0002037610515045238\n",
            "train loss:0.0001731822122597789\n",
            "train loss:0.00014976808410012974\n",
            "train loss:8.420567926900516e-05\n",
            "train loss:7.605857162575678e-05\n",
            "train loss:0.00010446490711888308\n",
            "train loss:0.000155416156957352\n",
            "train loss:0.00025135099124146376\n",
            "train loss:0.00012498857484017243\n",
            "train loss:0.00010325745461806374\n",
            "train loss:0.0005483748320919099\n",
            "train loss:0.0002559907080578373\n",
            "train loss:0.0001229875269882592\n",
            "train loss:0.00013057453569975222\n",
            "train loss:0.00012423558254706605\n",
            "train loss:9.470956585359139e-05\n",
            "train loss:0.00021613027416000452\n",
            "train loss:0.00010404629962801866\n",
            "train loss:0.00010527811772355386\n",
            "train loss:9.684125633838519e-05\n",
            "train loss:6.661085308139936e-05\n",
            "train loss:0.00014333451625883747\n",
            "train loss:0.00038257260216072663\n",
            "train loss:0.0005045426723595267\n",
            "train loss:0.00021581745265236347\n",
            "train loss:0.0001595241643403101\n",
            "train loss:0.00017548121853663538\n",
            "train loss:0.00011673805682740454\n",
            "train loss:0.0001405992776591208\n",
            "train loss:0.00010393166573375026\n",
            "train loss:0.00011092589703578924\n",
            "train loss:0.00010622956623630788\n",
            "train loss:9.198237861448343e-05\n",
            "train loss:8.770238295191854e-05\n",
            "train loss:0.00024193456783520883\n",
            "train loss:6.990115575067143e-05\n",
            "train loss:0.000456997511065406\n",
            "train loss:0.0001908448535688074\n",
            "train loss:0.00011358972278828315\n",
            "train loss:0.00028080095856298696\n",
            "train loss:6.718688664604007e-05\n",
            "train loss:0.00015627650425858414\n",
            "train loss:0.00019955625383224232\n",
            "train loss:0.0001151466740891173\n",
            "train loss:0.00014469100913941786\n",
            "train loss:0.00027740098447664253\n",
            "train loss:0.00024457180684826286\n",
            "train loss:0.00016697172327194095\n",
            "train loss:0.00011676856619462949\n",
            "train loss:0.00010990966405296517\n",
            "train loss:9.110334567880723e-05\n",
            "train loss:6.9021055849891e-05\n",
            "train loss:0.0002036081615092707\n",
            "train loss:0.00014484152990391439\n",
            "train loss:8.48287835022813e-05\n",
            "train loss:0.00013803608388866605\n",
            "train loss:8.987599117796366e-05\n",
            "train loss:7.807666613038845e-05\n",
            "train loss:0.00017078107870455473\n",
            "train loss:8.653825765220745e-05\n",
            "train loss:7.686896137699704e-05\n",
            "train loss:8.84574157220809e-05\n",
            "train loss:0.0003049376607424424\n",
            "train loss:0.00019933391532943337\n",
            "train loss:0.00010861279456286775\n",
            "train loss:0.000268905099611631\n",
            "=== epoch:177, train acc:1.0, test acc:0.942 ===\n",
            "train loss:0.00012248541127232314\n",
            "train loss:5.936310095408035e-05\n",
            "train loss:9.882927197598396e-05\n",
            "train loss:0.00018937897527662742\n",
            "train loss:0.00010058018846600588\n",
            "train loss:0.00016338740839404287\n",
            "train loss:7.693686879773351e-05\n",
            "train loss:0.00014692435925121972\n",
            "train loss:0.00013794399789286894\n",
            "train loss:0.00013285047946235359\n",
            "train loss:9.002473015601215e-05\n",
            "train loss:0.00017294487447089586\n",
            "train loss:0.00015135483291298763\n",
            "train loss:0.00010549496129668398\n",
            "train loss:0.0001551526638209731\n",
            "train loss:7.054509196804522e-05\n",
            "train loss:0.00013967258013030106\n",
            "train loss:0.000114857939405841\n",
            "train loss:0.00010539193783880746\n",
            "train loss:0.00010116055394703333\n",
            "train loss:0.00010950350135967413\n",
            "train loss:7.31735973617894e-05\n",
            "train loss:0.000204471838053167\n",
            "train loss:0.00011630728177333657\n",
            "train loss:0.00010850493565896135\n",
            "train loss:5.533848820648245e-05\n",
            "train loss:8.725726318875463e-05\n",
            "train loss:0.0001815849702799094\n",
            "train loss:0.00011420842611233536\n",
            "train loss:8.96936140408937e-05\n",
            "train loss:9.217207421468926e-05\n",
            "train loss:0.00012751950012159012\n",
            "train loss:8.66502514914311e-05\n",
            "train loss:7.398643328200415e-05\n",
            "train loss:0.00010835344142546242\n",
            "train loss:0.00010075451197903842\n",
            "train loss:7.359626312263108e-05\n",
            "train loss:0.00010614654839220243\n",
            "train loss:0.0001818771013146253\n",
            "train loss:8.371105221772571e-05\n",
            "train loss:8.636289881294909e-05\n",
            "train loss:8.174828558487785e-05\n",
            "train loss:5.638196504430088e-05\n",
            "train loss:0.00011028360692578571\n",
            "train loss:0.00016413341724080732\n",
            "train loss:0.00011050770101373182\n",
            "train loss:7.730232976662881e-05\n",
            "train loss:0.0001308435987758096\n",
            "train loss:0.00011590197173523952\n",
            "train loss:9.645833255536492e-05\n",
            "train loss:5.8662035635620656e-05\n",
            "train loss:0.00013640611848541093\n",
            "train loss:7.676540933102281e-05\n",
            "train loss:8.064659314711384e-05\n",
            "train loss:7.068880717371542e-05\n",
            "train loss:0.0001552369474300773\n",
            "train loss:0.0002011870061174122\n",
            "train loss:0.00012245219032542447\n",
            "train loss:0.00013920570481161138\n",
            "train loss:8.611620093746117e-05\n",
            "train loss:0.00016480036286703337\n",
            "train loss:0.00010525533295848654\n",
            "train loss:0.00012244140861337262\n",
            "train loss:0.00016960539766422723\n",
            "train loss:6.36576299865635e-05\n",
            "train loss:7.090873085323353e-05\n",
            "train loss:0.000307563285681452\n",
            "train loss:0.00013366283593768536\n",
            "train loss:0.00015611643759016547\n",
            "train loss:0.00012334285099227169\n",
            "train loss:0.00020154972457767074\n",
            "train loss:9.417618298097392e-05\n",
            "train loss:0.01541212323143468\n",
            "train loss:0.0002555845099864762\n",
            "train loss:0.0001704061935599934\n",
            "train loss:0.00025465214840979873\n",
            "train loss:0.00024457687525314073\n",
            "train loss:0.00011153523807918619\n",
            "train loss:0.00018747041846652813\n",
            "train loss:0.00015648553220247622\n",
            "=== epoch:178, train acc:1.0, test acc:0.942 ===\n",
            "train loss:0.00019711323263867816\n",
            "train loss:6.013697436086719e-05\n",
            "train loss:0.00019720559636394718\n",
            "train loss:0.00013601327929986924\n",
            "train loss:0.00018770174121445954\n",
            "train loss:0.00010735956167143224\n",
            "train loss:5.5969404266264296e-05\n",
            "train loss:0.000275725432341161\n",
            "train loss:0.00010601137715278592\n",
            "train loss:0.00015150438875746608\n",
            "train loss:0.00013078370266833436\n",
            "train loss:0.0001232517981188494\n",
            "train loss:0.000193062460200173\n",
            "train loss:0.0007567948242258557\n",
            "train loss:9.432807823381779e-05\n",
            "train loss:5.9212320060418545e-05\n",
            "train loss:0.00014941455005948318\n",
            "train loss:0.0005605462010622649\n",
            "train loss:9.115047719500428e-05\n",
            "train loss:0.0005111985217162603\n",
            "train loss:8.325974976422234e-05\n",
            "train loss:0.000132897685478416\n",
            "train loss:8.432634859604307e-05\n",
            "train loss:0.0004484662514126445\n",
            "train loss:8.988427377773733e-05\n",
            "train loss:0.00010093824773048809\n",
            "train loss:0.00011313891286892234\n",
            "train loss:8.550452893856848e-05\n",
            "train loss:0.0004401768575435207\n",
            "train loss:0.00010605766235233752\n",
            "train loss:0.0001505889375099363\n",
            "train loss:7.6980150750854e-05\n",
            "train loss:0.00018046236115921513\n",
            "train loss:0.00019302864088261817\n",
            "train loss:0.00011217335983551953\n",
            "train loss:0.00020925694934121656\n",
            "train loss:0.00047448881266417414\n",
            "train loss:0.00011167525641797379\n",
            "train loss:0.00012612479335479687\n",
            "train loss:0.00012558159587058688\n",
            "train loss:0.00010974524730394081\n",
            "train loss:0.00014283767582110181\n",
            "train loss:8.835800124085605e-05\n",
            "train loss:0.00012541987764010042\n",
            "train loss:0.0001219442650158182\n",
            "train loss:0.00014571877134185806\n",
            "train loss:9.055916532622812e-05\n",
            "train loss:9.623702137919114e-05\n",
            "train loss:0.00025565125032306135\n",
            "train loss:9.431562133017369e-05\n",
            "train loss:0.00010255473559095036\n",
            "train loss:9.504477489603987e-05\n",
            "train loss:0.00012430884128881224\n",
            "train loss:0.00011123885374595733\n",
            "train loss:0.00016977756051615286\n",
            "train loss:0.00010758691617910262\n",
            "train loss:0.00021177293859206832\n",
            "train loss:0.00018991091205500905\n",
            "train loss:0.00017987020407678268\n",
            "train loss:0.0001677591439227526\n",
            "train loss:0.00015923375710259684\n",
            "train loss:0.00011483286192777088\n",
            "train loss:7.720903446545261e-05\n",
            "train loss:9.555178590448835e-05\n",
            "train loss:0.00018764888353216242\n",
            "train loss:0.00011923189726594021\n",
            "train loss:0.0001489412593169044\n",
            "train loss:8.905868989778622e-05\n",
            "train loss:0.00011848375580286854\n",
            "train loss:0.0003763104358631876\n",
            "train loss:0.00010404339931703218\n",
            "train loss:0.00010842648336294895\n",
            "train loss:0.00030565184844892954\n",
            "train loss:0.00012890675652557315\n",
            "train loss:0.00010441003224340774\n",
            "train loss:0.000180677187597016\n",
            "train loss:0.00024384532434447065\n",
            "train loss:0.002377750322235114\n",
            "train loss:0.00016194733231298818\n",
            "train loss:0.00010832911965252389\n",
            "=== epoch:179, train acc:0.999875, test acc:0.942 ===\n",
            "train loss:0.0001135772772806193\n",
            "train loss:0.000280587943774849\n",
            "train loss:0.00014799608772505415\n",
            "train loss:0.00013946429243776704\n",
            "train loss:0.00013126631636274725\n",
            "train loss:0.0001227494552015376\n",
            "train loss:0.00010959123171805654\n",
            "train loss:9.904463224067828e-05\n",
            "train loss:0.000142689522860628\n",
            "train loss:7.381620081798641e-05\n",
            "train loss:0.00019400872960282481\n",
            "train loss:0.00013237569229064783\n",
            "train loss:0.00016731511092588438\n",
            "train loss:0.0001288567165947263\n",
            "train loss:0.00013039701884985172\n",
            "train loss:6.423149126759719e-05\n",
            "train loss:9.150751574561217e-05\n",
            "train loss:6.34858387033784e-05\n",
            "train loss:8.502661382140548e-05\n",
            "train loss:0.00012765470706315794\n",
            "train loss:7.09220782363653e-05\n",
            "train loss:8.265562156049906e-05\n",
            "train loss:9.604966474141965e-05\n",
            "train loss:0.0001804779017874801\n",
            "train loss:0.00012863252694007214\n",
            "train loss:0.00011393942170075252\n",
            "train loss:8.589997334344695e-05\n",
            "train loss:0.0001340380292211623\n",
            "train loss:9.158669156578347e-05\n",
            "train loss:0.00010671837624960968\n",
            "train loss:8.016727539136743e-05\n",
            "train loss:9.590092892634798e-05\n",
            "train loss:0.04154168289291445\n",
            "train loss:0.00015328515981831848\n",
            "train loss:0.00010855830868857996\n",
            "train loss:0.00015726809252786608\n",
            "train loss:8.672779311119555e-05\n",
            "train loss:7.071086331557978e-05\n",
            "train loss:7.064431818579916e-05\n",
            "train loss:0.000256020682004533\n",
            "train loss:7.283972841887718e-05\n",
            "train loss:6.399428496055592e-05\n",
            "train loss:0.0001687792690561395\n",
            "train loss:6.424011204801175e-05\n",
            "train loss:5.706745943987425e-05\n",
            "train loss:0.00010923582998158683\n",
            "train loss:4.6029820750061985e-05\n",
            "train loss:7.968579711011125e-05\n",
            "train loss:0.0001163151620683457\n",
            "train loss:7.459630903207211e-05\n",
            "train loss:5.6413250867217614e-05\n",
            "train loss:7.014008231951492e-05\n",
            "train loss:8.150303250230325e-05\n",
            "train loss:9.010848316833063e-05\n",
            "train loss:9.564360176007697e-05\n",
            "train loss:8.210886618536598e-05\n",
            "train loss:9.485231155825053e-05\n",
            "train loss:8.080274338113882e-05\n",
            "train loss:9.463306125099786e-05\n",
            "train loss:7.760297605082608e-05\n",
            "train loss:8.758316046864277e-05\n",
            "train loss:4.9509491320435525e-05\n",
            "train loss:4.541340879743973e-05\n",
            "train loss:7.492342616274291e-05\n",
            "train loss:9.218188447179663e-05\n",
            "train loss:6.179822842937082e-05\n",
            "train loss:5.533828363449754e-05\n",
            "train loss:5.951518349032639e-05\n",
            "train loss:8.823527096473229e-05\n",
            "train loss:8.443073653857015e-05\n",
            "train loss:5.90036973343413e-05\n",
            "train loss:6.329740239916773e-05\n",
            "train loss:5.709952753059867e-05\n",
            "train loss:0.0001236874433929943\n",
            "train loss:4.166626306358291e-05\n",
            "train loss:0.00014848608430968537\n",
            "train loss:0.0001076944039714671\n",
            "train loss:0.000147204807031996\n",
            "train loss:5.094857387388598e-05\n",
            "train loss:9.075976011837847e-05\n",
            "=== epoch:180, train acc:0.99975, test acc:0.94 ===\n",
            "train loss:9.940408508776736e-05\n",
            "train loss:0.0008890750689612614\n",
            "train loss:8.897957290423919e-05\n",
            "train loss:0.00014566760651617952\n",
            "train loss:8.634260588011226e-05\n",
            "train loss:5.4913364572557847e-05\n",
            "train loss:0.00011734667273599389\n",
            "train loss:6.65185464647233e-05\n",
            "train loss:7.142791331678225e-05\n",
            "train loss:4.1480032687608374e-05\n",
            "train loss:5.7779784184464065e-05\n",
            "train loss:7.033880853881862e-05\n",
            "train loss:0.0001089560544757455\n",
            "train loss:0.00022891197008937305\n",
            "train loss:8.265249313686744e-05\n",
            "train loss:8.239872832935514e-05\n",
            "train loss:0.00010442573592184414\n",
            "train loss:6.565924677071224e-05\n",
            "train loss:9.967393701244864e-05\n",
            "train loss:7.858827209033523e-05\n",
            "train loss:0.003279555941156134\n",
            "train loss:7.337547265932529e-05\n",
            "train loss:8.697038023992794e-05\n",
            "train loss:9.664797768465383e-05\n",
            "train loss:0.00017832206078481516\n",
            "train loss:7.061033184316395e-05\n",
            "train loss:5.8775296045641064e-05\n",
            "train loss:5.6658635186299336e-05\n",
            "train loss:8.315161175576135e-05\n",
            "train loss:0.00013870661842566907\n",
            "train loss:7.284949007550387e-05\n",
            "train loss:5.8933944561427405e-05\n",
            "train loss:0.00012139248792380317\n",
            "train loss:9.611722585807913e-05\n",
            "train loss:8.885036900690407e-05\n",
            "train loss:6.167403405470142e-05\n",
            "train loss:6.284550475074809e-05\n",
            "train loss:6.844355016130593e-05\n",
            "train loss:5.696931734753922e-05\n",
            "train loss:0.00011151379558014374\n",
            "train loss:6.0493827100745666e-05\n",
            "train loss:0.00011808943858114926\n",
            "train loss:0.00010243890119088903\n",
            "train loss:9.599791659033828e-05\n",
            "train loss:9.082448942369733e-05\n",
            "train loss:0.00014774579979894999\n",
            "train loss:5.7447020939602944e-05\n",
            "train loss:6.842978614402579e-05\n",
            "train loss:0.008026772844342394\n",
            "train loss:0.00011714846321460418\n",
            "train loss:0.00011268075361110921\n",
            "train loss:0.00014800223353301562\n",
            "train loss:0.00031582047567761223\n",
            "train loss:7.103308254997908e-05\n",
            "train loss:7.92681305340847e-05\n",
            "train loss:6.647731614229718e-05\n",
            "train loss:7.862764230906624e-05\n",
            "train loss:8.397799202745772e-05\n",
            "train loss:8.101960037099077e-05\n",
            "train loss:6.0997730579386036e-05\n",
            "train loss:0.0002136383923740852\n",
            "train loss:6.397890462039024e-05\n",
            "train loss:0.00034316087456062446\n",
            "train loss:8.510369626107511e-05\n",
            "train loss:0.000448746395835291\n",
            "train loss:0.00014865446018907274\n",
            "train loss:6.0479741377827776e-05\n",
            "train loss:7.576180937783572e-05\n",
            "train loss:7.555694732180336e-05\n",
            "train loss:6.046624335680938e-05\n",
            "train loss:6.459773287864229e-05\n",
            "train loss:0.0006227860149773021\n",
            "train loss:5.900967464720788e-05\n",
            "train loss:9.965049620218054e-05\n",
            "train loss:0.00017405542866298315\n",
            "train loss:8.163768729021467e-05\n",
            "train loss:4.871211711911831e-05\n",
            "train loss:7.13813813116079e-05\n",
            "train loss:0.0002361020983841286\n",
            "train loss:0.00013877139642008737\n",
            "=== epoch:181, train acc:1.0, test acc:0.9465 ===\n",
            "train loss:9.359277699999774e-05\n",
            "train loss:9.641101744044985e-05\n",
            "train loss:0.00014579640654191053\n",
            "train loss:6.696563230315055e-05\n",
            "train loss:0.00010503242748240972\n",
            "train loss:9.606220666196295e-05\n",
            "train loss:8.216628890522789e-05\n",
            "train loss:9.904884640475658e-05\n",
            "train loss:6.111972370220936e-05\n",
            "train loss:6.613772727985691e-05\n",
            "train loss:0.00010949369473075521\n",
            "train loss:6.597225466853806e-05\n",
            "train loss:5.2478512836734967e-05\n",
            "train loss:7.307067562116355e-05\n",
            "train loss:6.067608824978297e-05\n",
            "train loss:3.601133958506947e-05\n",
            "train loss:9.22462505855072e-05\n",
            "train loss:5.13299943540856e-05\n",
            "train loss:9.904287294324672e-05\n",
            "train loss:5.1040401638331465e-05\n",
            "train loss:7.994084234688751e-05\n",
            "train loss:7.35619318089485e-05\n",
            "train loss:4.786305172784834e-05\n",
            "train loss:8.045905260085471e-05\n",
            "train loss:5.017911111987833e-05\n",
            "train loss:4.060090450256958e-05\n",
            "train loss:8.035782388782078e-05\n",
            "train loss:9.38958869454753e-05\n",
            "train loss:0.0008264727643778649\n",
            "train loss:3.830374918871652e-05\n",
            "train loss:4.859004142110024e-05\n",
            "train loss:0.00010366054825435048\n",
            "train loss:6.224057711333103e-05\n",
            "train loss:7.579551022519139e-05\n",
            "train loss:5.4339100796860045e-05\n",
            "train loss:8.740449088132126e-05\n",
            "train loss:5.8261093787722336e-05\n",
            "train loss:8.513092407207546e-05\n",
            "train loss:5.3553846742300665e-05\n",
            "train loss:9.565063622739054e-05\n",
            "train loss:6.640721527826528e-05\n",
            "train loss:5.5564536549681575e-05\n",
            "train loss:5.153105567603952e-05\n",
            "train loss:8.533928690018261e-05\n",
            "train loss:5.883763121085155e-05\n",
            "train loss:4.509210387391261e-05\n",
            "train loss:0.0001841042793454194\n",
            "train loss:8.643463462536003e-05\n",
            "train loss:9.910261124652181e-05\n",
            "train loss:5.303837102886898e-05\n",
            "train loss:7.318744557830008e-05\n",
            "train loss:0.0001078847230078699\n",
            "train loss:0.00010432350847797833\n",
            "train loss:8.088145344022096e-05\n",
            "train loss:4.874468067236402e-05\n",
            "train loss:0.0001148478649226082\n",
            "train loss:0.00011016851142790048\n",
            "train loss:9.384725134973233e-05\n",
            "train loss:5.5722630497924906e-05\n",
            "train loss:9.762252519940725e-05\n",
            "train loss:0.00013795636225205776\n",
            "train loss:6.823503901652813e-05\n",
            "train loss:7.857556540188507e-05\n",
            "train loss:0.00010139231507174228\n",
            "train loss:7.183355065494004e-05\n",
            "train loss:5.099664016946061e-05\n",
            "train loss:7.924920973773338e-05\n",
            "train loss:0.00011396590472934724\n",
            "train loss:9.419127037305662e-05\n",
            "train loss:9.461629717682712e-05\n",
            "train loss:9.843417739774673e-05\n",
            "train loss:6.103531166424666e-05\n",
            "train loss:5.63527668187074e-05\n",
            "train loss:9.918664952564928e-05\n",
            "train loss:0.00010476675257232063\n",
            "train loss:9.235422118565969e-05\n",
            "train loss:8.907760691793296e-05\n",
            "train loss:5.2871205833926695e-05\n",
            "train loss:7.380326713377754e-05\n",
            "train loss:7.782199281052174e-05\n",
            "=== epoch:182, train acc:1.0, test acc:0.9445 ===\n",
            "train loss:9.140834153559035e-05\n",
            "train loss:7.878572925915781e-05\n",
            "train loss:0.00013454344642934002\n",
            "train loss:0.00010127682928912366\n",
            "train loss:0.00012099097365620628\n",
            "train loss:8.058323447763263e-05\n",
            "train loss:0.0001388781985684314\n",
            "train loss:0.00010581764561039902\n",
            "train loss:7.444850724292184e-05\n",
            "train loss:7.592943639018036e-05\n",
            "train loss:0.00016308819417964305\n",
            "train loss:7.888436527902361e-05\n",
            "train loss:5.950008110221494e-05\n",
            "train loss:0.00011625744513414079\n",
            "train loss:8.856255311028759e-05\n",
            "train loss:6.875549152572111e-05\n",
            "train loss:0.025251541314606323\n",
            "train loss:7.020049187771624e-05\n",
            "train loss:6.760855568341522e-05\n",
            "train loss:0.030491554335358863\n",
            "train loss:5.5893791529259374e-05\n",
            "train loss:0.00012017335383339763\n",
            "train loss:8.721180126060133e-05\n",
            "train loss:0.0001024186180224142\n",
            "train loss:9.956450465871823e-05\n",
            "train loss:0.000122067943709012\n",
            "train loss:0.00012494949822629406\n",
            "train loss:9.05025901265706e-05\n",
            "train loss:9.027602043029237e-05\n",
            "train loss:0.00011085498647887578\n",
            "train loss:0.0001059597685382191\n",
            "train loss:8.76572988316872e-05\n",
            "train loss:8.383295896327086e-05\n",
            "train loss:6.132956352107268e-05\n",
            "train loss:0.00011042889092065835\n",
            "train loss:0.0001176389069642008\n",
            "train loss:0.00016030976283097458\n",
            "train loss:8.853063033654962e-05\n",
            "train loss:0.00010362515001831889\n",
            "train loss:0.00012639155024633263\n",
            "train loss:6.049692400125987e-05\n",
            "train loss:0.00010802274664603806\n",
            "train loss:8.667726598832614e-05\n",
            "train loss:0.0001635614389905913\n",
            "train loss:7.94741089341851e-05\n",
            "train loss:0.00010474337963501515\n",
            "train loss:0.007946920175942156\n",
            "train loss:0.0001222754650392842\n",
            "train loss:8.93491154908813e-05\n",
            "train loss:8.97939229519246e-05\n",
            "train loss:9.52928487911714e-05\n",
            "train loss:0.00014336826595245545\n",
            "train loss:0.00013199064800875602\n",
            "train loss:0.0001345610373400714\n",
            "train loss:0.0006218998266808988\n",
            "train loss:0.00015711401108442984\n",
            "train loss:6.928609444970729e-05\n",
            "train loss:0.0028393549877252346\n",
            "train loss:0.00011374487178758288\n",
            "train loss:7.802068669110741e-05\n",
            "train loss:9.273667712928236e-05\n",
            "train loss:8.286104747026653e-05\n",
            "train loss:0.00010283465059839867\n",
            "train loss:0.0002449168404650869\n",
            "train loss:0.0002250161949609028\n",
            "train loss:0.00018782342037867883\n",
            "train loss:0.00014376294074966943\n",
            "train loss:7.748154720031142e-05\n",
            "train loss:0.00017797487924944434\n",
            "train loss:0.00014301173549044523\n",
            "train loss:0.0004686675678925328\n",
            "train loss:0.0001114441834982164\n",
            "train loss:8.738138429137319e-05\n",
            "train loss:0.00018681346245186424\n",
            "train loss:0.00021197263221416806\n",
            "train loss:0.00012335081614432762\n",
            "train loss:0.0001246740369978009\n",
            "train loss:0.00020295934115816347\n",
            "train loss:0.00015865130031738822\n",
            "train loss:0.0002088508686723357\n",
            "=== epoch:183, train acc:0.999875, test acc:0.9425 ===\n",
            "train loss:0.0004113227660887122\n",
            "train loss:0.0001046473936042186\n",
            "train loss:0.0001485230235647104\n",
            "train loss:0.0009096359791712069\n",
            "train loss:0.00017446191244904289\n",
            "train loss:0.00013274232220140841\n",
            "train loss:0.00019600631731741375\n",
            "train loss:0.0004886860069521987\n",
            "train loss:0.00018791366492626753\n",
            "train loss:0.0003715522057394954\n",
            "train loss:0.0001577024741727046\n",
            "train loss:0.00010234694796930342\n",
            "train loss:0.00010534782262112484\n",
            "train loss:0.00014713701954103482\n",
            "train loss:0.00022473817738237047\n",
            "train loss:0.00014912069177691546\n",
            "train loss:8.862328763284707e-05\n",
            "train loss:0.0001424997079441726\n",
            "train loss:0.00016581578396151\n",
            "train loss:0.0001577435760034474\n",
            "train loss:9.249825773243145e-05\n",
            "train loss:0.00014135841009411685\n",
            "train loss:0.0001489290707452217\n",
            "train loss:0.00030752698892219546\n",
            "train loss:0.00011256725716743857\n",
            "train loss:0.000213752696631681\n",
            "train loss:0.00028412060573656134\n",
            "train loss:0.00033442282699388593\n",
            "train loss:6.190182838120067e-05\n",
            "train loss:0.0001631203539088822\n",
            "train loss:0.0001231654415418643\n",
            "train loss:0.0001252476754406996\n",
            "train loss:7.774001484715711e-05\n",
            "train loss:0.0005496554922532793\n",
            "train loss:8.085749956720751e-05\n",
            "train loss:0.005234474356503243\n",
            "train loss:0.00020444750363762495\n",
            "train loss:0.0001370141736505558\n",
            "train loss:0.0002902973927343586\n",
            "train loss:0.00011525666434002141\n",
            "train loss:0.00016591307322268243\n",
            "train loss:0.00016414984760945668\n",
            "train loss:0.00014265582094231027\n",
            "train loss:9.502155898869054e-05\n",
            "train loss:8.023401029172334e-05\n",
            "train loss:0.00015702456823761632\n",
            "train loss:0.00024605343583827843\n",
            "train loss:0.00038632100498547326\n",
            "train loss:9.796824304996291e-05\n",
            "train loss:8.954336820350164e-05\n",
            "train loss:8.532184763285519e-05\n",
            "train loss:6.658638371525535e-05\n",
            "train loss:0.00011552958813702843\n",
            "train loss:6.72677862384857e-05\n",
            "train loss:8.093202163468482e-05\n",
            "train loss:9.72765951513165e-05\n",
            "train loss:0.0001776277120797672\n",
            "train loss:7.945827257091762e-05\n",
            "train loss:0.00011702886627545289\n",
            "train loss:9.223041295411546e-05\n",
            "train loss:8.299657733850668e-05\n",
            "train loss:8.781207168019225e-05\n",
            "train loss:0.00011265731950902998\n",
            "train loss:6.489126958031432e-05\n",
            "train loss:8.048428943447981e-05\n",
            "train loss:0.0002558090548334378\n",
            "train loss:0.00011719516761230937\n",
            "train loss:0.00012002042936028087\n",
            "train loss:0.00015531637090942786\n",
            "train loss:6.138873137616886e-05\n",
            "train loss:0.00022912372859082626\n",
            "train loss:0.0001566973363687573\n",
            "train loss:5.690306010528637e-05\n",
            "train loss:0.00012395735108120578\n",
            "train loss:0.00010331394324271835\n",
            "train loss:8.155588367761017e-05\n",
            "train loss:8.78056208008279e-05\n",
            "train loss:8.057925720222145e-05\n",
            "train loss:0.00019550686507782542\n",
            "train loss:0.0002339347523371533\n",
            "=== epoch:184, train acc:1.0, test acc:0.9405 ===\n",
            "train loss:0.00016472994454198045\n",
            "train loss:0.00010395979610479744\n",
            "train loss:0.00014820758852137077\n",
            "train loss:0.00014401205473328374\n",
            "train loss:9.123603358962787e-05\n",
            "train loss:7.045216013823626e-05\n",
            "train loss:7.308256775988044e-05\n",
            "train loss:6.175072714620112e-05\n",
            "train loss:0.00017094453315075436\n",
            "train loss:8.04608882166119e-05\n",
            "train loss:0.00010779027323724897\n",
            "train loss:0.00015651427489489882\n",
            "train loss:0.00010596776762897346\n",
            "train loss:0.00014855057326400891\n",
            "train loss:9.942350033105632e-05\n",
            "train loss:0.0001641411125064702\n",
            "train loss:0.00012731955000082356\n",
            "train loss:8.867693981676715e-05\n",
            "train loss:0.00014197165279528092\n",
            "train loss:0.00013150429639674784\n",
            "train loss:9.964458036279215e-05\n",
            "train loss:8.05580106051387e-05\n",
            "train loss:0.00012539087994913243\n",
            "train loss:8.508062649885573e-05\n",
            "train loss:0.00012054116951919441\n",
            "train loss:6.94878829743012e-05\n",
            "train loss:0.000109468968266146\n",
            "train loss:0.0001376678244280258\n",
            "train loss:0.00014831043258016637\n",
            "train loss:0.00012692266424989694\n",
            "train loss:7.506127438530708e-05\n",
            "train loss:8.349100295895595e-05\n",
            "train loss:0.00010598990255717005\n",
            "train loss:0.00010902284585207213\n",
            "train loss:8.030009368942365e-05\n",
            "train loss:0.00014438913071014304\n",
            "train loss:0.00011339594998826058\n",
            "train loss:0.00013445737926973762\n",
            "train loss:7.972651297982095e-05\n",
            "train loss:9.94876143285803e-05\n",
            "train loss:0.00013133654998890814\n",
            "train loss:8.443760589832414e-05\n",
            "train loss:7.98896538834977e-05\n",
            "train loss:9.362451330148348e-05\n",
            "train loss:0.00010913004388699488\n",
            "train loss:9.649237615096882e-05\n",
            "train loss:8.341593721515554e-05\n",
            "train loss:0.00010671539458971887\n",
            "train loss:0.00012389045807577735\n",
            "train loss:0.00017133789888824165\n",
            "train loss:0.000345050815780263\n",
            "train loss:7.811490498981133e-05\n",
            "train loss:0.0001476884894266362\n",
            "train loss:6.169564762786456e-05\n",
            "train loss:8.44081374362522e-05\n",
            "train loss:7.653000742969953e-05\n",
            "train loss:7.997199390484524e-05\n",
            "train loss:4.849284511541879e-05\n",
            "train loss:0.0001076968680943059\n",
            "train loss:8.551354025507635e-05\n",
            "train loss:7.028900081286741e-05\n",
            "train loss:8.829346534399189e-05\n",
            "train loss:0.00012151132975709611\n",
            "train loss:0.00017983954717177116\n",
            "train loss:8.951329092650413e-05\n",
            "train loss:6.74510839638243e-05\n",
            "train loss:0.00012889056258010123\n",
            "train loss:0.00017555741919095006\n",
            "train loss:9.481026976083919e-05\n",
            "train loss:8.491508452179897e-05\n",
            "train loss:0.00014258190961723578\n",
            "train loss:0.00012328849258744806\n",
            "train loss:0.00018733043665459785\n",
            "train loss:0.0001465293830205086\n",
            "train loss:9.232548600618999e-05\n",
            "train loss:0.00011529674087343971\n",
            "train loss:5.3711352495801355e-05\n",
            "train loss:7.518100805741526e-05\n",
            "train loss:6.892709060010352e-05\n",
            "train loss:0.00013444803602771706\n",
            "=== epoch:185, train acc:1.0, test acc:0.94 ===\n",
            "train loss:0.00010129654556544514\n",
            "train loss:9.199916176398556e-05\n",
            "train loss:3.590317303742538e-05\n",
            "train loss:6.196529912454784e-05\n",
            "train loss:6.516260938549738e-05\n",
            "train loss:9.973286271550267e-05\n",
            "train loss:0.00011748090828440368\n",
            "train loss:0.00011330176210046945\n",
            "train loss:7.526389899584265e-05\n",
            "train loss:0.00012172344043134612\n",
            "train loss:8.509375661872905e-05\n",
            "train loss:6.828245509669859e-05\n",
            "train loss:0.00010486942708048277\n",
            "train loss:0.00013429228496572617\n",
            "train loss:0.0001297873203591076\n",
            "train loss:0.00016396328450776467\n",
            "train loss:0.00019506978402843687\n",
            "train loss:0.0001357792607819686\n",
            "train loss:0.00010558423475730463\n",
            "train loss:0.00011422555196534977\n",
            "train loss:0.0001321801141021262\n",
            "train loss:0.00012953512663629405\n",
            "train loss:0.0001052309304420732\n",
            "train loss:6.0399581705070684e-05\n",
            "train loss:0.00035523723016850676\n",
            "train loss:5.977740830333541e-05\n",
            "train loss:0.0001146396980806122\n",
            "train loss:7.302050802446397e-05\n",
            "train loss:9.746215755627504e-05\n",
            "train loss:0.00010095841615474695\n",
            "train loss:0.00011057089068546166\n",
            "train loss:0.00014158422250574855\n",
            "train loss:7.926857458334628e-05\n",
            "train loss:8.610450619612381e-05\n",
            "train loss:6.409004776054407e-05\n",
            "train loss:7.77428121679667e-05\n",
            "train loss:0.00010697955856344449\n",
            "train loss:8.74004050397639e-05\n",
            "train loss:6.737541820158743e-05\n",
            "train loss:0.00013059527737763004\n",
            "train loss:5.936736350765205e-05\n",
            "train loss:0.0001681545933729987\n",
            "train loss:8.44482977049901e-05\n",
            "train loss:8.04536574841464e-05\n",
            "train loss:7.507124741794735e-05\n",
            "train loss:7.231135440783516e-05\n",
            "train loss:8.734162854935655e-05\n",
            "train loss:9.091817421639562e-05\n",
            "train loss:0.00011508711937715866\n",
            "train loss:0.00017802146927583238\n",
            "train loss:0.00015890338330165854\n",
            "train loss:0.00010513892578423993\n",
            "train loss:0.00011643189482662832\n",
            "train loss:0.00015949133171427844\n",
            "train loss:0.00010722980456035589\n",
            "train loss:0.00013768463205956582\n",
            "train loss:0.00010473568097472376\n",
            "train loss:9.044406534354895e-05\n",
            "train loss:0.00011012365284726861\n",
            "train loss:0.00027244623586564936\n",
            "train loss:0.00010174781320818421\n",
            "train loss:7.949947602252151e-05\n",
            "train loss:8.114880283943212e-05\n",
            "train loss:0.00011501024203750364\n",
            "train loss:4.908378760122851e-05\n",
            "train loss:7.403708685078757e-05\n",
            "train loss:0.0001200675557118941\n",
            "train loss:0.00011093279919806531\n",
            "train loss:8.285880179421854e-05\n",
            "train loss:7.917139683445956e-05\n",
            "train loss:6.0927072268070605e-05\n",
            "train loss:5.954302773664357e-05\n",
            "train loss:7.289636456239023e-05\n",
            "train loss:8.767722079554124e-05\n",
            "train loss:0.0008773005218395627\n",
            "train loss:9.13384841664144e-05\n",
            "train loss:0.00011779645584657875\n",
            "train loss:0.00032853942959710407\n",
            "train loss:0.0031346829384081027\n",
            "train loss:8.882055050684838e-05\n",
            "=== epoch:186, train acc:1.0, test acc:0.9415 ===\n",
            "train loss:0.00010716523868149324\n",
            "train loss:0.00014971494261127523\n",
            "train loss:0.00011292002469655891\n",
            "train loss:0.0002776665405753105\n",
            "train loss:0.00023716723254438806\n",
            "train loss:0.0013286824769748351\n",
            "train loss:0.00022325649441493918\n",
            "train loss:0.00012638447357414575\n",
            "train loss:7.155346071010384e-05\n",
            "train loss:0.00010017420299844105\n",
            "train loss:0.009413227745094508\n",
            "train loss:0.00018671631536246132\n",
            "train loss:0.0006473565707282425\n",
            "train loss:0.0001808499361375636\n",
            "train loss:8.463900662903184e-05\n",
            "train loss:0.0001847702631095923\n",
            "train loss:0.000758904075287898\n",
            "train loss:0.00017405692443731133\n",
            "train loss:0.00011305918748420679\n",
            "train loss:0.00011787844004995302\n",
            "train loss:8.399310638481085e-05\n",
            "train loss:0.00013235193299082566\n",
            "train loss:0.00012954002163489508\n",
            "train loss:0.0001767080675273334\n",
            "train loss:0.00012144962406519704\n",
            "train loss:6.990153566727541e-05\n",
            "train loss:6.099222633045612e-05\n",
            "train loss:7.088477416456265e-05\n",
            "train loss:0.00012841721305021075\n",
            "train loss:0.00014208913409541275\n",
            "train loss:0.00019527729045492212\n",
            "train loss:0.0001067229998274593\n",
            "train loss:8.6303256043589e-05\n",
            "train loss:0.0001563428207051019\n",
            "train loss:0.00011644863336094062\n",
            "train loss:0.00012201919508997766\n",
            "train loss:0.00017228318644930754\n",
            "train loss:0.0001245500049333014\n",
            "train loss:0.00018968552074127037\n",
            "train loss:8.459254346663127e-05\n",
            "train loss:8.624561442207347e-05\n",
            "train loss:0.00011195726505859488\n",
            "train loss:7.962626177409825e-05\n",
            "train loss:0.00012593156471386272\n",
            "train loss:0.00014004941519974798\n",
            "train loss:9.492039762906473e-05\n",
            "train loss:0.0001248648284953785\n",
            "train loss:0.00011882267558654283\n",
            "train loss:7.43004644825939e-05\n",
            "train loss:0.00013146808474660766\n",
            "train loss:7.919417563814052e-05\n",
            "train loss:0.0019231395416895792\n",
            "train loss:0.00013129072984979488\n",
            "train loss:0.00013231879436962504\n",
            "train loss:7.547663645131708e-05\n",
            "train loss:0.00012194629785053615\n",
            "train loss:6.59650531130227e-05\n",
            "train loss:7.398940520812575e-05\n",
            "train loss:6.430575704409953e-05\n",
            "train loss:8.601941524702962e-05\n",
            "train loss:9.314949310197723e-05\n",
            "train loss:7.872652469983206e-05\n",
            "train loss:6.778194594351848e-05\n",
            "train loss:7.691431018841956e-05\n",
            "train loss:0.0011042203396899619\n",
            "train loss:5.4367813728113764e-05\n",
            "train loss:8.38098590779162e-05\n",
            "train loss:5.0516720279963876e-05\n",
            "train loss:4.722113733488117e-05\n",
            "train loss:7.332979723648092e-05\n",
            "train loss:8.698440786068394e-05\n",
            "train loss:0.00014775109405589044\n",
            "train loss:0.0001592730906177061\n",
            "train loss:0.00020275421437989099\n",
            "train loss:0.00012549716673827889\n",
            "train loss:0.00010377375166797851\n",
            "train loss:5.2972380183126036e-05\n",
            "train loss:0.00011192771279559172\n",
            "train loss:8.382388257518917e-05\n",
            "train loss:9.105159892078842e-05\n",
            "=== epoch:187, train acc:0.99975, test acc:0.94 ===\n",
            "train loss:0.0001918142858918297\n",
            "train loss:4.524660843296019e-05\n",
            "train loss:4.575511824968661e-05\n",
            "train loss:0.00014920628250762632\n",
            "train loss:0.00010820362092340234\n",
            "train loss:7.880387710535e-05\n",
            "train loss:8.620821895538977e-05\n",
            "train loss:0.0007234714414812923\n",
            "train loss:8.92200290355216e-05\n",
            "train loss:0.0001337384358162712\n",
            "train loss:0.00018399729216072216\n",
            "train loss:8.118304596236057e-05\n",
            "train loss:9.572966695183956e-05\n",
            "train loss:0.0001582982631991211\n",
            "train loss:0.00014845225167824132\n",
            "train loss:0.0001229791906522156\n",
            "train loss:0.030533417533285374\n",
            "train loss:0.0001264417722882353\n",
            "train loss:0.00015096070208941257\n",
            "train loss:0.00022072021491050618\n",
            "train loss:0.00011948811465992579\n",
            "train loss:0.0001520421263445738\n",
            "train loss:0.0012503458717979896\n",
            "train loss:0.00014790225071212515\n",
            "train loss:0.0012395829732071474\n",
            "train loss:0.0001155502912918792\n",
            "train loss:0.00017210193168304644\n",
            "train loss:0.010657191312436289\n",
            "train loss:9.270087168498178e-05\n",
            "train loss:8.957052549706936e-05\n",
            "train loss:5.861784883557115e-05\n",
            "train loss:0.00017457957153781868\n",
            "train loss:0.00018251474024857717\n",
            "train loss:7.85372065238198e-05\n",
            "train loss:0.00011803303222516123\n",
            "train loss:8.471958220869201e-05\n",
            "train loss:5.1037901463874054e-05\n",
            "train loss:7.22507855629074e-05\n",
            "train loss:0.0001179729957080662\n",
            "train loss:5.082357925079559e-05\n",
            "train loss:0.00010790400303756798\n",
            "train loss:5.2606760369158436e-05\n",
            "train loss:4.3942767567332584e-05\n",
            "train loss:0.00010164071288232672\n",
            "train loss:7.48932419728102e-05\n",
            "train loss:0.0033543036971370953\n",
            "train loss:6.830274774832693e-05\n",
            "train loss:0.00010107910649396734\n",
            "train loss:0.0001325327617847464\n",
            "train loss:0.00011317138517863536\n",
            "train loss:0.00011647329576970261\n",
            "train loss:0.00021694366787197917\n",
            "train loss:0.0013062234433744286\n",
            "train loss:0.00013857066820230428\n",
            "train loss:5.878321194332369e-05\n",
            "train loss:7.95832347860466e-05\n",
            "train loss:0.0002950034092710756\n",
            "train loss:4.4215268737728064e-05\n",
            "train loss:7.530789465998416e-05\n",
            "train loss:7.559436646925474e-05\n",
            "train loss:0.0001162628354923617\n",
            "train loss:7.670086477457174e-05\n",
            "train loss:6.593290049895942e-05\n",
            "train loss:4.946452228313891e-05\n",
            "train loss:8.14284368153165e-05\n",
            "train loss:0.0001362392633901511\n",
            "train loss:6.665190352926872e-05\n",
            "train loss:9.31276167445736e-05\n",
            "train loss:8.844008078991606e-05\n",
            "train loss:7.931362510360533e-05\n",
            "train loss:0.00019693895191396399\n",
            "train loss:0.0001893885124879089\n",
            "train loss:9.905487884922929e-05\n",
            "train loss:0.0001365064514981809\n",
            "train loss:6.959736603165364e-05\n",
            "train loss:0.0001046991690997124\n",
            "train loss:0.0002915830406001738\n",
            "train loss:0.00018908318790394007\n",
            "train loss:8.079714088631281e-05\n",
            "train loss:0.0003004632874412952\n",
            "=== epoch:188, train acc:1.0, test acc:0.943 ===\n",
            "train loss:0.00027767500784672925\n",
            "train loss:8.114310803424677e-05\n",
            "train loss:9.464789629879974e-05\n",
            "train loss:8.852072786011403e-05\n",
            "train loss:7.022357894988812e-05\n",
            "train loss:0.00012993828259436326\n",
            "train loss:7.101452753793657e-05\n",
            "train loss:6.884964372046718e-05\n",
            "train loss:9.172971437042756e-05\n",
            "train loss:5.866726767562847e-05\n",
            "train loss:8.811595007238337e-05\n",
            "train loss:5.3374210373386125e-05\n",
            "train loss:0.00011951461169482722\n",
            "train loss:0.00011686566944811638\n",
            "train loss:9.263231639052793e-05\n",
            "train loss:5.486947436396405e-05\n",
            "train loss:0.00011368703361634877\n",
            "train loss:8.345089354132464e-05\n",
            "train loss:6.394468860212847e-05\n",
            "train loss:7.374452710808573e-05\n",
            "train loss:0.00011064715715126624\n",
            "train loss:0.0001454122113785379\n",
            "train loss:6.774234974713223e-05\n",
            "train loss:8.899256379355868e-05\n",
            "train loss:9.909095477957727e-05\n",
            "train loss:0.0001847953835346232\n",
            "train loss:0.0001383094071766788\n",
            "train loss:0.00011430946095532288\n",
            "train loss:0.00011226919500220651\n",
            "train loss:5.519397902150217e-05\n",
            "train loss:8.12983458577819e-05\n",
            "train loss:7.638055663254343e-05\n",
            "train loss:0.00017883208598251118\n",
            "train loss:8.64758462283041e-05\n",
            "train loss:4.68356354014139e-05\n",
            "train loss:6.462259142804554e-05\n",
            "train loss:7.028065746758878e-05\n",
            "train loss:6.91910682748231e-05\n",
            "train loss:8.116501075094091e-05\n",
            "train loss:6.367900956792485e-05\n",
            "train loss:5.8080937137179374e-05\n",
            "train loss:8.605486439322604e-05\n",
            "train loss:5.45640137600605e-05\n",
            "train loss:7.259993835533778e-05\n",
            "train loss:5.234773455459413e-05\n",
            "train loss:4.8382916460830705e-05\n",
            "train loss:4.8096090097829996e-05\n",
            "train loss:6.664418363255951e-05\n",
            "train loss:4.1674442802831526e-05\n",
            "train loss:4.9922598967835624e-05\n",
            "train loss:5.285489586286886e-05\n",
            "train loss:9.790059571243737e-05\n",
            "train loss:7.582826974867409e-05\n",
            "train loss:7.851281294042455e-05\n",
            "train loss:8.925156853568129e-05\n",
            "train loss:0.0001262333082850147\n",
            "train loss:7.132981143777817e-05\n",
            "train loss:4.1725018012318705e-05\n",
            "train loss:7.129686577914144e-05\n",
            "train loss:6.168743623809082e-05\n",
            "train loss:6.279735433014125e-05\n",
            "train loss:9.29322924351689e-05\n",
            "train loss:6.145279667942982e-05\n",
            "train loss:5.374121724847492e-05\n",
            "train loss:4.9102199532957545e-05\n",
            "train loss:6.63616819567918e-05\n",
            "train loss:5.760406755109506e-05\n",
            "train loss:6.204726819628588e-05\n",
            "train loss:5.8121449690974983e-05\n",
            "train loss:5.490853055828896e-05\n",
            "train loss:6.974371590738445e-05\n",
            "train loss:6.315780061497482e-05\n",
            "train loss:9.600692247786595e-05\n",
            "train loss:9.221223475854354e-05\n",
            "train loss:0.00022010353804406137\n",
            "train loss:8.661251770472532e-05\n",
            "train loss:8.796071457810015e-05\n",
            "train loss:7.541574834279739e-05\n",
            "train loss:0.00014980027692693625\n",
            "train loss:0.00010761194251281776\n",
            "=== epoch:189, train acc:1.0, test acc:0.941 ===\n",
            "train loss:7.981733209172521e-05\n",
            "train loss:6.191419156554535e-05\n",
            "train loss:9.9340222233894e-05\n",
            "train loss:0.00011340318776476662\n",
            "train loss:8.409062618251272e-05\n",
            "train loss:8.550676296542308e-05\n",
            "train loss:7.644544786476024e-05\n",
            "train loss:8.287388310822854e-05\n",
            "train loss:0.00021460658771897876\n",
            "train loss:0.00010512020172371237\n",
            "train loss:7.286000168381635e-05\n",
            "train loss:0.00018923458038013112\n",
            "train loss:8.611379947118849e-05\n",
            "train loss:7.544393291376558e-05\n",
            "train loss:7.951960155110344e-05\n",
            "train loss:0.00018178955861752346\n",
            "train loss:0.00013352806810171172\n",
            "train loss:0.0001448724917626306\n",
            "train loss:7.903972731657849e-05\n",
            "train loss:7.33009085630754e-05\n",
            "train loss:0.00020011657597085526\n",
            "train loss:0.0002138595573300271\n",
            "train loss:0.00011562849235307771\n",
            "train loss:0.00015627715366073116\n",
            "train loss:0.00010914738401848844\n",
            "train loss:0.00011246910259358004\n",
            "train loss:8.382321322820227e-05\n",
            "train loss:9.914443794020016e-05\n",
            "train loss:0.00010265829512950445\n",
            "train loss:0.00012257863632409127\n",
            "train loss:9.599757812448438e-05\n",
            "train loss:6.853745619677564e-05\n",
            "train loss:9.960695605080899e-05\n",
            "train loss:0.00021268188241988257\n",
            "train loss:0.00019200239791280046\n",
            "train loss:6.344173993533287e-05\n",
            "train loss:8.4612888358282e-05\n",
            "train loss:7.633577820287079e-05\n",
            "train loss:8.935721997457777e-05\n",
            "train loss:0.00012063696342543606\n",
            "train loss:0.0001192092042819997\n",
            "train loss:0.0001173876376468414\n",
            "train loss:0.00013116153631136367\n",
            "train loss:0.00011739395928867186\n",
            "train loss:0.00013622095571154028\n",
            "train loss:6.311720295035902e-05\n",
            "train loss:7.45729889235955e-05\n",
            "train loss:8.297185636547674e-05\n",
            "train loss:8.915671911623444e-05\n",
            "train loss:0.00013326058530406515\n",
            "train loss:0.0001123465747827716\n",
            "train loss:7.303727850057019e-05\n",
            "train loss:7.190087039966364e-05\n",
            "train loss:8.180789437775683e-05\n",
            "train loss:9.197578538805788e-05\n",
            "train loss:0.00014753597027162806\n",
            "train loss:0.00014791815210894854\n",
            "train loss:0.00017775455423555173\n",
            "train loss:0.00013859091677529393\n",
            "train loss:0.000155196212003744\n",
            "train loss:0.0003331338590043903\n",
            "train loss:9.940516065029756e-05\n",
            "train loss:8.136001669544996e-05\n",
            "train loss:0.0001467341823217048\n",
            "train loss:0.00017793645916793307\n",
            "train loss:0.0001805301405505277\n",
            "train loss:9.609326124443901e-05\n",
            "train loss:0.00011252465501147125\n",
            "train loss:6.091938176845037e-05\n",
            "train loss:9.341239578605132e-05\n",
            "train loss:0.0011859628405132307\n",
            "train loss:6.982202798647724e-05\n",
            "train loss:8.605842029966342e-05\n",
            "train loss:7.938422190346018e-05\n",
            "train loss:5.361298190625193e-05\n",
            "train loss:8.908257112232504e-05\n",
            "train loss:0.00017985866479901583\n",
            "train loss:0.00011337810448968767\n",
            "train loss:0.00011698453637815075\n",
            "train loss:9.713496101772344e-05\n",
            "=== epoch:190, train acc:1.0, test acc:0.943 ===\n",
            "train loss:8.229085573150306e-05\n",
            "train loss:8.099335537698777e-05\n",
            "train loss:0.0001007686054231344\n",
            "train loss:0.00011368185882156459\n",
            "train loss:7.808034370094105e-05\n",
            "train loss:5.880249765288763e-05\n",
            "train loss:0.00011129891749306507\n",
            "train loss:0.00012131466253832544\n",
            "train loss:8.710631726788837e-05\n",
            "train loss:8.498363871968818e-05\n",
            "train loss:0.00013529847207832546\n",
            "train loss:0.00013853495610584112\n",
            "train loss:8.376365476525198e-05\n",
            "train loss:0.0001949830331535552\n",
            "train loss:6.776233138029046e-05\n",
            "train loss:9.074593099811507e-05\n",
            "train loss:9.027577556660704e-05\n",
            "train loss:0.00012179007356002985\n",
            "train loss:0.00011881746117303037\n",
            "train loss:7.897178789462049e-05\n",
            "train loss:0.00027386129412417204\n",
            "train loss:8.732969652356183e-05\n",
            "train loss:8.301911594747979e-05\n",
            "train loss:6.444216797724261e-05\n",
            "train loss:0.00010076113817174377\n",
            "train loss:6.714044261471964e-05\n",
            "train loss:5.569379109083426e-05\n",
            "train loss:6.744242099584167e-05\n",
            "train loss:6.349951653626784e-05\n",
            "train loss:7.56619609352069e-05\n",
            "train loss:5.6382430058917714e-05\n",
            "train loss:0.00010574105604419445\n",
            "train loss:4.0399046719006014e-05\n",
            "train loss:5.778319050184766e-05\n",
            "train loss:6.65109719246473e-05\n",
            "train loss:0.00010055694158163054\n",
            "train loss:6.520024909971094e-05\n",
            "train loss:0.00010722234797085388\n",
            "train loss:6.765381675543883e-05\n",
            "train loss:4.030732405347486e-05\n",
            "train loss:6.142381811962298e-05\n",
            "train loss:6.138508562974315e-05\n",
            "train loss:8.187627755295358e-05\n",
            "train loss:0.00047953210499833714\n",
            "train loss:5.5586330955764814e-05\n",
            "train loss:0.0005584674653027875\n",
            "train loss:3.8042159613792804e-05\n",
            "train loss:6.379024825482063e-05\n",
            "train loss:9.837051977135041e-05\n",
            "train loss:6.616889055166608e-05\n",
            "train loss:0.0001475702838382505\n",
            "train loss:4.874246553831863e-05\n",
            "train loss:0.0001212242082024455\n",
            "train loss:5.61902557745819e-05\n",
            "train loss:0.00013357267449403882\n",
            "train loss:6.06210460017133e-05\n",
            "train loss:7.284122111390498e-05\n",
            "train loss:0.00013940596664226826\n",
            "train loss:0.00010071438667105631\n",
            "train loss:6.150722872104192e-05\n",
            "train loss:5.869914708846527e-05\n",
            "train loss:0.00019765076639081472\n",
            "train loss:9.875943826553169e-05\n",
            "train loss:0.0002060530229171415\n",
            "train loss:0.00011072187434305691\n",
            "train loss:7.543172188951067e-05\n",
            "train loss:8.593200695405859e-05\n",
            "train loss:6.929228965422573e-05\n",
            "train loss:9.114367444046549e-05\n",
            "train loss:5.398657266397128e-05\n",
            "train loss:0.00011941471141750935\n",
            "train loss:0.00010550472973327181\n",
            "train loss:0.00011056289007538157\n",
            "train loss:6.820014851141517e-05\n",
            "train loss:8.019430327830439e-05\n",
            "train loss:3.424380980555323e-05\n",
            "train loss:3.8125516094614004e-05\n",
            "train loss:8.404000714836734e-05\n",
            "train loss:5.08029335798144e-05\n",
            "train loss:9.544016570504267e-05\n",
            "=== epoch:191, train acc:1.0, test acc:0.9405 ===\n",
            "train loss:6.218606287731969e-05\n",
            "train loss:9.43822235699672e-05\n",
            "train loss:6.641013211031797e-05\n",
            "train loss:9.195882171783095e-05\n",
            "train loss:8.711051582233046e-05\n",
            "train loss:7.452079094851645e-05\n",
            "train loss:8.77680115880917e-05\n",
            "train loss:8.695089516689574e-05\n",
            "train loss:9.8857296064922e-05\n",
            "train loss:5.726285589369621e-05\n",
            "train loss:5.022004960594534e-05\n",
            "train loss:9.427801414265299e-05\n",
            "train loss:7.5179510287545e-05\n",
            "train loss:8.410078478159588e-05\n",
            "train loss:4.6641243265897026e-05\n",
            "train loss:7.585037933863934e-05\n",
            "train loss:4.709257385713179e-05\n",
            "train loss:7.529304016908492e-05\n",
            "train loss:5.1494360264467705e-05\n",
            "train loss:7.695327315089781e-05\n",
            "train loss:9.83636360329196e-05\n",
            "train loss:5.801351115687631e-05\n",
            "train loss:5.866975693464424e-05\n",
            "train loss:5.808258425180623e-05\n",
            "train loss:8.125219619666406e-05\n",
            "train loss:8.819875824969446e-05\n",
            "train loss:5.499823301785978e-05\n",
            "train loss:5.1106414321748434e-05\n",
            "train loss:8.301100389068413e-05\n",
            "train loss:5.8456365209071166e-05\n",
            "train loss:6.858905287649474e-05\n",
            "train loss:3.865011023276374e-05\n",
            "train loss:7.037209213726481e-05\n",
            "train loss:7.699447934322278e-05\n",
            "train loss:5.776292972072671e-05\n",
            "train loss:8.674229350550441e-05\n",
            "train loss:8.5843026093273e-05\n",
            "train loss:0.00010026627578692074\n",
            "train loss:6.291291586855337e-05\n",
            "train loss:6.21413163074489e-05\n",
            "train loss:6.405759795041104e-05\n",
            "train loss:9.564947341677175e-05\n",
            "train loss:5.458738575847043e-05\n",
            "train loss:7.692802354505515e-05\n",
            "train loss:6.431884656777487e-05\n",
            "train loss:6.265350986650199e-05\n",
            "train loss:6.604853736808672e-05\n",
            "train loss:0.00010445200437300647\n",
            "train loss:4.9855553832519765e-05\n",
            "train loss:6.241329138347667e-05\n",
            "train loss:9.030539627434713e-05\n",
            "train loss:6.247143054738594e-05\n",
            "train loss:5.70285457113837e-05\n",
            "train loss:8.881208419434518e-05\n",
            "train loss:0.0001651532252985745\n",
            "train loss:4.915106953879683e-05\n",
            "train loss:6.491905617709482e-05\n",
            "train loss:5.120084259949301e-05\n",
            "train loss:8.023406480135285e-05\n",
            "train loss:5.527594288737099e-05\n",
            "train loss:0.00010796570596345928\n",
            "train loss:8.807247606271877e-05\n",
            "train loss:0.00012237186542501935\n",
            "train loss:0.00010954460244062243\n",
            "train loss:3.720666048003295e-05\n",
            "train loss:7.750766923122378e-05\n",
            "train loss:9.604183893075279e-05\n",
            "train loss:5.888662831508008e-05\n",
            "train loss:4.7918055425051475e-05\n",
            "train loss:0.0001539932754291362\n",
            "train loss:9.810860120256184e-05\n",
            "train loss:0.00021409897359363778\n",
            "train loss:6.980974136640259e-05\n",
            "train loss:6.943198872504454e-05\n",
            "train loss:8.499319183432748e-05\n",
            "train loss:4.3839927824999995e-05\n",
            "train loss:5.6274430042255804e-05\n",
            "train loss:3.9263981943343016e-05\n",
            "train loss:9.76188607533152e-05\n",
            "train loss:4.3418046193997775e-05\n",
            "=== epoch:192, train acc:1.0, test acc:0.94 ===\n",
            "train loss:5.801344052431833e-05\n",
            "train loss:8.682743121233799e-05\n",
            "train loss:4.576805656229178e-05\n",
            "train loss:9.630359745327078e-05\n",
            "train loss:0.0001018823430338885\n",
            "train loss:8.362299233019361e-05\n",
            "train loss:5.827383271589012e-05\n",
            "train loss:4.067667166402236e-05\n",
            "train loss:4.009025685425988e-05\n",
            "train loss:0.00010591366519106071\n",
            "train loss:0.0015407504928146593\n",
            "train loss:0.0001703052836639684\n",
            "train loss:5.764742178850234e-05\n",
            "train loss:7.401885307768492e-05\n",
            "train loss:0.00021461537546866867\n",
            "train loss:3.898464667288643e-05\n",
            "train loss:5.144594768689431e-05\n",
            "train loss:4.863164965362847e-05\n",
            "train loss:9.257131787630707e-05\n",
            "train loss:7.418101477271543e-05\n",
            "train loss:0.00010306386098494663\n",
            "train loss:5.99621067432784e-05\n",
            "train loss:5.388121560918299e-05\n",
            "train loss:6.395742478215922e-05\n",
            "train loss:6.252344453892928e-05\n",
            "train loss:4.9127502021391105e-05\n",
            "train loss:6.770857710935909e-05\n",
            "train loss:5.963270126101645e-05\n",
            "train loss:6.51809375956966e-05\n",
            "train loss:0.00014387377581295398\n",
            "train loss:0.00014754312027882632\n",
            "train loss:4.659476104434315e-05\n",
            "train loss:9.920848695867747e-05\n",
            "train loss:7.394132402784033e-05\n",
            "train loss:9.443436128807941e-05\n",
            "train loss:0.00010862488707339088\n",
            "train loss:0.00010381320187709355\n",
            "train loss:3.9708732206711475e-05\n",
            "train loss:7.275355983987083e-05\n",
            "train loss:5.963148543732951e-05\n",
            "train loss:5.877049415604117e-05\n",
            "train loss:8.525168694066803e-05\n",
            "train loss:6.456812164154808e-05\n",
            "train loss:0.000144120317861161\n",
            "train loss:8.659978549878204e-05\n",
            "train loss:9.60487882708533e-05\n",
            "train loss:8.046302468781804e-05\n",
            "train loss:6.341897727213177e-05\n",
            "train loss:9.043633518572604e-05\n",
            "train loss:7.458876714518418e-05\n",
            "train loss:7.315252855754152e-05\n",
            "train loss:4.5129590052498135e-05\n",
            "train loss:8.348590965813722e-05\n",
            "train loss:8.464907236643572e-05\n",
            "train loss:0.0001054398950225995\n",
            "train loss:8.068687426523449e-05\n",
            "train loss:6.63185532155984e-05\n",
            "train loss:9.006296074951057e-05\n",
            "train loss:5.687675708176688e-05\n",
            "train loss:0.00011988927280445667\n",
            "train loss:8.013592247470703e-05\n",
            "train loss:8.350955224272733e-05\n",
            "train loss:7.414302602320284e-05\n",
            "train loss:6.812580744376073e-05\n",
            "train loss:6.48629800182044e-05\n",
            "train loss:5.3234035570824436e-05\n",
            "train loss:5.469260564093649e-05\n",
            "train loss:6.882493570598187e-05\n",
            "train loss:7.226518226551561e-05\n",
            "train loss:8.141646708037192e-05\n",
            "train loss:0.00010065717012212393\n",
            "train loss:6.722192371287454e-05\n",
            "train loss:0.00012437613907633128\n",
            "train loss:0.00012475291351098276\n",
            "train loss:6.125901312330392e-05\n",
            "train loss:0.00013963431623519756\n",
            "train loss:5.3215257015269084e-05\n",
            "train loss:0.00012635839790703618\n",
            "train loss:8.712904696829275e-05\n",
            "train loss:5.563322191255307e-05\n",
            "=== epoch:193, train acc:1.0, test acc:0.944 ===\n",
            "train loss:5.5211873801489665e-05\n",
            "train loss:9.421590864666765e-05\n",
            "train loss:0.00010475265205594962\n",
            "train loss:6.726911391525888e-05\n",
            "train loss:0.00014923372768786518\n",
            "train loss:8.253933379955746e-05\n",
            "train loss:5.95692579073248e-05\n",
            "train loss:0.00011101488234713477\n",
            "train loss:0.00021930136253692765\n",
            "train loss:8.578616168725437e-05\n",
            "train loss:7.389224826749629e-05\n",
            "train loss:5.7966183368335897e-05\n",
            "train loss:5.8907612685641056e-05\n",
            "train loss:6.916018954710633e-05\n",
            "train loss:8.844178607714497e-05\n",
            "train loss:0.00013888271397095492\n",
            "train loss:0.00011530755971866138\n",
            "train loss:0.00014020438081285627\n",
            "train loss:7.983541506470187e-05\n",
            "train loss:9.334248971462531e-05\n",
            "train loss:4.825646146435724e-05\n",
            "train loss:5.04898426176512e-05\n",
            "train loss:0.0001428959427571542\n",
            "train loss:0.00011767767439660633\n",
            "train loss:8.918803886680927e-05\n",
            "train loss:0.0001277563216694461\n",
            "train loss:7.477267137837699e-05\n",
            "train loss:6.075124692532383e-05\n",
            "train loss:9.756116178989964e-05\n",
            "train loss:6.064878259467765e-05\n",
            "train loss:0.00010141002641160479\n",
            "train loss:0.00010481803915274193\n",
            "train loss:5.238678239272569e-05\n",
            "train loss:0.00012169401125133524\n",
            "train loss:7.656758159927295e-05\n",
            "train loss:7.266417944877854e-05\n",
            "train loss:7.596121013387535e-05\n",
            "train loss:8.346772790548447e-05\n",
            "train loss:6.801265814411978e-05\n",
            "train loss:0.0001740303846790467\n",
            "train loss:0.00013675849185132647\n",
            "train loss:0.0001850274577752203\n",
            "train loss:0.00010306669726250845\n",
            "train loss:9.03307042380686e-05\n",
            "train loss:0.00015104179042492924\n",
            "train loss:6.626522116362706e-05\n",
            "train loss:4.5724231797582835e-05\n",
            "train loss:0.00013503949769375878\n",
            "train loss:0.00010573586296178057\n",
            "train loss:7.577726927589091e-05\n",
            "train loss:0.00015645110976487072\n",
            "train loss:7.712728026447779e-05\n",
            "train loss:9.75120489954487e-05\n",
            "train loss:8.69816496651773e-05\n",
            "train loss:8.111114117978226e-05\n",
            "train loss:5.098466980690273e-05\n",
            "train loss:7.124747754795981e-05\n",
            "train loss:0.00020756848886886153\n",
            "train loss:8.115737089732416e-05\n",
            "train loss:5.383508616219233e-05\n",
            "train loss:8.547480572081838e-05\n",
            "train loss:0.00010166234095136869\n",
            "train loss:0.0002233949087545623\n",
            "train loss:9.678015878102876e-05\n",
            "train loss:0.00013041856256410311\n",
            "train loss:0.00011014102627919485\n",
            "train loss:3.8235628542666484e-05\n",
            "train loss:4.4253398066831963e-05\n",
            "train loss:6.747326611771621e-05\n",
            "train loss:0.00011710013967402698\n",
            "train loss:8.44826753693843e-05\n",
            "train loss:4.158875301975966e-05\n",
            "train loss:0.00025454453078651933\n",
            "train loss:9.854743242444699e-05\n",
            "train loss:9.360351284010498e-05\n",
            "train loss:9.964684936558943e-05\n",
            "train loss:7.014995279054501e-05\n",
            "train loss:9.363641306206172e-05\n",
            "train loss:0.00029996984215992\n",
            "train loss:5.2473100584906415e-05\n",
            "=== epoch:194, train acc:1.0, test acc:0.944 ===\n",
            "train loss:7.620857278910006e-05\n",
            "train loss:0.00012882339372369072\n",
            "train loss:5.6597805466783515e-05\n",
            "train loss:0.00010715921202846058\n",
            "train loss:5.627044648297105e-05\n",
            "train loss:7.62862550912049e-05\n",
            "train loss:0.00011984854982263732\n",
            "train loss:5.405309287859345e-05\n",
            "train loss:5.031637378793939e-05\n",
            "train loss:8.930215109365161e-05\n",
            "train loss:8.405151297350174e-05\n",
            "train loss:4.463798962069185e-05\n",
            "train loss:8.456837031628421e-05\n",
            "train loss:7.444185214981023e-05\n",
            "train loss:0.00010054415169959808\n",
            "train loss:5.911040675011541e-05\n",
            "train loss:6.070551176095955e-05\n",
            "train loss:9.502490784990455e-05\n",
            "train loss:7.793781808909643e-05\n",
            "train loss:7.412592147369936e-05\n",
            "train loss:7.996000351497978e-05\n",
            "train loss:5.954466887373763e-05\n",
            "train loss:8.937214102944142e-05\n",
            "train loss:9.12857797649705e-05\n",
            "train loss:0.00011560945543929703\n",
            "train loss:0.00036534959009281635\n",
            "train loss:9.824218675192486e-05\n",
            "train loss:4.966648190349224e-05\n",
            "train loss:9.367042156404805e-05\n",
            "train loss:9.189038378021165e-05\n",
            "train loss:9.225380139211287e-05\n",
            "train loss:0.00010334340287633199\n",
            "train loss:8.355920271240043e-05\n",
            "train loss:6.753417559676218e-05\n",
            "train loss:8.357078957597603e-05\n",
            "train loss:6.644579953398922e-05\n",
            "train loss:8.114688723809038e-05\n",
            "train loss:6.609315656113289e-05\n",
            "train loss:0.00018442250093492606\n",
            "train loss:5.7180325013341e-05\n",
            "train loss:0.00011998268383340786\n",
            "train loss:0.00020173250181251525\n",
            "train loss:8.363114911410751e-05\n",
            "train loss:0.00010763308109333072\n",
            "train loss:9.6192859570444e-05\n",
            "train loss:7.411744621970059e-05\n",
            "train loss:8.202597639529846e-05\n",
            "train loss:0.00017563497188761987\n",
            "train loss:0.00013568840417685802\n",
            "train loss:0.0001244436684580014\n",
            "train loss:0.00014646460189400508\n",
            "train loss:0.00014847210314876134\n",
            "train loss:9.524045289795485e-05\n",
            "train loss:0.00010919919086915946\n",
            "train loss:7.124168509946948e-05\n",
            "train loss:0.0002188702229050042\n",
            "train loss:0.00011867223976551729\n",
            "train loss:9.274015977053634e-05\n",
            "train loss:8.05798706066263e-05\n",
            "train loss:7.669226376039814e-05\n",
            "train loss:0.00020339270263976673\n",
            "train loss:9.061167505667541e-05\n",
            "train loss:7.144466916689861e-05\n",
            "train loss:8.177161879729292e-05\n",
            "train loss:0.00012287330759128799\n",
            "train loss:7.426796341711709e-05\n",
            "train loss:0.00010981911898934251\n",
            "train loss:6.568649134817406e-05\n",
            "train loss:0.00013743077337932067\n",
            "train loss:0.00018910569660833554\n",
            "train loss:7.660862931975436e-05\n",
            "train loss:0.00011321349527206337\n",
            "train loss:0.00011163900936909516\n",
            "train loss:0.00011272311567103393\n",
            "train loss:6.803140836029487e-05\n",
            "train loss:0.0001459950199838353\n",
            "train loss:0.00024780549170593\n",
            "train loss:0.00010633859942950406\n",
            "train loss:0.00013173000236793575\n",
            "train loss:9.395134380872869e-05\n",
            "=== epoch:195, train acc:1.0, test acc:0.9455 ===\n",
            "train loss:9.597135035381406e-05\n",
            "train loss:0.00011383795134293584\n",
            "train loss:9.924030923240096e-05\n",
            "train loss:0.00012353027848540315\n",
            "train loss:0.00021114971711472572\n",
            "train loss:0.00022096092701625748\n",
            "train loss:9.580926084441534e-05\n",
            "train loss:6.766031414674329e-05\n",
            "train loss:7.304264893675052e-05\n",
            "train loss:9.802106838688252e-05\n",
            "train loss:3.752468436233573e-05\n",
            "train loss:7.428157976709309e-05\n",
            "train loss:9.798340671921666e-05\n",
            "train loss:5.857892974286282e-05\n",
            "train loss:5.9720884201735176e-05\n",
            "train loss:8.896361734937074e-05\n",
            "train loss:6.053626242638452e-05\n",
            "train loss:0.00014264415556890418\n",
            "train loss:0.00010146341326712911\n",
            "train loss:7.789624761274488e-05\n",
            "train loss:5.591187700472062e-05\n",
            "train loss:6.44515802712008e-05\n",
            "train loss:7.934263238217047e-05\n",
            "train loss:4.5192821071851294e-05\n",
            "train loss:4.3038363314900846e-05\n",
            "train loss:3.8612293038837305e-05\n",
            "train loss:6.828009401329349e-05\n",
            "train loss:5.8017886734547725e-05\n",
            "train loss:6.0592792026117555e-05\n",
            "train loss:6.016136989739704e-05\n",
            "train loss:6.0171886371988336e-05\n",
            "train loss:5.054194128598438e-05\n",
            "train loss:0.00013683555682040004\n",
            "train loss:5.3406324429165495e-05\n",
            "train loss:0.00013673919954503293\n",
            "train loss:9.506986343003501e-05\n",
            "train loss:5.9228958262528365e-05\n",
            "train loss:0.00014213400621180806\n",
            "train loss:9.918422143450106e-05\n",
            "train loss:5.615688683208606e-05\n",
            "train loss:7.521518873294692e-05\n",
            "train loss:7.728015698889877e-05\n",
            "train loss:4.387267486371245e-05\n",
            "train loss:7.977016641745399e-05\n",
            "train loss:8.595442850082554e-05\n",
            "train loss:7.285943045240109e-05\n",
            "train loss:4.817117908344462e-05\n",
            "train loss:0.00020087214206906484\n",
            "train loss:7.967394628943302e-05\n",
            "train loss:0.00014567317144205155\n",
            "train loss:5.125313247990458e-05\n",
            "train loss:0.00011103736964487222\n",
            "train loss:8.706289385093168e-05\n",
            "train loss:5.233678791253209e-05\n",
            "train loss:5.413472087065169e-05\n",
            "train loss:9.129213734002154e-05\n",
            "train loss:0.00185443563345955\n",
            "train loss:6.779236259821325e-05\n",
            "train loss:0.00011105075336359625\n",
            "train loss:0.00019977163854700342\n",
            "train loss:9.018856529955295e-05\n",
            "train loss:0.0003668516416061191\n",
            "train loss:0.0001144042905362306\n",
            "train loss:0.00013544949399742033\n",
            "train loss:0.00014458543181835687\n",
            "train loss:0.00013248288214392497\n",
            "train loss:0.00010373536130449706\n",
            "train loss:0.00013755440015867637\n",
            "train loss:0.00023845639746116008\n",
            "train loss:0.0002744716284302181\n",
            "train loss:0.00015126912165781077\n",
            "train loss:0.00016065089841501169\n",
            "train loss:4.66127921835934e-05\n",
            "train loss:0.00012134759026753272\n",
            "train loss:0.00012964571753809544\n",
            "train loss:6.678693412956922e-05\n",
            "train loss:0.0001131340203379111\n",
            "train loss:0.0001961920975740052\n",
            "train loss:0.000115480477659916\n",
            "train loss:0.002592719053069238\n",
            "=== epoch:196, train acc:0.999875, test acc:0.9425 ===\n",
            "train loss:0.0011450566932092743\n",
            "train loss:0.00013986290239331134\n",
            "train loss:0.00010965312321443115\n",
            "train loss:9.769471844089887e-05\n",
            "train loss:7.091959420686271e-05\n",
            "train loss:0.0001328918997792321\n",
            "train loss:0.00010192065107674357\n",
            "train loss:0.00012621717883838074\n",
            "train loss:6.559122000110486e-05\n",
            "train loss:0.00010526680347115665\n",
            "train loss:0.00010813313885747465\n",
            "train loss:8.738953939703283e-05\n",
            "train loss:0.00011239589350562769\n",
            "train loss:4.674638839413627e-05\n",
            "train loss:0.00022475672570045356\n",
            "train loss:0.0001611049779271634\n",
            "train loss:8.494318244895962e-05\n",
            "train loss:8.774087923506497e-05\n",
            "train loss:0.0005259189522392205\n",
            "train loss:9.816825616943195e-05\n",
            "train loss:0.00016151945691634026\n",
            "train loss:8.326176030452663e-05\n",
            "train loss:0.00020247835712622567\n",
            "train loss:8.1525491958204e-05\n",
            "train loss:0.0001505938349423826\n",
            "train loss:8.315200945281432e-05\n",
            "train loss:0.0001641559596451513\n",
            "train loss:0.00018841880288957555\n",
            "train loss:0.00012983144268190186\n",
            "train loss:0.00018862004549431336\n",
            "train loss:0.00015600139602991715\n",
            "train loss:0.00020262199828142657\n",
            "train loss:0.00014517088665216572\n",
            "train loss:0.0005595459036183937\n",
            "train loss:0.00018991987175588269\n",
            "train loss:0.00012109956709871594\n",
            "train loss:0.0001375007428747865\n",
            "train loss:0.0002523719138765685\n",
            "train loss:0.00023662076186931272\n",
            "train loss:0.0002042208681691897\n",
            "train loss:0.000278552422353069\n",
            "train loss:0.00010934196967205039\n",
            "train loss:0.00010277656892615252\n",
            "train loss:0.0001712887523770448\n",
            "train loss:8.956314765900857e-05\n",
            "train loss:0.00010728156800479324\n",
            "train loss:0.0002053019619862881\n",
            "train loss:0.00015111318139833372\n",
            "train loss:0.0002092619917683584\n",
            "train loss:0.00016174506730608148\n",
            "train loss:8.752094240526112e-05\n",
            "train loss:7.292250858565276e-05\n",
            "train loss:0.00010860114558229792\n",
            "train loss:7.316621210180764e-05\n",
            "train loss:7.558187771373625e-05\n",
            "train loss:8.398726117700997e-05\n",
            "train loss:0.00010719597294481723\n",
            "train loss:9.013169405756983e-05\n",
            "train loss:7.696295685021604e-05\n",
            "train loss:0.00021099750720786288\n",
            "train loss:0.00015030746478048997\n",
            "train loss:9.395758309005918e-05\n",
            "train loss:0.00011823761930642765\n",
            "train loss:7.865122146541465e-05\n",
            "train loss:0.00017739190699491127\n",
            "train loss:0.00017467312277011244\n",
            "train loss:0.000208061424456379\n",
            "train loss:0.00017876962139720912\n",
            "train loss:0.00012603410240752714\n",
            "train loss:0.00015818796105446958\n",
            "train loss:9.287546426346851e-05\n",
            "train loss:0.00012958441793806415\n",
            "train loss:6.62418995344885e-05\n",
            "train loss:8.480066231467553e-05\n",
            "train loss:0.00012542159728047895\n",
            "train loss:8.530832320355285e-05\n",
            "train loss:7.445137310336095e-05\n",
            "train loss:0.00010855714325756127\n",
            "train loss:7.362567553405719e-05\n",
            "train loss:7.645792463510065e-05\n",
            "=== epoch:197, train acc:1.0, test acc:0.9445 ===\n",
            "train loss:7.360072273295508e-05\n",
            "train loss:0.00011175186771596948\n",
            "train loss:7.815782939786952e-05\n",
            "train loss:8.642231358181083e-05\n",
            "train loss:0.00011322079358467001\n",
            "train loss:6.510718392841215e-05\n",
            "train loss:0.00015123619443814838\n",
            "train loss:8.354210657459537e-05\n",
            "train loss:9.950543828407807e-05\n",
            "train loss:9.689852162877469e-05\n",
            "train loss:5.0718338098661506e-05\n",
            "train loss:7.35587094983041e-05\n",
            "train loss:5.33405904554363e-05\n",
            "train loss:6.303552955691111e-05\n",
            "train loss:0.0001128398426646641\n",
            "train loss:0.00014907146151743055\n",
            "train loss:7.773808024634317e-05\n",
            "train loss:9.1174906482462e-05\n",
            "train loss:0.00020734293741358402\n",
            "train loss:6.561125861677592e-05\n",
            "train loss:0.0001611346819930826\n",
            "train loss:6.769426394909874e-05\n",
            "train loss:0.00012178600499585142\n",
            "train loss:3.7333186723936257e-05\n",
            "train loss:0.00011955590074406557\n",
            "train loss:6.129341914381494e-05\n",
            "train loss:9.987966200667865e-05\n",
            "train loss:6.718110804790978e-05\n",
            "train loss:0.00012093957784502316\n",
            "train loss:5.455765409498394e-05\n",
            "train loss:4.388787210289727e-05\n",
            "train loss:6.0481583435089255e-05\n",
            "train loss:7.242036975757071e-05\n",
            "train loss:0.00012605620688891143\n",
            "train loss:7.122840276489429e-05\n",
            "train loss:9.395688414787084e-05\n",
            "train loss:0.00014724776686485431\n",
            "train loss:0.00014136595365392757\n",
            "train loss:7.467331653854475e-05\n",
            "train loss:5.002691078439877e-05\n",
            "train loss:6.796158363633694e-05\n",
            "train loss:7.234651660285541e-05\n",
            "train loss:8.730019014878949e-05\n",
            "train loss:8.207708552010549e-05\n",
            "train loss:5.101822522728758e-05\n",
            "train loss:4.1508926244874104e-05\n",
            "train loss:0.0001358270551006701\n",
            "train loss:4.7858911453623704e-05\n",
            "train loss:6.585708892279509e-05\n",
            "train loss:0.00013658427920569676\n",
            "train loss:8.376527514676434e-05\n",
            "train loss:5.267478074429427e-05\n",
            "train loss:0.00012843174494385005\n",
            "train loss:0.00011332032520852663\n",
            "train loss:0.00011425555206980386\n",
            "train loss:6.530008717432981e-05\n",
            "train loss:0.0001495844496280538\n",
            "train loss:8.193050453543929e-05\n",
            "train loss:7.197978110854777e-05\n",
            "train loss:5.7904362876606307e-05\n",
            "train loss:7.01598943742585e-05\n",
            "train loss:9.236444734657314e-05\n",
            "train loss:5.302987776759321e-05\n",
            "train loss:6.038110599127576e-05\n",
            "train loss:6.808085710431527e-05\n",
            "train loss:6.119628473262069e-05\n",
            "train loss:4.6922755767206165e-05\n",
            "train loss:7.164935952953923e-05\n",
            "train loss:8.016774138773833e-05\n",
            "train loss:6.373452273293751e-05\n",
            "train loss:6.716022235361984e-05\n",
            "train loss:0.00013609550054956754\n",
            "train loss:4.8759671772786085e-05\n",
            "train loss:9.380389716054159e-05\n",
            "train loss:0.00014265956746713606\n",
            "train loss:0.0023527443296564535\n",
            "train loss:5.824880360370391e-05\n",
            "train loss:8.438927646509659e-05\n",
            "train loss:5.237698863027932e-05\n",
            "train loss:5.4406186874307214e-05\n",
            "=== epoch:198, train acc:1.0, test acc:0.9435 ===\n",
            "train loss:0.00010001643780857916\n",
            "train loss:0.00010599122492745156\n",
            "train loss:7.841767492862334e-05\n",
            "train loss:7.676548328040876e-05\n",
            "train loss:0.00010598101526416392\n",
            "train loss:7.900764594858001e-05\n",
            "train loss:6.83562526718354e-05\n",
            "train loss:0.00012248946100495287\n",
            "train loss:5.942991003224601e-05\n",
            "train loss:6.679609993206536e-05\n",
            "train loss:8.723954512168462e-05\n",
            "train loss:4.73065610327297e-05\n",
            "train loss:6.640867026945675e-05\n",
            "train loss:6.422349407975049e-05\n",
            "train loss:0.00010251452575478677\n",
            "train loss:7.376301330309848e-05\n",
            "train loss:5.406178095068903e-05\n",
            "train loss:0.0006172193534554983\n",
            "train loss:6.869661772203518e-05\n",
            "train loss:4.936097663661724e-05\n",
            "train loss:6.839447001282099e-05\n",
            "train loss:5.901038786266584e-05\n",
            "train loss:9.819988656687288e-05\n",
            "train loss:0.00010058544332932903\n",
            "train loss:6.914890310652704e-05\n",
            "train loss:4.8535111493429285e-05\n",
            "train loss:7.607703944701621e-05\n",
            "train loss:8.486332181639876e-05\n",
            "train loss:8.959231716505997e-05\n",
            "train loss:7.767572851136335e-05\n",
            "train loss:8.319310890434778e-05\n",
            "train loss:0.00010428020699274796\n",
            "train loss:0.001128670577390621\n",
            "train loss:7.088516736156059e-05\n",
            "train loss:4.606936366926385e-05\n",
            "train loss:6.185526921093951e-05\n",
            "train loss:4.304207237749244e-05\n",
            "train loss:9.22114945153864e-05\n",
            "train loss:7.326402571013756e-05\n",
            "train loss:7.928974855811719e-05\n",
            "train loss:6.91508564896288e-05\n",
            "train loss:5.9375363123269165e-05\n",
            "train loss:7.366974323620865e-05\n",
            "train loss:0.00012827549258390592\n",
            "train loss:8.687090013541529e-05\n",
            "train loss:4.770096431355176e-05\n",
            "train loss:5.273330240310571e-05\n",
            "train loss:7.047362746781786e-05\n",
            "train loss:8.305073441137183e-05\n",
            "train loss:7.236753535549197e-05\n",
            "train loss:6.461897936737988e-05\n",
            "train loss:0.00011802554771047534\n",
            "train loss:7.417678419994556e-05\n",
            "train loss:8.567363140698033e-05\n",
            "train loss:3.6588102856727864e-05\n",
            "train loss:6.741686059516015e-05\n",
            "train loss:6.304289162034346e-05\n",
            "train loss:8.187130603908069e-05\n",
            "train loss:4.356699724173453e-05\n",
            "train loss:8.042250214088554e-05\n",
            "train loss:5.4002110606563346e-05\n",
            "train loss:4.139649567552189e-05\n",
            "train loss:3.954772635678206e-05\n",
            "train loss:7.307980267471933e-05\n",
            "train loss:6.760238790347988e-05\n",
            "train loss:0.00010349088560762678\n",
            "train loss:8.240394263137926e-05\n",
            "train loss:6.39188164239227e-05\n",
            "train loss:5.6653440700670965e-05\n",
            "train loss:4.012634846885978e-05\n",
            "train loss:5.445479119219191e-05\n",
            "train loss:4.699913404992101e-05\n",
            "train loss:5.595709190656796e-05\n",
            "train loss:3.700628216909139e-05\n",
            "train loss:3.6409811441569194e-05\n",
            "train loss:3.262717576053141e-05\n",
            "train loss:4.420004648185618e-05\n",
            "train loss:4.1759822649619335e-05\n",
            "train loss:6.513569612319989e-05\n",
            "train loss:2.328672346049414e-05\n",
            "=== epoch:199, train acc:1.0, test acc:0.9435 ===\n",
            "train loss:3.9622167206044455e-05\n",
            "train loss:5.066065555653967e-05\n",
            "train loss:3.3960858386677385e-05\n",
            "train loss:5.6800191385485264e-05\n",
            "train loss:0.0003082596234754284\n",
            "train loss:5.368096125849585e-05\n",
            "train loss:3.600988829113813e-05\n",
            "train loss:4.144414713803009e-05\n",
            "train loss:5.477782032681171e-05\n",
            "train loss:3.397612912138996e-05\n",
            "train loss:6.356955914587783e-05\n",
            "train loss:8.374462688740382e-05\n",
            "train loss:6.719701701285305e-05\n",
            "train loss:6.516278904158305e-05\n",
            "train loss:9.85484966136227e-05\n",
            "train loss:5.985653625964297e-05\n",
            "train loss:6.136581365763757e-05\n",
            "train loss:4.3458880896362964e-05\n",
            "train loss:6.888003300988388e-05\n",
            "train loss:0.00010542862643351453\n",
            "train loss:5.39521189362171e-05\n",
            "train loss:6.0548547309853366e-05\n",
            "train loss:8.833024662744895e-05\n",
            "train loss:4.0721630733597396e-05\n",
            "train loss:0.0001155467011492726\n",
            "train loss:5.857463822627475e-05\n",
            "train loss:0.0006153442751370249\n",
            "train loss:5.590840473316204e-05\n",
            "train loss:8.20318602785282e-05\n",
            "train loss:4.8232810603642066e-05\n",
            "train loss:4.7865108720646774e-05\n",
            "train loss:5.176090286154362e-05\n",
            "train loss:7.85986092608886e-05\n",
            "train loss:9.208873103088065e-05\n",
            "train loss:4.741903866044312e-05\n",
            "train loss:6.212790993582421e-05\n",
            "train loss:7.602025220141024e-05\n",
            "train loss:4.591646803692917e-05\n",
            "train loss:5.328395043584487e-05\n",
            "train loss:5.036202948267943e-05\n",
            "train loss:3.6970482943559445e-05\n",
            "train loss:3.934671210502633e-05\n",
            "train loss:6.465041501534686e-05\n",
            "train loss:4.92473930042511e-05\n",
            "train loss:3.654136755212099e-05\n",
            "train loss:5.374269989669101e-05\n",
            "train loss:5.246207118987904e-05\n",
            "train loss:7.12248419737005e-05\n",
            "train loss:5.5248116553797955e-05\n",
            "train loss:5.595707719967608e-05\n",
            "train loss:5.383176986997721e-05\n",
            "train loss:3.064635381665615e-05\n",
            "train loss:8.17841604544301e-05\n",
            "train loss:4.184115569095515e-05\n",
            "train loss:5.952411650373338e-05\n",
            "train loss:0.00028672246928379953\n",
            "train loss:6.0254845923685113e-05\n",
            "train loss:4.567176090292082e-05\n",
            "train loss:4.297072894552971e-05\n",
            "train loss:4.36545556965095e-05\n",
            "train loss:0.0001111413506796251\n",
            "train loss:5.255865338793313e-05\n",
            "train loss:6.732874996132815e-05\n",
            "train loss:3.515952432759983e-05\n",
            "train loss:0.00010906206265090052\n",
            "train loss:6.539953167679074e-05\n",
            "train loss:7.100999159586614e-05\n",
            "train loss:8.040986019294142e-05\n",
            "train loss:6.084005296298556e-05\n",
            "train loss:8.181403730268939e-05\n",
            "train loss:8.58083331957552e-05\n",
            "train loss:5.408134878874622e-05\n",
            "train loss:9.513431289186043e-05\n",
            "train loss:0.00011723151119623418\n",
            "train loss:9.303369463094025e-05\n",
            "train loss:0.00012664075533856643\n",
            "train loss:9.241845005981352e-05\n",
            "train loss:6.0353337815959664e-05\n",
            "train loss:7.65600041877674e-05\n",
            "train loss:7.81619860116137e-05\n",
            "=== epoch:200, train acc:1.0, test acc:0.948 ===\n",
            "train loss:4.276265798633852e-05\n",
            "train loss:6.153651242678452e-05\n",
            "train loss:0.0019010993798849614\n",
            "train loss:4.504187564477357e-05\n",
            "train loss:7.562781973954127e-05\n",
            "train loss:6.836266608918548e-05\n",
            "train loss:6.378198426311635e-05\n",
            "train loss:9.72952810762617e-05\n",
            "train loss:7.8032157421766e-05\n",
            "train loss:0.00012980986819422353\n",
            "train loss:6.205511305652484e-05\n",
            "train loss:7.947684159096498e-05\n",
            "train loss:6.594822480053775e-05\n",
            "train loss:0.00021708374891236587\n",
            "train loss:0.000111904467310658\n",
            "train loss:8.564932700720883e-05\n",
            "train loss:8.005189618219953e-05\n",
            "train loss:8.152579490631966e-05\n",
            "train loss:9.072021398701279e-05\n",
            "train loss:8.118117644817772e-05\n",
            "train loss:7.629846022887578e-05\n",
            "train loss:7.599801244940994e-05\n",
            "train loss:9.672024117031264e-05\n",
            "train loss:8.160242412153423e-05\n",
            "train loss:7.48398973299932e-05\n",
            "train loss:8.875000872327475e-05\n",
            "train loss:6.727410468423706e-05\n",
            "train loss:0.00010991443075065924\n",
            "train loss:8.882835216961694e-05\n",
            "train loss:5.9024365555644595e-05\n",
            "train loss:0.00017583051209637873\n",
            "train loss:0.00019642371210744144\n",
            "train loss:0.0032742928523168367\n",
            "train loss:7.772404249680199e-05\n",
            "train loss:5.516456420326065e-05\n",
            "train loss:8.312146181230948e-05\n",
            "train loss:7.625236446790162e-05\n",
            "train loss:7.375792196197198e-05\n",
            "train loss:0.00015062142627735443\n",
            "train loss:9.731496236690425e-05\n",
            "train loss:0.0001075297160498645\n",
            "train loss:8.7583741610688e-05\n",
            "train loss:6.819146265183667e-05\n",
            "train loss:0.00019629928902471251\n",
            "train loss:6.24524052035984e-05\n",
            "train loss:9.21024286575187e-05\n",
            "train loss:7.394334376392673e-05\n",
            "train loss:0.00010101681112429735\n",
            "train loss:5.1566853994774156e-05\n",
            "train loss:0.00013111343137983393\n",
            "train loss:7.806304203804384e-05\n",
            "train loss:7.692356212604573e-05\n",
            "train loss:6.590937916236407e-05\n",
            "train loss:0.00013222377884973636\n",
            "train loss:0.00012693932299684814\n",
            "train loss:7.219547569763066e-05\n",
            "train loss:8.538310312124656e-05\n",
            "train loss:9.501371278442052e-05\n",
            "train loss:7.233075749665567e-05\n",
            "train loss:0.00017332044608290998\n",
            "train loss:4.706399065088722e-05\n",
            "train loss:9.737058481292645e-05\n",
            "train loss:0.00011934112529141732\n",
            "train loss:6.96957477421364e-05\n",
            "train loss:0.0001342776357531545\n",
            "train loss:5.6247178550481155e-05\n",
            "train loss:0.00018329596425926877\n",
            "train loss:9.79091957061175e-05\n",
            "train loss:8.350251167555585e-05\n",
            "train loss:0.00010984860118206177\n",
            "train loss:0.00015659563348573865\n",
            "train loss:9.610852816080255e-05\n",
            "train loss:0.00016773789396938492\n",
            "train loss:7.909501052018191e-05\n",
            "train loss:0.0001293236347446641\n",
            "train loss:0.00010457312110005777\n",
            "train loss:0.00012383411720853028\n",
            "train loss:9.362311420886317e-05\n",
            "train loss:9.757813185155017e-05\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.9445\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOAfP93TXhgz"
      },
      "source": [
        "# Convolutional Neural Network (CNN)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jylKowwOYlxN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2c3817-f93e-4c37-9058-5a7f295090c0"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = load_mnist(flatten=False)\r\n",
        "x_train = x_train[:800]\r\n",
        "t_train = t_train[:800]\r\n",
        "x_test=x_test[:200]\r\n",
        "t_test=t_test[:200]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Converting train-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting train-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-images-idx3-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Converting t10k-labels-idx1-ubyte.gz to NumPy Array ...\n",
            "Done\n",
            "Creating pickle file ...\n",
            "Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1LfYp82Xtoy"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9mjf_GAUXke-",
        "outputId": "0f33f59c-fb31-470a-a28b-705ebb55226a"
      },
      "source": [
        "network = DeepConvNet()  \r\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\r\n",
        "                  epochs=10, mini_batch_size=100,\r\n",
        "                  optimizer='Adam', optimizer_param={'lr':0.001}, verbose=True)\r\n",
        "trainer.train()\r\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\r\n",
        "\r\n",
        "markers = {'train': 'o', 'test': 's'}\r\n",
        "x = np.arange(len(train_acc_list))\r\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\r\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\r\n",
        "plt.xlabel(\"epochs\")\r\n",
        "plt.ylabel(\"accuracy\")\r\n",
        "plt.ylim(0, 1.0)\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.306328491560718\n",
            "=== epoch:1, train acc:0.1075, test acc:0.08 ===\n",
            "train loss:2.3088596806043102\n",
            "train loss:2.319756442775913\n",
            "train loss:2.330648641402162\n",
            "train loss:2.2801642404318674\n",
            "train loss:2.2882959146725135\n",
            "train loss:2.266215524983661\n",
            "train loss:2.25826115797134\n",
            "train loss:2.2555971611600185\n",
            "=== epoch:2, train acc:0.1675, test acc:0.195 ===\n",
            "train loss:2.2673314901091244\n",
            "train loss:2.2557301467453237\n",
            "train loss:2.2645722872244263\n",
            "train loss:2.254054846374546\n",
            "train loss:2.2426717269982235\n",
            "train loss:2.1911283859662416\n",
            "train loss:2.169033907031\n",
            "train loss:2.184768991360225\n",
            "=== epoch:3, train acc:0.4575, test acc:0.4 ===\n",
            "train loss:2.1403067937560345\n",
            "train loss:2.1437269103837973\n",
            "train loss:2.16831562409481\n",
            "train loss:1.959587975917247\n",
            "train loss:2.1236547379490287\n",
            "train loss:1.9915024701169752\n",
            "train loss:2.09275711041643\n",
            "train loss:2.0731744524946647\n",
            "=== epoch:4, train acc:0.62375, test acc:0.575 ===\n",
            "train loss:2.069778769055776\n",
            "train loss:1.9378841524585437\n",
            "train loss:1.9603180257470996\n",
            "train loss:1.7300465060564505\n",
            "train loss:1.8733331190350018\n",
            "train loss:1.8234972381965842\n",
            "train loss:1.8421737248316095\n",
            "train loss:1.72007685506094\n",
            "=== epoch:5, train acc:0.6925, test acc:0.625 ===\n",
            "train loss:1.771883991014167\n",
            "train loss:1.7685150475264906\n",
            "train loss:1.7201341207886716\n",
            "train loss:1.6840801117513136\n",
            "train loss:1.9358502097209618\n",
            "train loss:1.7163008172906742\n",
            "train loss:1.7090235284361421\n",
            "train loss:1.6533886590113205\n",
            "=== epoch:6, train acc:0.785, test acc:0.715 ===\n",
            "train loss:1.6222604464002073\n",
            "train loss:1.79124663701171\n",
            "train loss:1.684284511527689\n",
            "train loss:1.6841951728258662\n",
            "train loss:1.7730023632240386\n",
            "train loss:1.6508669921478754\n",
            "train loss:1.8183552616537435\n",
            "train loss:1.856996107251579\n",
            "=== epoch:7, train acc:0.8375, test acc:0.785 ===\n",
            "train loss:1.8924100336564096\n",
            "train loss:1.7753505567201815\n",
            "train loss:1.6327330081571139\n",
            "train loss:1.6613707665235609\n",
            "train loss:1.583153343060432\n",
            "train loss:1.6939888205088593\n",
            "train loss:1.6007205973848884\n",
            "train loss:1.6696628952852202\n",
            "=== epoch:8, train acc:0.84, test acc:0.84 ===\n",
            "train loss:1.6936289325786882\n",
            "train loss:1.5413033026528373\n",
            "train loss:1.525435164395416\n",
            "train loss:1.3994938906059438\n",
            "train loss:1.609792072745283\n",
            "train loss:1.7571699500511944\n",
            "train loss:1.5832165809539616\n",
            "train loss:1.3752594619645486\n",
            "=== epoch:9, train acc:0.9, test acc:0.92 ===\n",
            "train loss:1.5951039522680959\n",
            "train loss:1.697572211102278\n",
            "train loss:1.518846679325388\n",
            "train loss:1.4679812303690882\n",
            "train loss:1.5588552892963932\n",
            "train loss:1.6784765817231793\n",
            "train loss:1.6594811573935706\n",
            "train loss:1.5736333409501426\n",
            "=== epoch:10, train acc:0.92125, test acc:0.93 ===\n",
            "train loss:1.6067498895525696\n",
            "train loss:1.418448441675367\n",
            "train loss:1.4474928946853374\n",
            "train loss:1.4613634580895416\n",
            "train loss:1.4800471292542332\n",
            "train loss:1.4764699992958419\n",
            "train loss:1.4373955072770805\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.92\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dd3hUVfrA8e+bRkgICSTUFGrooUgoUpQiHREEO666Kq4rrrtrX7vbWN31p66KoqCsgCKIgoCACKiIlNA7hJaEAAkJCZBCypzfH3eQgAkMmJuZZN7P8/CQe++ZuW9Gue/cc895jxhjUEop5b183B2AUkop99JEoJRSXk4TgVJKeTlNBEop5eU0ESillJfTRKCUUl7OtkQgIlNEJE1EtpVxXETkTRFJFJEtInKVXbEopZQqm513BB8Bgy9yfAgQ6/wzDphoYyxKKaXKYFsiMMZ8D2RepMkNwP+MZTUQJiIN7IpHKaVU6fzceO5IILnEdopz35ELG4rIOKy7BoKDgzu3atWqQgJUSqmqYv369ceNMXVKO+bOROAyY8wkYBJAfHy8SUhIcHNESilVuYjIobKOuXPU0GEgusR2lHOfUkqpCuTORDAP+I1z9FB3INsY84tuIaWUUvayrWtIRD4B+gARIpICvAD4Axhj3gUWAkOBRCAXuMeuWJRSSpXNtkRgjLntEscN8JBd51dKKeUanVmslFJeThOBUkp5OU0ESinl5TQRKKWUl9NEoJRSXk4TgVJKeTlNBEop5eU0ESillJerFEXnlFLKaxXmQX425GVBjboQVLvcT6GJQCml7OQoti7k+dmQn3Xuol7qdin7is+ce6/h/wfxvy33EDURKKXUxRjj/FZ+qYt4GRf1Mycv/v7iC9XDIDDU+ScMakb+vM9RLYxsgkgrDKRWne7UteFX1ESglFKlKSqATdPgh9cgO/nibQNqWBfwwFDrAh4WDYFx57ZLXuTP2xcGAcHkFzlIzswlKTOXQxnOv4/ncCgzl5TMPAqKHQD8NSiQOxuV/6+qiUAppUoqLoKtn8GKCZB1CKK7QZd7rYt2yQt4yb99L30pzcot4FBGLocyc0nan8OhjAwOZSaTlJHL0ZP557WtUc2PmNpBtKwXwoA29WhUO5hG4UG0blDTll9ZE4FSSgE4HLB9Dqz4J2QkQoOOMOw/0Pw6EHHh5YajJ/Od3+hzzl30M3I5lJHDyfyi89rXCalGo9pB9Gge/vOFPiY8iEa1g6gdHIC4cM7yoolAKeXdjIFd82H5PyBtB9RtC7dMh1bDfpEA8guLSTlRovvm579zSD6RR0GR4+e2fj5CZK3qxNQOokN0QxrVDrYu9OFBxNQOIijAcy6/nhOJUkpVJGMgcSks+xsc2QThsTBmCrQZBT7WFKvtqdl8vv4w21OzScq0unCMOfcWwQG+xIQHE1s3hP6t6xFT27rQN6odTMOwQPx8K8dULU0ESinvs/87KwGkrIWwRjByIsTdDL5+ZOcWMndzEjPXJbM99SQBvj60i6zJ1U3DS3yjt7pywiu4C8cumgiUUt4jabWVAA7+YA3RHP46dBqLQ/z4aX8GnyUk8/W2oxQUOWjdoCYvXt+GkZ0iCQsKcHfkttJEoJSq+g5vgOV/t7qCguvCkFfgqrtIzTHMXnGQWeuTSc7MIyTQj1vio7mlSzTtIkPdHXWF0USglKq6jm6zHgLvXgDVa8OAlzlz1W9Zuvc0Mz/ewg970zEGejQL57GBLRnUtj6B/r7ujrrCaSJQSlU96XusYaDb50C1UOj7LHuajOXTzSf44tWfOJFbSIPQQMb3bc5NnaOJCQ9yd8RupYlAKVV1ZO6H716BLTPBP4gzPR5lXtAopm3KZvPXG/D3FQa0qcfN8dH0jq2Dr0/lf9BbHjQRKKUqv6xk+P5V2DQd4+PHkdb3MrFoOLN+yCO/MImW9UJ4bngbRnZsSHiNau6O1uNoIlBKVV6njlq1gNZ/iAG2NhjNi5mD2LAhkBrVzjCqUxS3dImmQ1RolRjmaRdNBEqpyifnOPz4OmbtB5jiQn4IHsgzGYNJ2RdB1ya1+c910QyJq+9Rs3c9mX5KSqnKI+8ErHoLx+qJUJjHQunNK/kjyfeLYcy1UdwUH02TiGB3R1npaCJQSnm+M6c48+PbyKq3CCg6xYLi7vy3eAyNW3XihS7RXNuiTqUp5+CJNBEopTyWKcghZcmb1N44keDibJYUd2ZWyG/o0r030ztFUSdEH/yWB00ESqnzGGM4daaIomJDkcNBscNQVGysvx0Ois7bPrf/bDtrn+PcsRJtixyOC157rl1h8bntGvlHaJuxhK7HZhJtsvjBdGRjswfpee1AJsXU0ge/5UwTgVIKYww7jpxk/pYjLNhyhKTM3Ao9v6+PEO5zmqE+a7je50c6sxOArQEd2NT5Tbr2GUbvanq5sot+skp5KWMMu4+dYsGWI8zfcoQDx3Pw9RF6NAvnjm4xBPr74usj+PsKvj4++PkIvj5y7m/nfv8Lti9s5+/rc/7rfHzw9XVuF+fht3cRsnW2VQfIUQgRLSDuWYgbQ1ztJu7+mLyCJgKlvMzeY6esb/5bj5CYdhofgaubhTPumqYMaluf2sE2V9osLoL9K6zlIHfOh8IcCGkI3X8HcTdB/fYurQimyo8mAqW8wL700yxwdvvsPnYKEejWpDZ392jH4Hb1ibB7tq0xkJJgXfy3zYHc49Zav3GjrXUAGvUAH+8r9uYpNBEoVUUdPJ7Dgq1Wt8/OIycRgS6NavPSiLYMiatP3ZBA+4NI3w1bZ1l/ThwEv0BoMdj65h87APx01I8n0ESgVBWSnJnr7PZJZdvhkwB0blSL54e3YWhcA+qHVsDF/2QqbJ1tXfyPbgHxgSbXwrVPQqvhEFjT/hjUZbE1EYjIYOANwBf4wBgz4YLjMcBUIMzZ5iljzEI7Y1KqqjmclcfCLUeYvyWVzSnZAHSMDuPZYa0ZEteAyLDq9geRdwJ2zLMu/gdXAgYiO8PgCdD2RgipZ38M6orZlghExBd4GxgApADrRGSeMWZHiWbPAp8ZYyaKSBtgIdDYrpiUqiqOZOexcOtR5m9JZWNSFgBxkaE8PaQVQ+MaEF27AurrF+bBnkXWt/+9S6C4AMKbQ5+nIW4MhDezPwZVLuy8I+gKJBpj9gOIyKfADUDJRGCAs/eJoUCqjfEoVamlncxn4VZrtM+6gycAaNOgJk8MbsmwuAY0Cq+AGjuOYjjwHWyZBTu/goJTUKM+dLkf2t8EDTrqiJ9KyM5EEAkkl9hOAbpd0OZFYImIPAwEA9eV9kYiMg4YBxATE1PugSrlqdJPnWHRNuuB79qDmRgDreqH8OiAFgxr34CmdWrYH4QxkLrBuvhv+xxy0qBaTWh7g/XQt3FvHfFTybn7YfFtwEfGmP+IyNXAxyLSzhjjKNnIGDMJmAQQHx9v3BCnUhUmM6eARdusbp/V+zNwGGhetwaP9I9lePsGNK8bUjGBHE88N+Incx/4BkCLQdZwz9iB4F8BD55VhbAzERwGoktsRzn3lXQvMBjAGPOTiAQCEUCajXEp5ZHW7M/greWJrNqXQbHD0DQimPF9mzOsfUNa1q+gi39hHmz+FDZMhdSNgECTa6DXn6D19VA9rGLiUBXKzkSwDogVkSZYCeBW4PYL2iQB/YGPRKQ1EAik2xiTUh7n2Ml8/r5gJ/M2p9IgNJDfXduUYXENad0gpOKKq51Oh3UfwLr3ITcD6sfBoH9YI35qNqiYGJTb2JYIjDFFIjIeWIw1NHSKMWa7iLwMJBhj5gGPAu+LyJ+wHhzfbYzRrh/lFQqKHHz44wHe/HYvhQ7DH/rH8uC1zageUIH97el74Ke3rLuA4jPQYgj0eNia6asPfb2Grc8InHMCFl6w7/kSP+8AetoZg1Ke6Ie96bwwbzv703O4rnVdnh/elpjwChjyCdbD34MrrQSwZ5E127fj7XD1QxARWzExKI/i7ofFSnmVlBO5/H3BTr7edpRG4UFMuTuefq0qaLJVcSFs/xJ++i8c2QxBEdaY/y73QXBExcSgPJImAqUqQH5hMe9/v5+3VyQC8PigltzbqwmB/hXQDZSfDRv+B6vfhZMpEB4L178B7W8B/wqYdaw8niYCpWy2bNcxXvpqB4cychkaV59nhrWpmLIPWcmw5l1YP9Wa+NW4Nwz7jzX000fX91XnaCJQyiaHMnJ4+asdfLsrjWZ1gpl2bzd6xVZAF0zqRlj1Fmz/wtpud6PV/9+wk/3nVpWSJgKlylleQTETVyTy7vf78fcR/jK0FXf3aEKAn43fwh0O2LvYSgCHVkJACHR/ELr9DsKiL/165dU0EShVTowxLN5+lL/O38nhrDxGdmzI00NbU6+mjTNwC/Ng8yfw0zuQsRdqRsHAv8FVv7EWflHKBZoIlCoH+9JP8+K87fyw9zit6ocwc1x3ujUNt++EF04Aa9ARRk+GNjeAr79951VVkiYCpX6F02eK+O+yvUxZeYBAf19evL4NY7s3ws/Xpm6g9D2w+m3Y9IlzAthg5wSwnjoBTF0xTQRKXQFjDF9tOcLfF+zg2Mkz3BwfxRODW9mz9u+FE8B8q0HH26D7Q1CnRfmfT3kdTQRKXabdR0/x/NxtrDmQSbvImkwc25mrYmqV/4l+MQEsHK59ypoAVqNO+Z9PeS1NBEq5KDuvkNeX7uF/Px0iJNCPf4yK45Yu0fj6lHOXzC8mgDWH4a9Dh1t1ApiyhSYCpS7B4TDM2XiYCV/vJCOngNu7xvDYwJbUCg4o3xOdToMf3zg3AaxRLxj2b4gdpBPAlK00ESh1EdsOZ/P83G1sSMqiU0wYH93TlXaR5Twss6gA1r4H370CBTnQdiRcPR4iryrf8yhVBk0ESpUiK7eAVxfvZsbaJMKDA3h1THtGXxWFT3l2AxljLfq+6GlrBbDYgdYaAFoBVFUwTQRKlVDsMMxcl8yri3dxMr+Iu3s05o/XtSC0ejmPzU/fA4ufhsSl1jOA22dBi4Hlew6lXKSJQCmnjUkneGHedrakZNO1SW1eGtGW1g1qlu9J8rLgu3/B2kngH2TdAXS5H/zK+XmDUpdBE4HyeocycnhrWSKz1qdQr2Y13ri1IyM6NCzfZSIdxdZIoGV/hdxMqwREv+d0GKjyCJoIlFcyxrDmQCaTVx5g6c5j+PkID1zTlIf7x1KjWjn/szi4Er5+Co5thZgeMGQCNOhQvudQ6lfQRKC8SkGRg/lbUpm88gDbU09SK8if8X2bc2f3RtQt7+JwWUmw5DnY8aVVDG7Mh9B2lJaCUB5HE4HyCidyCpixNompqw6SduoMzevW4B+j4rjxqsjyXyWsIMeaD/DjG4BAn79Y9YACKmhNYqUukyYCVaUlpp1myo8HmLMhhfxCB71jI3hlTHuuia1TvkNBwRoOuu1z+OZ5OHkY2o2G617S9QCUx9NEoKocYwwrE48zeeUBVuxOJ8DPh1EdI/ltrya0rB9iz0lTN1rPAZJXQ/32VknoRlfbcy6lypkmAlVl5BcWM3fTYaasPMjuY6eIqBHAn65rwR3dY+ypCgpw6hgsexk2TofgCBjxX+h4B/hUwKL0SpUTTQSq0ks/dYZpqw8xbfUhMnIKaFU/hFfHtGdEx4ZU87PpglxUYC0M/90rUJQPPcbDNY/rqmCqUtJEoCqtXUdPMvmHA8zdlEpBsYN+repyb68m9GgWXr5zAEoyBvYshsV/cZaFGOQsC9HcnvMpVQE0EahKxeEwfLcnnckrD7Ay8TiB/j7c3CWKe3o2oVmdGvaePH23VRdo37cQHgt3zIbYAfaeU6kKoIlAVQp5BcV8viGFKT8eYH96DvVqVuPxQS25vWtM+ZeD/sXJT8AKZ1mIgBow6J/Q9X5dG1hVGZoIlEc7djKfqasOMmNtElm5hcRFhvL6LR0ZGteAAD+ba/Q7imHDVFj2N6ssROe7rLIQwRH2nlepCqaJQHmkbYezmbzyAPO3pFLkMAxsU497ezWlS+Na9vX/l3TgB6sb6NhWa2H4wROgQXv7z6uUG2giUB6j2GFYuvMYk1ceYO2BTIIDfBnbvRH39GhCTHgFzco9cQi+eQ52zIXQaLjpI2gzUstCqCpNE4Fyu9NnipiVkMxHqw5yKCOXyLDqPDO0Nbd0jaZmYAX1wxfkwMrXYdWbgEDfZ6yyELpGsPICmgiU2zgchteX7uHDVQc5lV/EVTFhPDGoFYPa1sPPtwLW6HU4IGUd7JwHW2fD6aMQdxNc9yKERtl/fqU8hCYC5RbGGJ6bu43pa5IY0q4+91/TlKtiatl/4uIiOLQSdn4FO+dbF38ff2jWF3pPhZju9seglIfRRKDc4tXFu5m+JonfXduMp4a0svdkhfmwf4V18d+9wBoO6h8Eza+D1iOsJSJ1RrDyYrYmAhEZDLwB+AIfGGMmlNLmZuBFwACbjTG32xmTcr93v9vHOyv2cXu3GJ4c3NKek5w5DYnfWBf/PUug4BRUC4WWg62Lf7N+WhZaKSfbEoGI+AJvAwOAFGCdiMwzxuwo0SYWeBroaYw5ISJ17YpHeYYZa5KY8PUuru/QkL/e0K58h4LmnYDdi6w+/8RvofgMBNeBuNHQ+npofI2uDaxUKey8I+gKJBpj9gOIyKfADcCOEm3uB942xpwAMMak2RiPcrN5m1N55sut9G1Zh9du7oBveawHcOqY1d2zYx4c/AEcRdZqYPG/tS7+Md21EqhSl2BnIogEkktspwDdLmjTAkBEfsTqPnrRGLPowjcSkXHAOICYmBhbglX2Wr4rjT/P3ESXxrV5547O+P+aUUFZSdaD3p3zIGk1YKB2M2u4Z+vroeFVOu5fqcvg7ofFfkAs0AeIAr4XkThjTFbJRsaYScAkgPj4eFPRQapfZ83+DH43bT2tGoQw+a54qgdcwTf043utSV47v4Ijm6x99eKgz9PWxb9ua734K3WFXEoEIjIHmAx8bYxxuPjeh4GSa/RFOfeVlAKsMcYUAgdEZA9WYljn4jmUh9uaks29UxOIqlWdqfd0JcTVCWLGwNGt1rf+nV9B+i5rf1QXGPAytBoO4c3sC1wpL+LqHcE7wD3AmyIyC/jQGLP7Eq9ZB8SKSBOsBHArcOGIoC+B24APRSQCq6tov6vBK8+WmHaauz5cS2h1f6bd143wS60S5nDA4YRz3/yzDoH4WLV+4u+FVsMgNLJiglfKi7iUCIwxS4GlIhKKdeFeKiLJwPvANOc3+gtfUyQi44HFWP3/U4wx20XkZSDBGDPPeWygiOwAioHHjTEZ5fKbKbdKzsxl7Adr8BFh2n3daBBaRqmGi03wuuYxaDlUq30qZTMxxrUudxEJB8YCdwKpwHSgFxBnjOljV4AXio+PNwkJCRV1OnUF0k7lc/O7P5GZU8DMB66mdYOapTfMSoKPhll/6wQvpWwlIuuNMfGlHXP1GcEXQEvgY+B6Y8wR56GZIqJXZfWz7NxCfjN5LcdOnmHafd3KTgK5mTBtNORnw01TIXagTvBSyk1cfUbwpjFmeWkHysowyvvkFhRxz0dr2Z+ew+S74+ncqIzaQYV58MltcOIg3PklNO5ZoXEqpc7n6mDuNiISdnZDRGqJyO9tiklVQmeKinng4/VsSs7izds60ju2TukNHcUw535IXgM3TtIkoJQHcDUR3F9ybL9zJvD99oSkKpuiYgePfLKJH/Ye51+j2zO4XYPSGxoDi56yHgwP/ie0HVWxgSqlSuVqIvCVEkVhnHWEtGiLwuEwPDVnK4u2H+X54W24KT667MY/vmEtAH/1eOj+YMUFqZS6KFefESzCejD8nnP7Aec+5cWMMfxtwU5mr0/hkf6x/LZXk7Ibb/kMlr4A7UbDgL9WXJBKqUtyNRE8iXXxP/s17hvgA1siUpXGm98mMuXHA9zTszF/vC627Ib7V8CXv4fGvWHkRPCpgNXHlFIuc3VCmQOY6PyjFFNWHuD/lu5hTOconhvWpuxy0ke3wqdjIaIF3DIN/C4xu1gpVeFcnUcQC/wTaAMEnt1vjGlqU1zKg81en8LL83cwqG09JtwYh09Z5aSzkmDaGAisCXfMguphpbdTSrmVq/foH2LdDRQBfYH/AdPsCkp5rkXbjvLE7M30ah7Bm7d1KnuR+dxMKwkU5cHYz7VGkFIezNVEUN0Y8y1WSYpDxpgXgWH2haU80cq9x/nDJxvpEB3Ge3d2pppfGeWkC/Ph09vhxAG4dYZVIlop5bFcfVh8RkR8gL3OQnKHgRr2haU8zYakE4z7OIGmdYL56O6uBFcr43+dsxPGklbDmCnQuFfFBqqUumyu3hE8AgQBfwA6YxWfu8uuoJRn2XnkJHdPWUudkGr8796uhAaVsaaAMbDoaWsNgUH/gHY3VmygSqkrcsk7AufksVuMMY8Bp7HWJVBe4uDxHO6cvJagAD+m3duNuiGBZTde9Sasfc+aMHa1ViBRqrK45B2BMaYYq9y08jJHs/O544M1FDscTLuvK9G1L1IddMss+OZ5nTCmVCXk6jOCjSIyD5gF5JzdaYyZY0tUyu0ycwoYO3kN2XmFfHJ/d5rXDSm78f4V8OWDOmFMqUrK1UQQCGQA/UrsM4AmgiroVH4hd01ZS3JmLlN/25W4qIssEvPzhLFYnTCmVCXl6sxifS7gJfILi7lvagI7j5xk0m86071peNmNz5swNlsnjClVSbk6s/hDrDuA8xhjflvuESm3KSx28PvpG1h7MJPXb+lIv1b1ym58dsJYYR7cu1gnjClVibnaNTS/xM+BwCisdYtVFVHsMDz62WaW7Urj76PacUPHi1zYS04Yu/MLnTCmVCXnatfQ5yW3ReQTYKUtEakKZ4zh+bnbmLc5lScHt+KObo3KbvzzhLGfYMyHOmFMqSrgSod3xAJ1yzMQ5T6vLt7N9DVJPNinGQ/2aVZ2w/MmjP1TJ4wpVUW4+ozgFOc/IziKtUaBquTe/W4f76zYxx3dYnhiUMuLN9YJY0pVSa52DV1kELmqrGasSWLC17sY0aEhL9/Qruw1BeDchLG2N+qEMaWqGJe6hkRklIiEltgOE5GR9oWl7PbV5lSe+XIr/VrV5T83d8C3rDUF4PwJY6Pe1QljSlUxrv6LfsEYk312wxiTBbxgT0jKbpk5BTw9ZyudY2rxzh1X4V/WmgJgTRibeadOGFOqCnM1EZTWztWhp8rDvLUskdyCIiaMjiPQv4w1BQCykmH6TVAtRCeMKVWFuZoIEkTkNRFp5vzzGrDezsCUPVJO5DJt9SFu6hx98fpBuZkwbTQU5FpJQCeMKVVluZoIHgYKgJnAp0A+8JBdQSn7vPbNHkTgjwNiy25UmA+f3mFNGLttBtRrU3EBKqUqnKujhnKAp2yORdls55GTfLHxMON6N6VBaPXSGzmK4YtxkLRKJ4wp5SVcHTX0jYiEldiuJSKL7QtL2eHVxbsJqeZX9qQxY2DxX2DHXF1hTCkv4mrXUIRzpBAAxpgT6MziSmXtgUyW7UrjwT7NCQsKKL3Rqv/CmnedE8a0508pb+FqInCISMzZDRFpTCnVSJVnMsYw4eud1KtZjbt7NC690ZZZ8M1zOmFMKS/k6hDQZ4CVIvIdIEBvYJxtUalytWTHMTYkZfHPG+OoHlDKcNH93+mEMaW8mKsPixeJSDzWxX8j8CWQZ2dgqnwUFTt4dfFumtYJ5qbOUb9scHQbzNQVxpTyZq4+LL4P+BZ4FHgM+Bh40YXXDRaR3SKSKCJljjoSkdEiYpzJRpWjORsOk5h2micGtcTvwhnEWckwfYxOGFPKy7naB/AI0AU4ZIzpC3QCsi72AhHxBd4GhgBtgNtE5BcD0kUkxPn+ay4jbuWC/MJiXvtmDx2jwxjUtv75B/NO6IQxpRTgeiLIN8bkA4hINWPMLuASNYvpCiQaY/YbYwqwJqLdUEq7vwL/wpqkpsrR1FUHOXoyn6eGtDq/smhxkVU/SCeMKaVwPRGkOOcRfAl8IyJzgUOXeE0kkFzyPZz7fiYiVwHRxpgFF3sjERknIgkikpCenu5iyN4tO6+Qd1bso0/LOr9cgH753+HgDzDivzphTCnl8sPiUc4fXxSR5UAosOjXnFhEfIDXgLtdOP8kYBJAfHy8Dlt1wbvf7eNkfiFPDGp1/oE9i2Hla9D5buhwq1tiU0p5lsuuIGqM+c7FpoeB6BLbUc59Z4UA7YAVzm6L+sA8ERlhjEm43LjUOUez85my8gA3dGhIm4Y1zx3ISoI546B+HAz+l/sCVEp5FDsHjK8DYkWkiYgEALcC884eNMZkG2MijDGNjTGNgdWAJoFy8Ma3e3AYw6MDSzzGKSqAWfeAccBNU8E/0H0BKqU8im2JwBhTBIwHFgM7gc+MMdtF5GURGWHXeb1dYtppPktI4Y5ujYiuHXTuwNIX4HAC3PAWhF9kgXqllNexdXEZY8xCYOEF+54vo20fO2PxFv9evJtAPx/G92t+bueOubD6Hej2ILQpbeCWUsqbaS2BKmRj0gkWbT/KuGuaEVHDOUM4Yx/MHQ+R8TDgZfcGqJTySJoIqgirsNwuImoEcF/vJtbOwnyYdRf4+MJNH4FfGVVHlVJeTRNBFbFiTzprDmTycL9Ygqs5e/wWPWUtPj/qPQiLvvgbKKW8liaCKsDhMLyyaDcxtYO4rauzWviWz2D9h9DrT9BikHsDVEp5NE0EVcC8zansPHKSRwe2IMDPB9J2wVePQKOe0PdZd4enlPJwmggquTNFxfx7yW7aNqzJ9e0bQkGO9VwgIBhGTwZfWweGKaWqAE0EldyMNUmknMjjicGt8BFg/p8hfTeM/gBqNnB3eEqpSkATQSV2+kwRby1LpEezcK6JjYAN/4Mtn0Kfp6FpH3eHp5SqJDQRVGLvf7+fjJwCnhzcCjm6FRY+Dk37wjWPuTs0pVQloomgkko/dYb3f9jP0Lj6dKjjYz0XCKoNN75vzRtQSikX6ZPESuqtZXs5U+TgsQEtYN54OHEI7l4ANeq4OzSlVCWjdwSV0KGMHGasTeKWLtE0PTADdnwJ170Aja52d2hKqUpIE0El9J8le/D1ER5tcwoWPwMthsDVD7s7LKVUJaWJoJLZdjibeZtT+X232oQvfABCGsDId8BH//efH0MAABF3SURBVFMqpa6MPiOoZF5ZvJta1X158MS/4dQRuHex9ZBYKaWukCaCSmRV4nG+35POzLZr8N+3BIa8CpGd3R2WUqqS0/6ESsIYw78W7WJIyH667n8L2oyErve7OyylVBWgdwSVxNfbjpKSksQnoW8itRrDiP+CiLvDUkpVAZoIKoHCYgevLdrB+zXepXrxSbh5DgTWdHdYSqkqQhNBJfBZQjJDs2Zwlf9m606gfpy7Q1JKVSGaCDxcXkExq5bM5r/+n2M63Ip0utPdISmlqhh9WOzhPl22lpeKXudMWHNk2Gv6XEApVe70jsCDnTiVS/uf/kQNnwIC75huLTajlFLlTO8IPNiO6U/QWXaS2e9VqNPS3eEopaooTQQeKmPDXHoe/ZhVtUbQsPdv3B2OUqoK00TgibKSqL7gIXaYxjS64013R6OUquI0EXiaogLyZtxJcVERy+NeJTKilrsjUkpVcZoIPM03z1M9bRPP8XtuH9LH3dEopbyAJgJPsv1LWDORyUVDaH7tbdQKDnB3REopL6DDRz1Fxj7M3PHs9W/JB3538W2vJu6OSCnlJfSOwBMU5sGsuyjEl7tPPcRD17UhKEBztFKqYmgi8ASLnoKjW3nJ7w9Ui2jELV2i3R2RUsqLaCJwt80zYf1H7Gp+P9MzW/HYwJb4++p/FqVUxdH+B3dK2wXz/4gjpgf3Jw2kfVQQQ+PquzsqpZSXsfWrp4gMFpHdIpIoIk+VcvzPIrJDRLaIyLci0sjOeDxKQQ589hsICObTmBdIPlnIk4NbIVpUTilVwWxLBCLiC7wNDAHaALeJSJsLmm0E4o0x7YHZwCt2xeNRjIH5f4bje8i5/j1eWXWS3rER9Gwe4e7IlFJeyM47gq5AojFmvzGmAPgUuKFkA2PMcmNMrnNzNRBlYzyeY8NU2PIp9P0L7xyKJCvXuhtQSil3sDMRRALJJbZTnPvKci/wdWkHRGSciCSISEJ6eno5hugGqZtg4RPQrB9pHcczeeUBru/QkHaRoe6OTCnlpTxieIqIjAXigVdLO26MmWSMiTfGxNepU6digytPW2fDR8MhKBxufJ83lu2jqNjw6IAW7o5MKeXF7Bw1dBgoOSA+yrnvPCJyHfAMcK0x5oyN8bhPQS58/QRs/Biiu8HoyezPDeTTdcnc0S2GxhG64IxSyn3sTATrgFgRaYKVAG4Fbi/ZQEQ6Ae8Bg40xaTbG4j5pO2HW3ZC+G3r9Gfr+BXz9+c/0DVTz8+HhfrHujlAp5eVsSwTGmCIRGQ8sBnyBKcaY7SLyMpBgjJmH1RVUA5jlHDaZZIwZYVdMFcoY2PA/+PpJqFYDxn4OzfsDsCUliwVbj/CHfs2pE1LNzYEqpbydrRPKjDELgYUX7Hu+xM/X2Xl+t8k/CfP/CNs+hybXwo3vQ0g9AIwxTPh6F7WDA7j/mqZuDlQppXRmcflL3Qiz7oGsQ9DvWas7yMf358OLth1l1b4Mnh/ehpBAfzcGqpR3KSwsJCUlhfz8fHeHYqvAwECioqLw93f9+qKJoLwYA2vehSXPQY26cPcCaNQDgKzcAuZuSmXmumR2HDlJk4hg7uge4+aAlfIuKSkphISE0Lhx4yo7g98YQ0ZGBikpKTRp4nope00E5SE3E+Y+BLsXQovBMHIijsBarNp7nJkJySzefpSCIgdtG9bk5RvackPHSKr5+V76fZVS5SY/P79KJwEAESE8PJzLnW+lieDXSloNs++F08dg0D853OpuZq1KYVbCJg5n5RFa3Z/bukRzU3y0ThpTys2qchI460p+R00EV8rhgB//D5b9HRMazcprZzBpRygr5y3HGOjVPIInBrdkUNv6BPrrt3+llOfSRHAlTqfBnHGwfznbavXnd1l3kvJ1IQ1DT/Nwv1hu6hxFdO0gd0eplPoVvtx4mFcX7yY1K4+GYdV5fFBLRna6WJWci8vKymLGjBn8/ve/v6zXDR06lBkzZhAWFnbF574UTQSXKWfXUnzmjMOn4BQvFN7HnLT+DGhbn3/ER9OzeQS+PlX/1lOpqu7LjYd5es5W8gqLATiclcfTc7YCXHEyyMrK4p133vlFIigqKsLPr+xL8cKFC8s8Vl40EbjAGMPqxDROLfor1x2fxj7TkP+EvkjXbr1Z0ymSWsEB7g5RKXUZXvpqOztST5Z5fGNSFgXFjvP25RUW88TsLXyyNqnU17RpWJMXrm9b5ns+9dRT7Nu3j44dO+Lv709gYCC1atVi165d7Nmzh5EjR5KcnEx+fj6PPPII48aNA6Bx48YkJCRw+vRphgwZQq9evVi1ahWRkZHMnTuX6tWrX8EncD5NBBdxNDufzzeksHztBp7I+TcDfXaTED6Matf/m4mNG3jFgyelvNGFSeBS+10xYcIEtm3bxqZNm1ixYgXDhg1j27ZtPw/znDJlCrVr1yYvL48uXbowevRowsPDz3uPvXv38sknn/D+++9z88038/nnnzN27NgrjuksTQQXKChysGzXMWauS+a7Pen0lfV8WG0Sgf7FFAyfRHynW9wdolLqV7rYN3eAnhOWcTgr7xf7I8OqM/OBq8slhq5du5431v/NN9/kiy++ACA5OZm9e/f+IhE0adKEjh07AtC5c2cOHjxYLrFoInDae+wUnyUkM2fDYTJyCogM8eGzxvOIP/Ip1GsPN30E4c3cHaZSqgI8Pqjlec8IAKr7+/L4oJbldo7g4HNVh1esWMHSpUv56aefCAoKok+fPqXOgK5W7VxtMl9fX/LyfpmsroRXJIKynv6fPlPE/M2pzExIZmNSFn4+wnWt63FXKwfdNzyGHNkEXR+AgX8FPy0Op5S3OPtAuDxHDYWEhHDq1KlSj2VnZ1OrVi2CgoLYtWsXq1evvuLzXIkqnwhKe/r/xOdbmLHmEFsPnySvsJjmdWvwzNDWjLoqkogDX8FXfwQfH7hlGrS+3s2/gVLKHUZ2ivxVF/4LhYeH07NnT9q1a0f16tWpV6/ez8cGDx7Mu+++S+vWrWnZsiXdu3cvt/O6QowxFXrCXys+Pt4kJCS43L6svj4BbukSzc1doukUHYYU5sGip6z1hKO6wpjJEKb1gJSqKnbu3Enr1q3dHUaFKO13FZH1xpj40tpX+TuC1FKSwFkTRre3fkjb5Vw8Zif0+hP0fQZ8tTKoUso7VPlE0DCseql3BA3DqlsVQzdOg4WPQ0Cwc/GYqrlEglJKlcUjFq+30+ODWlL9glo/1f19ebpfFMy5H+aNh+gu8OCPmgSUUl6pyt8RjFzah5G+adZimSV97QsY6Pss9D5/8RillPImVT4RkJNW+n5TDHcvhMY9KzYepZTyMFW+a+iiNAkopZQX3BEopdTlejW29N6E4Lrw+N4ressrLUMN8PrrrzNu3DiCguwpb+/ddwRKKVWasrqUy9rvgrNlqK/E66+/Tm5u7hWf+1L0jkAp5X2+fgqObr2y1344rPT99eNgyIQyX1ayDPWAAQOoW7cun332GWfOnGHUqFG89NJL5OTkcPPNN5OSkkJxcTHPPfccx44dIzU1lb59+xIREcHy5cuvLO6LqPqJILhu2bd4SilVQUqWoV6yZAmzZ89m7dq1GGMYMWIE33//Penp6TRs2JAFCxYAVg2i0NBQXnvtNZYvX05ERIQtsVX9RHCF/XlKqSrsIt/cAXgxtOxj9yz41adfsmQJS5YsoVOnTgCcPn2avXv30rt3bx599FGefPJJhg8fTu/evX/1uVxR9ROBUkp5GGMMTz/9NA888MAvjm3YsIGFCxfy7LPP0r9/f55//nnb49GHxUopdaGyuo5/RZdyyTLUgwYNYsqUKZw+fRqAw4cPk5aWRmpqKkFBQYwdO5bHH3+cDRs2/OK1dtA7AqWUupANXcoly1APGTKE22+/nauvtlY7q1GjBtOmTSMxMZHHH38cHx8f/P39mThxIgDjxo1j8ODBNGzY0JaHxVW+DLVSSoGWob5YGWrtGlJKKS+niUAppbycJgKllNeobF3hV+JKfkdNBEoprxAYGEhGRkaVTgbGGDIyMggMDLys1+moIaWUV4iKiiIlJYX09HR3h2KrwMBAoqKiLus1mgiUUl7B39+fJk2auDsMj2Rr15CIDBaR3SKSKCJPlXK8mojMdB5fIyKN7YxHKaXUL9mWCETEF3gbGAK0AW4TkTYXNLsXOGGMaQ78H/Avu+JRSilVOjvvCLoCicaY/caYAuBT4IYL2twATHX+PBvoLyJiY0xKKaUuYOczgkggucR2CtCtrDbGmCIRyQbCgeMlG4nIOGCcc/O0iOy+wpgiLnxvL6efx/n08zhHP4vzVYXPo1FZByrFw2JjzCRg0q99HxFJKGuKtTfSz+N8+nmco5/F+ar652Fn19BhILrEdpRzX6ltRMQPCAUybIxJKaXUBexMBOuAWBFpIiIBwK3AvAvazAPucv48BlhmqvJsD6WU8kC2dQ05+/zHA4sBX2CKMWa7iLwMJBhj5gGTgY9FJBHIxEoWdvrV3UtVjH4e59PP4xz9LM5XpT+PSleGWimlVPnSWkNKKeXlNBEopZSX85pEcKlyF95CRKJFZLmI7BCR7SLyiLtj8gQi4isiG0VkvrtjcTcRCROR2SKyS0R2isjV7o7JXUTkT85/J9tE5BMRubyynpWEVyQCF8tdeIsi4FFjTBugO/CQF38WJT0C7HR3EB7iDWCRMaYV0AEv/VxEJBL4AxBvjGmHNejF7gEtbuEViQDXyl14BWPMEWPMBufPp7D+kUe6Nyr3EpEoYBjwgbtjcTcRCQWuwRrRhzGmwBiT5d6o3MoPqO6c5xQEpLo5Hlt4SyIordyFV1/8AJzVXjsBa9wbidu9DjwBONwdiAdoAqQDHzq7yj4QkWB3B+UOxpjDwL+BJOAIkG2MWeLeqOzhLYlAXUBEagCfA380xpx0dzzuIiLDgTRjzHp3x+Ih/ICrgInGmE5ADuCVz9REpBZWz0EToCEQLCJj3RuVPbwlEbhS7sJriIg/VhKYboyZ4+543KwnMEJEDmJ1GfYTkWnuDcmtUoAUY8zZu8TZWInBG10HHDDGpBtjCoE5QA83x2QLb0kErpS78ArOMt+TgZ3GmNfcHY+7GWOeNsZEGWMaY/1/scwYUyW/9bnCGHMUSBaRls5d/YEdbgzJnZKA7iIS5Px3058q+uC8UlQf/bXKKnfh5rDcpSdwJ7BVRDY59/3FGLPQjTEpz/IwMN35pWk/cI+b43ELY8waEZkNbMAabbeRKlpqQktMKKWUl/OWriGllFJl0ESglFJeThOBUkp5OU0ESinl5TQRKKWUl9NEoJTNRKSPVjVVnkwTgVJKeTlNBEo5ichYEVkrIptE5D3nGgWnReT/nDXpvxWROs62HUVktYhsEZEvnHVpEJHmIrJURDaLyAYRaeZ8+xolavxPd85URUQmONeG2CIi/3bTr668nCYCpQARaQ3cAvQ0xnQEioE7gGAgwRjTFvgOeMH5kv8BTxpj2gNbS+yfDrxtjOmAVZfmiHN/J+CPWOthNAV6ikg4MApo63yfv9n7WypVOk0ESln6A52Bdc7SG/2xLtgOYKazzTSgl7Nmf5gx5jvn/qnANSISAkQaY74AMMbkG2NynW3WGmNSjDEOYBPQGMgG8oHJInIjcLatUhVKE4FSFgGmGmM6Ov+0NMa8WEq7K63JcqbEz8WAnzGmCGvRpNnAcGDRFb63Ur+KJgKlLN8CY0SkLoCI1BaRRlj/RsY429wOrDTGZAMnRKS3c/+dwHfOFd9SRGSk8z2qiUhQWSd0rgkR6iz49yesZSGVqnBeUX1UqUsxxuwQkWeBJSLiAxQCD2EtzNLVeSwN6zkCwF3Au84LfckKnXcC74nIy873uOkipw0B5joXRBfgz+X8aynlEq0+qtRFiMhpY0wNd8ehlJ20a0gppbyc3hEopZSX0zsCpZTycpoIlFLKy2kiUEopL6eJQCmlvJwmAqWU8nL/D/ywzO5QR7YnAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TorgALP16HYK"
      },
      "source": [
        "## Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xb0_7b8drf-B"
      },
      "source": [
        "class DeepConvNet:\r\n",
        "    \"\"\"정확도 99% 이상의 고정밀 합성곱 신경망\r\n",
        "\r\n",
        "    네트워크 구성은 아래와 같음\r\n",
        "        conv - relu - conv- relu - pool -\r\n",
        "        conv - relu - conv- relu - pool -\r\n",
        "        conv - relu - conv- relu - pool -\r\n",
        "        affine - relu - dropout - affine - dropout - softmax\r\n",
        "    \"\"\"\r\n",
        "    def __init__(self, input_dim=(1, 28, 28),\r\n",
        "                 conv_param_1 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\r\n",
        "                 conv_param_2 = {'filter_num':16, 'filter_size':3, 'pad':1, 'stride':1},\r\n",
        "                 conv_param_3 = {'filter_num':32, 'filter_size':3, 'pad':1, 'stride':1},\r\n",
        "                 conv_param_4 = {'filter_num':32, 'filter_size':3, 'pad':2, 'stride':1},\r\n",
        "                 conv_param_5 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\r\n",
        "                 conv_param_6 = {'filter_num':64, 'filter_size':3, 'pad':1, 'stride':1},\r\n",
        "                 hidden_size=50, output_size=10):\r\n",
        "        # 가중치 초기화===========\r\n",
        "        # 각 층의 뉴런 하나당 앞 층의 몇 개 뉴런과 연결되는가（TODO: 자동 계산되게 바꿀 것）\r\n",
        "        pre_node_nums = np.array([1*3*3, 16*3*3, 16*3*3, 32*3*3, 32*3*3, 64*3*3, 64*4*4, hidden_size])\r\n",
        "        wight_init_scales = np.sqrt(2.0 / pre_node_nums)  # ReLU를 사용할 때의 권장 초깃값\r\n",
        "        \r\n",
        "        self.params = {}\r\n",
        "        pre_channel_num = input_dim[0]\r\n",
        "        for idx, conv_param in enumerate([conv_param_1, conv_param_2, conv_param_3, conv_param_4, conv_param_5, conv_param_6]):\r\n",
        "            self.params['W' + str(idx+1)] = wight_init_scales[idx] * np.random.randn(conv_param['filter_num'], pre_channel_num, conv_param['filter_size'], conv_param['filter_size'])\r\n",
        "            self.params['b' + str(idx+1)] = np.zeros(conv_param['filter_num'])\r\n",
        "            pre_channel_num = conv_param['filter_num']\r\n",
        "        self.params['W7'] = wight_init_scales[6] * np.random.randn(64*4*4, hidden_size)\r\n",
        "        self.params['b7'] = np.zeros(hidden_size)\r\n",
        "        self.params['W8'] = wight_init_scales[7] * np.random.randn(hidden_size, output_size)\r\n",
        "        self.params['b8'] = np.zeros(output_size)\r\n",
        "\r\n",
        "        # 계층 생성===========\r\n",
        "        self.layers = []\r\n",
        "        self.layers.append(Convolution(self.params['W1'], self.params['b1'], \r\n",
        "                           conv_param_1['stride'], conv_param_1['pad']))\r\n",
        "        self.layers.append(Relu())\r\n",
        "        self.layers.append(Convolution(self.params['W2'], self.params['b2'], \r\n",
        "                           conv_param_2['stride'], conv_param_2['pad']))\r\n",
        "        self.layers.append(Relu())\r\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\r\n",
        "        self.layers.append(Dropout(0.2))\r\n",
        "        \r\n",
        "        self.layers.append(Convolution(self.params['W3'], self.params['b3'], \r\n",
        "                           conv_param_3['stride'], conv_param_3['pad']))\r\n",
        "        self.layers.append(Relu())\r\n",
        "        self.layers.append(Convolution(self.params['W4'], self.params['b4'],\r\n",
        "                           conv_param_4['stride'], conv_param_4['pad']))\r\n",
        "        self.layers.append(Relu())\r\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\r\n",
        "        self.layers.append(Dropout(0.2))\r\n",
        "        \r\n",
        "        self.layers.append(Convolution(self.params['W5'], self.params['b5'],\r\n",
        "                           conv_param_5['stride'], conv_param_5['pad']))\r\n",
        "        self.layers.append(Relu())\r\n",
        "        self.layers.append(Convolution(self.params['W6'], self.params['b6'],\r\n",
        "                           conv_param_6['stride'], conv_param_6['pad']))\r\n",
        "        self.layers.append(Relu())\r\n",
        "        self.layers.append(Pooling(pool_h=2, pool_w=2, stride=2))\r\n",
        "        self.layers.append(Dropout(0.2))\r\n",
        "        \r\n",
        "        self.layers.append(Affine(self.params['W7'], self.params['b7']))\r\n",
        "        self.layers.append(Relu())\r\n",
        "        self.layers.append(Affine(self.params['W8'], self.params['b8']))\r\n",
        "        \r\n",
        "        self.last_layer = SoftmaxWithLoss()\r\n",
        "\r\n",
        "    def predict(self, x, train_flg=False):\r\n",
        "        for layer in self.layers:\r\n",
        "            if isinstance(layer, Dropout):\r\n",
        "                x = layer.forward(x, train_flg)\r\n",
        "            else:\r\n",
        "                x = layer.forward(x)\r\n",
        "        return x\r\n",
        "\r\n",
        "    def loss(self, x, t):\r\n",
        "        y = self.predict(x, train_flg=True)\r\n",
        "        return self.last_layer.forward(y, t)\r\n",
        "\r\n",
        "    def accuracy(self, x, t, batch_size=100):\r\n",
        "        if t.ndim != 1 : t = np.argmax(t, axis=1)\r\n",
        "\r\n",
        "        acc = 0.0\r\n",
        "\r\n",
        "        for i in range(int(x.shape[0] / batch_size)):\r\n",
        "            tx = x[i*batch_size:(i+1)*batch_size]\r\n",
        "            tt = t[i*batch_size:(i+1)*batch_size]\r\n",
        "            y = self.predict(tx, train_flg=False)\r\n",
        "            y = np.argmax(y, axis=1)\r\n",
        "            acc += np.sum(y == tt)\r\n",
        "\r\n",
        "        return acc / x.shape[0]\r\n",
        "\r\n",
        "    def gradient(self, x, t):\r\n",
        "        # forward\r\n",
        "        self.loss(x, t)\r\n",
        "\r\n",
        "        # backward\r\n",
        "        dout = 1\r\n",
        "        dout = self.last_layer.backward(dout)\r\n",
        "\r\n",
        "        tmp_layers = self.layers.copy()\r\n",
        "        tmp_layers.reverse()\r\n",
        "        for layer in tmp_layers:\r\n",
        "            dout = layer.backward(dout)\r\n",
        "\r\n",
        "        # 결과 저장\r\n",
        "        grads = {}\r\n",
        "        for i, layer_idx in enumerate((0, 2, 6, 8, 12, 14, 18, 20)):\r\n",
        "            grads['W' + str(i+1)] = self.layers[layer_idx].dW\r\n",
        "            grads['b' + str(i+1)] = self.layers[layer_idx].db\r\n",
        "\r\n",
        "        return grads\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4S8fNMls0_e",
        "outputId": "00429405-af04-4563-bd5b-135f75860e00"
      },
      "source": [
        "network = DeepConvNet()  \r\n",
        "trainer = Trainer(network, x_train, t_train, x_test, t_test,\r\n",
        "                  epochs=10, mini_batch_size=100,\r\n",
        "                  optimizer='Adam', optimizer_param={'lr':0.001}, verbose=True)\r\n",
        "trainer.train()\r\n",
        "train_acc_list, test_acc_list = trainer.train_acc_list, trainer.test_acc_list\r\n",
        "\r\n",
        "markers = {'train': 'o', 'test': 's'}\r\n",
        "x = np.arange(len(train_acc_list))\r\n",
        "plt.plot(x, train_acc_list, marker='o', label='train', markevery=10)\r\n",
        "plt.plot(x, test_acc_list, marker='s', label='test', markevery=10)\r\n",
        "plt.xlabel(\"epochs\")\r\n",
        "plt.ylabel(\"accuracy\")\r\n",
        "plt.ylim(0, 1.0)\r\n",
        "plt.legend(loc='lower right')\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train loss:2.3182265158447835\n",
            "=== epoch:1, train acc:0.15125, test acc:0.175 ===\n",
            "train loss:2.2470057265260643\n",
            "train loss:2.3021216219113323\n",
            "train loss:2.2939028735513682\n",
            "train loss:2.2644759937799277\n",
            "train loss:2.2360284461510154\n",
            "train loss:2.264091771735145\n",
            "train loss:2.263618995310719\n",
            "train loss:2.1887028675046807\n",
            "=== epoch:2, train acc:0.35125, test acc:0.335 ===\n",
            "train loss:2.1621396793385266\n",
            "train loss:2.188031927362617\n",
            "train loss:2.1263486643075193\n",
            "train loss:2.118761304061116\n",
            "train loss:2.089515387709476\n",
            "train loss:2.045876118306391\n",
            "train loss:2.041910453826908\n",
            "train loss:1.9153636916874799\n",
            "=== epoch:3, train acc:0.61625, test acc:0.545 ===\n",
            "train loss:1.8225511240363828\n",
            "train loss:1.7150736024924036\n",
            "train loss:1.5582798614522084\n",
            "train loss:1.704456814566666\n",
            "train loss:1.4511987501608166\n",
            "train loss:1.4752890971381196\n",
            "train loss:1.4099049352797268\n",
            "train loss:1.2122054318273903\n",
            "=== epoch:4, train acc:0.755, test acc:0.725 ===\n",
            "train loss:1.0188642163551689\n",
            "train loss:1.0359631462370842\n",
            "train loss:1.0732925977082937\n",
            "train loss:0.9605580897353007\n",
            "train loss:0.9821038798971835\n",
            "train loss:0.9240425994689293\n",
            "train loss:0.8826078347108696\n",
            "train loss:0.6922775371694311\n",
            "=== epoch:5, train acc:0.76875, test acc:0.78 ===\n",
            "train loss:0.8720851810593839\n",
            "train loss:0.7138674636055248\n",
            "train loss:0.8228203768327954\n",
            "train loss:0.6975927018367145\n",
            "train loss:0.6326336944377379\n",
            "train loss:0.7397962699095624\n",
            "train loss:0.6533387007429774\n",
            "train loss:0.4901890679425201\n",
            "=== epoch:6, train acc:0.92375, test acc:0.925 ===\n",
            "train loss:0.49143832565444634\n",
            "train loss:0.7910882183548295\n",
            "train loss:0.5278387602237672\n",
            "train loss:0.48870767758694583\n",
            "train loss:0.42488958294060175\n",
            "train loss:0.6314365680375563\n",
            "train loss:0.428091316333771\n",
            "train loss:0.30202848351258\n",
            "=== epoch:7, train acc:0.9275, test acc:0.91 ===\n",
            "train loss:0.49797068512020315\n",
            "train loss:0.4373490487502105\n",
            "train loss:0.35551335220546976\n",
            "train loss:0.2778220292811719\n",
            "train loss:0.4907149669853195\n",
            "train loss:0.2794445201151876\n",
            "train loss:0.44719194102169013\n",
            "train loss:0.262279791816221\n",
            "=== epoch:8, train acc:0.9175, test acc:0.93 ===\n",
            "train loss:0.2762263845265689\n",
            "train loss:0.2696802278004176\n",
            "train loss:0.3042387369205143\n",
            "train loss:0.2724923614954301\n",
            "train loss:0.2307103531242802\n",
            "train loss:0.3196505877094409\n",
            "train loss:0.17247702168187598\n",
            "train loss:0.2548231393881811\n",
            "=== epoch:9, train acc:0.95125, test acc:0.94 ===\n",
            "train loss:0.3636863902211333\n",
            "train loss:0.252861352106948\n",
            "train loss:0.17031338652451367\n",
            "train loss:0.3227342943796701\n",
            "train loss:0.17315916563260964\n",
            "train loss:0.2921447524195025\n",
            "train loss:0.09217932814053233\n",
            "train loss:0.2811949135839741\n",
            "=== epoch:10, train acc:0.95875, test acc:0.945 ===\n",
            "train loss:0.2764830039734832\n",
            "train loss:0.3049045807510479\n",
            "train loss:0.16538706033005185\n",
            "train loss:0.1803177323573761\n",
            "train loss:0.26914348295635\n",
            "train loss:0.2576149521900593\n",
            "train loss:0.14909709323164533\n",
            "=============== Final Test Accuracy ===============\n",
            "test acc:0.96\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1C-pqsytiLD"
      },
      "source": [
        "# Recurrent Neural Network (RNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnxfN_oWz5gX"
      },
      "source": [
        "## Base Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "95TUmBVtHyLq",
        "outputId": "6bda1524-b63e-4689-810c-819453e7c72e"
      },
      "source": [
        "batch_size = 100\r\n",
        "wordvec_size = 100\r\n",
        "hidden_size = 100   \r\n",
        "time_size = 35     \r\n",
        "lr = 20.0\r\n",
        "max_epoch = 4\r\n",
        "max_grad = 0.25\r\n",
        "\r\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\r\n",
        "corpus_test, _, _ = ptb.load_data('test')\r\n",
        "corpus=corpus[:300000]\r\n",
        "corpus_test=corpus_test[:30000]\r\n",
        "vocab_size = len(word_to_id)\r\n",
        "xs = corpus[:-1]\r\n",
        "ts = corpus[1:]\r\n",
        "\r\n",
        "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\r\n",
        "optimizer = SGD(lr)\r\n",
        "trainer = RnnlmTrainer(model, optimizer)\r\n",
        "\r\n",
        "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\r\n",
        "            eval_interval=20)\r\n",
        "trainer.plot(ylim=(0, 1000))\r\n",
        "\r\n",
        "model.reset_state()\r\n",
        "ppl_test = eval_perplexity(model, corpus_test)\r\n",
        "print('테스트 퍼플렉서티: ', ppl_test)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 85 | 시간 0[s] | 퍼플렉서티 10000.48\n",
            "| 에폭 1 |  반복 21 / 85 | 시간 17[s] | 퍼플렉서티 2578.13\n",
            "| 에폭 1 |  반복 41 / 85 | 시간 33[s] | 퍼플렉서티 1139.29\n",
            "| 에폭 1 |  반복 61 / 85 | 시간 49[s] | 퍼플렉서티 962.38\n",
            "| 에폭 1 |  반복 81 / 85 | 시간 65[s] | 퍼플렉서티 863.38\n",
            "| 에폭 2 |  반복 1 / 85 | 시간 69[s] | 퍼플렉서티 752.62\n",
            "| 에폭 2 |  반복 21 / 85 | 시간 86[s] | 퍼플렉서티 734.71\n",
            "| 에폭 2 |  반복 41 / 85 | 시간 102[s] | 퍼플렉서티 644.15\n",
            "| 에폭 2 |  반복 61 / 85 | 시간 118[s] | 퍼플렉서티 610.70\n",
            "| 에폭 2 |  반복 81 / 85 | 시간 134[s] | 퍼플렉서티 574.05\n",
            "| 에폭 3 |  반복 1 / 85 | 시간 138[s] | 퍼플렉서티 539.99\n",
            "| 에폭 3 |  반복 21 / 85 | 시간 155[s] | 퍼플렉서티 506.16\n",
            "| 에폭 3 |  반복 41 / 85 | 시간 171[s] | 퍼플렉서티 474.04\n",
            "| 에폭 3 |  반복 61 / 85 | 시간 187[s] | 퍼플렉서티 454.20\n",
            "| 에폭 3 |  반복 81 / 85 | 시간 204[s] | 퍼플렉서티 443.70\n",
            "| 에폭 4 |  반복 1 / 85 | 시간 208[s] | 퍼플렉서티 376.40\n",
            "| 에폭 4 |  반복 21 / 85 | 시간 224[s] | 퍼플렉서티 398.15\n",
            "| 에폭 4 |  반복 41 / 85 | 시간 241[s] | 퍼플렉서티 369.84\n",
            "| 에폭 4 |  반복 61 / 85 | 시간 257[s] | 퍼플렉서티 363.90\n",
            "| 에폭 4 |  반복 81 / 85 | 시간 274[s] | 퍼플렉서티 347.72\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54140 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54540 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47113 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54000 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54140 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54540 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47113 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54000 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "퍼플렉서티 평가 중 ...\n",
            "84 / 85\n",
            "테스트 퍼플렉서티:  306.36136536817565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O9vW-Qo7vLJ"
      },
      "source": [
        "## Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iPVj8Xco9iFH",
        "outputId": "f9c3bde2-0018-47b2-9100-396149d935d6"
      },
      "source": [
        "batch_size = 100\r\n",
        "wordvec_size = 100\r\n",
        "hidden_size = 100   \r\n",
        "time_size = 35     \r\n",
        "lr = 20.0\r\n",
        "max_epoch = 4\r\n",
        "max_grad = 0.25\r\n",
        "\r\n",
        "corpus, word_to_id, id_to_word = ptb.load_data('train')\r\n",
        "corpus_test, _, _ = ptb.load_data('test')\r\n",
        "corpus=corpus[:300000]\r\n",
        "corpus_test=corpus_test[:30000]\r\n",
        "vocab_size = len(word_to_id)\r\n",
        "xs = corpus[:-1]\r\n",
        "ts = corpus[1:]\r\n",
        "\r\n",
        "model = Rnnlm(vocab_size, wordvec_size, hidden_size)\r\n",
        "optimizer = RMSprop(0.01)\r\n",
        "trainer = RnnlmTrainer(model, optimizer)\r\n",
        "\r\n",
        "trainer.fit(xs, ts, max_epoch, batch_size, time_size, max_grad,\r\n",
        "            eval_interval=20)\r\n",
        "trainer.plot(ylim=(0, 1000))\r\n",
        "\r\n",
        "model.reset_state()\r\n",
        "ppl_test = eval_perplexity(model, corpus_test)\r\n",
        "print('테스트 퍼플렉서티: ', ppl_test)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 85 | 시간 0[s] | 퍼플렉서티 9999.14\n",
            "| 에폭 1 |  반복 21 / 85 | 시간 19[s] | 퍼플렉서티 1057.91\n",
            "| 에폭 1 |  반복 41 / 85 | 시간 38[s] | 퍼플렉서티 391.35\n",
            "| 에폭 1 |  반복 61 / 85 | 시간 57[s] | 퍼플렉서티 305.92\n",
            "| 에폭 1 |  반복 81 / 85 | 시간 76[s] | 퍼플렉서티 267.50\n",
            "| 에폭 2 |  반복 1 / 85 | 시간 81[s] | 퍼플렉서티 230.25\n",
            "| 에폭 2 |  반복 21 / 85 | 시간 100[s] | 퍼플렉서티 206.33\n",
            "| 에폭 2 |  반복 41 / 85 | 시간 119[s] | 퍼플렉서티 173.73\n",
            "| 에폭 2 |  반복 61 / 85 | 시간 138[s] | 퍼플렉서티 157.09\n",
            "| 에폭 2 |  반복 81 / 85 | 시간 157[s] | 퍼플렉서티 151.22\n",
            "| 에폭 3 |  반복 1 / 85 | 시간 161[s] | 퍼플렉서티 136.77\n",
            "| 에폭 3 |  반복 21 / 85 | 시간 180[s] | 퍼플렉서티 129.86\n",
            "| 에폭 3 |  반복 41 / 85 | 시간 199[s] | 퍼플렉서티 119.34\n",
            "| 에폭 3 |  반복 61 / 85 | 시간 218[s] | 퍼플렉서티 109.91\n",
            "| 에폭 3 |  반복 81 / 85 | 시간 237[s] | 퍼플렉서티 107.92\n",
            "| 에폭 4 |  반복 1 / 85 | 시간 242[s] | 퍼플렉서티 96.72\n",
            "| 에폭 4 |  반복 21 / 85 | 시간 261[s] | 퍼플렉서티 94.93\n",
            "| 에폭 4 |  반복 41 / 85 | 시간 279[s] | 퍼플렉서티 90.57\n",
            "| 에폭 4 |  반복 61 / 85 | 시간 298[s] | 퍼플렉서티 83.28\n",
            "| 에폭 4 |  반복 81 / 85 | 시간 316[s] | 퍼플렉서티 83.45\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54140 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54540 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 47113 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54000 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48152 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 48373 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54140 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54540 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 47113 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49436 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54000 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "퍼플렉서티 평가 중 ...\n",
            "84 / 85\n",
            "테스트 퍼플렉서티:  210.17772751478512\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-Fr9mhc6fGT"
      },
      "source": [
        "# Sequence to Sequence (seq2seq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJYcC_OliSM7"
      },
      "source": [
        "## Tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_-Xgz8b6gI4",
        "outputId": "ec7952ec-441b-4114-cf7d-bd140363e6cd"
      },
      "source": [
        "(x_train, t_train), (x_test, t_test) = sequence.load_data('addition.txt')\r\n",
        "char_to_id, id_to_char = sequence.get_vocab()\r\n",
        "\r\n",
        "is_reverse = True    \r\n",
        "if is_reverse:\r\n",
        "    x_train, x_test = x_train[:, ::-1], x_test[:, ::-1]\r\n",
        "\r\n",
        "vocab_size = len(char_to_id)\r\n",
        "wordvec_size = 32\r\n",
        "hideen_size = 256\r\n",
        "batch_size = 128\r\n",
        "max_epoch = 10\r\n",
        "max_grad = 5.0\r\n",
        "\r\n",
        "model = AttentionSeq2seq(vocab_size, wordvec_size, hideen_size)\r\n",
        "optimizer = Adam(0.01)\r\n",
        "trainer = Trainer(model, optimizer)\r\n",
        "\r\n",
        "acc_list = []\r\n",
        "for epoch in range(max_epoch):\r\n",
        "    trainer.fit(x_train, t_train, max_epoch=1,\r\n",
        "                batch_size=batch_size, max_grad=max_grad)\r\n",
        "\r\n",
        "    correct_num = 0\r\n",
        "    for i in range(len(x_test)):\r\n",
        "        question, correct = x_test[[i]], t_test[[i]]\r\n",
        "        verbose = i < 10\r\n",
        "        correct_num += eval_seq2seq(model, question, correct,\r\n",
        "                                    id_to_char, verbose, is_reverse)\r\n",
        "\r\n",
        "    acc = float(correct_num) / len(x_test)\r\n",
        "    acc_list.append(acc)\r\n",
        "    print('검증 정확도 %.3f%%' % (acc * 100))\r\n",
        "\r\n",
        "x = np.arange(len(acc_list))\r\n",
        "plt.plot(x, acc_list, marker='o')\r\n",
        "plt.xlabel('epoch')\r\n",
        "plt.ylabel(\"acc\")\r\n",
        "plt.ylim(0, 1.0)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| 에폭 1 |  반복 1 / 351 | 시간 0[s] | 손실 2.56\n",
            "| 에폭 1 |  반복 21 / 351 | 시간 2[s] | 손실 2.11\n",
            "| 에폭 1 |  반복 41 / 351 | 시간 5[s] | 손실 1.80\n",
            "| 에폭 1 |  반복 61 / 351 | 시간 8[s] | 손실 1.76\n",
            "| 에폭 1 |  반복 81 / 351 | 시간 11[s] | 손실 1.72\n",
            "| 에폭 1 |  반복 101 / 351 | 시간 13[s] | 손실 1.67\n",
            "| 에폭 1 |  반복 121 / 351 | 시간 16[s] | 손실 1.58\n",
            "| 에폭 1 |  반복 141 / 351 | 시간 19[s] | 손실 1.46\n",
            "| 에폭 1 |  반복 161 / 351 | 시간 22[s] | 손실 1.40\n",
            "| 에폭 1 |  반복 181 / 351 | 시간 25[s] | 손실 1.34\n",
            "| 에폭 1 |  반복 201 / 351 | 시간 28[s] | 손실 1.29\n",
            "| 에폭 1 |  반복 221 / 351 | 시간 30[s] | 손실 1.24\n",
            "| 에폭 1 |  반복 241 / 351 | 시간 33[s] | 손실 1.21\n",
            "| 에폭 1 |  반복 261 / 351 | 시간 36[s] | 손실 1.17\n",
            "| 에폭 1 |  반복 281 / 351 | 시간 39[s] | 손실 1.13\n",
            "| 에폭 1 |  반복 301 / 351 | 시간 42[s] | 손실 1.10\n",
            "| 에폭 1 |  반복 321 / 351 | 시간 44[s] | 손실 1.08\n",
            "| 에폭 1 |  반복 341 / 351 | 시간 47[s] | 손실 1.03\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[91m☒\u001b[0m 160 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1171\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 673 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[91m☒\u001b[0m 161 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 420 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 878 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1128\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1428\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[91m☒\u001b[0m 865 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 229 \n",
            "---\n",
            "검증 정확도 7.980%\n",
            "| 에폭 2 |  반복 1 / 351 | 시간 0[s] | 손실 0.97\n",
            "| 에폭 2 |  반복 21 / 351 | 시간 3[s] | 손실 0.90\n",
            "| 에폭 2 |  반복 41 / 351 | 시간 5[s] | 손실 0.79\n",
            "| 에폭 2 |  반복 61 / 351 | 시간 8[s] | 손실 0.71\n",
            "| 에폭 2 |  반복 81 / 351 | 시간 11[s] | 손실 0.67\n",
            "| 에폭 2 |  반복 101 / 351 | 시간 14[s] | 손실 0.61\n",
            "| 에폭 2 |  반복 121 / 351 | 시간 17[s] | 손실 0.55\n",
            "| 에폭 2 |  반복 141 / 351 | 시간 20[s] | 손실 0.53\n",
            "| 에폭 2 |  반복 161 / 351 | 시간 23[s] | 손실 0.46\n",
            "| 에폭 2 |  반복 181 / 351 | 시간 26[s] | 손실 0.41\n",
            "| 에폭 2 |  반복 201 / 351 | 시간 29[s] | 손실 0.39\n",
            "| 에폭 2 |  반복 221 / 351 | 시간 31[s] | 손실 0.37\n",
            "| 에폭 2 |  반복 241 / 351 | 시간 34[s] | 손실 0.36\n",
            "| 에폭 2 |  반복 261 / 351 | 시간 37[s] | 손실 0.30\n",
            "| 에폭 2 |  반복 281 / 351 | 시간 40[s] | 손실 0.26\n",
            "| 에폭 2 |  반복 301 / 351 | 시간 43[s] | 손실 0.25\n",
            "| 에폭 2 |  반복 321 / 351 | 시간 46[s] | 손실 0.26\n",
            "| 에폭 2 |  반복 341 / 351 | 시간 49[s] | 손실 0.26\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1149\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1153\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1417\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[91m☒\u001b[0m 226 \n",
            "---\n",
            "검증 정확도 69.300%\n",
            "| 에폭 3 |  반복 1 / 351 | 시간 0[s] | 손실 0.24\n",
            "| 에폭 3 |  반복 21 / 351 | 시간 3[s] | 손실 0.22\n",
            "| 에폭 3 |  반복 41 / 351 | 시간 5[s] | 손실 0.21\n",
            "| 에폭 3 |  반복 61 / 351 | 시간 8[s] | 손실 0.19\n",
            "| 에폭 3 |  반복 81 / 351 | 시간 11[s] | 손실 0.17\n",
            "| 에폭 3 |  반복 101 / 351 | 시간 14[s] | 손실 0.17\n",
            "| 에폭 3 |  반복 121 / 351 | 시간 17[s] | 손실 0.16\n",
            "| 에폭 3 |  반복 141 / 351 | 시간 20[s] | 손실 0.15\n",
            "| 에폭 3 |  반복 161 / 351 | 시간 23[s] | 손실 0.14\n",
            "| 에폭 3 |  반복 181 / 351 | 시간 25[s] | 손실 0.12\n",
            "| 에폭 3 |  반복 201 / 351 | 시간 28[s] | 손실 0.14\n",
            "| 에폭 3 |  반복 221 / 351 | 시간 31[s] | 손실 0.16\n",
            "| 에폭 3 |  반복 241 / 351 | 시간 34[s] | 손실 0.14\n",
            "| 에폭 3 |  반복 261 / 351 | 시간 37[s] | 손실 0.12\n",
            "| 에폭 3 |  반복 281 / 351 | 시간 40[s] | 손실 0.11\n",
            "| 에폭 3 |  반복 301 / 351 | 시간 42[s] | 손실 0.10\n",
            "| 에폭 3 |  반복 321 / 351 | 시간 45[s] | 손실 0.09\n",
            "| 에폭 3 |  반복 341 / 351 | 시간 48[s] | 손실 0.09\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[91m☒\u001b[0m 412 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[91m☒\u001b[0m 1437\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 87.560%\n",
            "| 에폭 4 |  반복 1 / 351 | 시간 0[s] | 손실 0.10\n",
            "| 에폭 4 |  반복 21 / 351 | 시간 3[s] | 손실 0.11\n",
            "| 에폭 4 |  반복 41 / 351 | 시간 5[s] | 손실 0.10\n",
            "| 에폭 4 |  반복 61 / 351 | 시간 8[s] | 손실 0.10\n",
            "| 에폭 4 |  반복 81 / 351 | 시간 11[s] | 손실 0.09\n",
            "| 에폭 4 |  반복 101 / 351 | 시간 14[s] | 손실 0.08\n",
            "| 에폭 4 |  반복 121 / 351 | 시간 17[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 141 / 351 | 시간 20[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 161 / 351 | 시간 23[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 181 / 351 | 시간 25[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 201 / 351 | 시간 28[s] | 손실 0.06\n",
            "| 에폭 4 |  반복 221 / 351 | 시간 31[s] | 손실 0.06\n",
            "| 에폭 4 |  반복 241 / 351 | 시간 34[s] | 손실 0.06\n",
            "| 에폭 4 |  반복 261 / 351 | 시간 37[s] | 손실 0.08\n",
            "| 에폭 4 |  반복 281 / 351 | 시간 40[s] | 손실 0.08\n",
            "| 에폭 4 |  반복 301 / 351 | 시간 43[s] | 손실 0.07\n",
            "| 에폭 4 |  반복 321 / 351 | 시간 45[s] | 손실 0.06\n",
            "| 에폭 4 |  반복 341 / 351 | 시간 48[s] | 손실 0.07\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1063\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 92.100%\n",
            "| 에폭 5 |  반복 1 / 351 | 시간 0[s] | 손실 0.05\n",
            "| 에폭 5 |  반복 21 / 351 | 시간 2[s] | 손실 0.06\n",
            "| 에폭 5 |  반복 41 / 351 | 시간 5[s] | 손실 0.06\n",
            "| 에폭 5 |  반복 61 / 351 | 시간 8[s] | 손실 0.07\n",
            "| 에폭 5 |  반복 81 / 351 | 시간 11[s] | 손실 0.07\n",
            "| 에폭 5 |  반복 101 / 351 | 시간 14[s] | 손실 0.07\n",
            "| 에폭 5 |  반복 121 / 351 | 시간 16[s] | 손실 0.06\n",
            "| 에폭 5 |  반복 141 / 351 | 시간 19[s] | 손실 0.06\n",
            "| 에폭 5 |  반복 161 / 351 | 시간 22[s] | 손실 0.07\n",
            "| 에폭 5 |  반복 181 / 351 | 시간 25[s] | 손실 0.06\n",
            "| 에폭 5 |  반복 201 / 351 | 시간 28[s] | 손실 0.05\n",
            "| 에폭 5 |  반복 221 / 351 | 시간 30[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 241 / 351 | 시간 33[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 261 / 351 | 시간 36[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 281 / 351 | 시간 39[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 301 / 351 | 시간 42[s] | 손실 0.04\n",
            "| 에폭 5 |  반복 321 / 351 | 시간 44[s] | 손실 0.05\n",
            "| 에폭 5 |  반복 341 / 351 | 시간 47[s] | 손실 0.05\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 94.020%\n",
            "| 에폭 6 |  반복 1 / 351 | 시간 0[s] | 손실 0.03\n",
            "| 에폭 6 |  반복 21 / 351 | 시간 3[s] | 손실 0.05\n",
            "| 에폭 6 |  반복 41 / 351 | 시간 5[s] | 손실 0.06\n",
            "| 에폭 6 |  반복 61 / 351 | 시간 8[s] | 손실 0.05\n",
            "| 에폭 6 |  반복 81 / 351 | 시간 11[s] | 손실 0.05\n",
            "| 에폭 6 |  반복 101 / 351 | 시간 14[s] | 손실 0.04\n",
            "| 에폭 6 |  반복 121 / 351 | 시간 17[s] | 손실 0.04\n",
            "| 에폭 6 |  반복 141 / 351 | 시간 20[s] | 손실 0.03\n",
            "| 에폭 6 |  반복 161 / 351 | 시간 23[s] | 손실 0.04\n",
            "| 에폭 6 |  반복 181 / 351 | 시간 26[s] | 손실 0.03\n",
            "| 에폭 6 |  반복 201 / 351 | 시간 28[s] | 손실 0.02\n",
            "| 에폭 6 |  반복 221 / 351 | 시간 31[s] | 손실 0.03\n",
            "| 에폭 6 |  반복 241 / 351 | 시간 34[s] | 손실 0.03\n",
            "| 에폭 6 |  반복 261 / 351 | 시간 37[s] | 손실 0.03\n",
            "| 에폭 6 |  반복 281 / 351 | 시간 40[s] | 손실 0.04\n",
            "| 에폭 6 |  반복 301 / 351 | 시간 42[s] | 손실 0.04\n",
            "| 에폭 6 |  반복 321 / 351 | 시간 45[s] | 손실 0.04\n",
            "| 에폭 6 |  반복 341 / 351 | 시간 48[s] | 손실 0.05\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 847 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[91m☒\u001b[0m 1043\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 94.500%\n",
            "| 에폭 7 |  반복 1 / 351 | 시간 0[s] | 손실 0.05\n",
            "| 에폭 7 |  반복 21 / 351 | 시간 2[s] | 손실 0.04\n",
            "| 에폭 7 |  반복 41 / 351 | 시간 5[s] | 손실 0.06\n",
            "| 에폭 7 |  반복 61 / 351 | 시간 8[s] | 손실 0.08\n",
            "| 에폭 7 |  반복 81 / 351 | 시간 11[s] | 손실 0.08\n",
            "| 에폭 7 |  반복 101 / 351 | 시간 14[s] | 손실 0.07\n",
            "| 에폭 7 |  반복 121 / 351 | 시간 16[s] | 손실 0.06\n",
            "| 에폭 7 |  반복 141 / 351 | 시간 19[s] | 손실 0.05\n",
            "| 에폭 7 |  반복 161 / 351 | 시간 22[s] | 손실 0.05\n",
            "| 에폭 7 |  반복 181 / 351 | 시간 25[s] | 손실 0.05\n",
            "| 에폭 7 |  반복 201 / 351 | 시간 28[s] | 손실 0.04\n",
            "| 에폭 7 |  반복 221 / 351 | 시간 30[s] | 손실 0.04\n",
            "| 에폭 7 |  반복 241 / 351 | 시간 33[s] | 손실 0.04\n",
            "| 에폭 7 |  반복 261 / 351 | 시간 36[s] | 손실 0.04\n",
            "| 에폭 7 |  반복 281 / 351 | 시간 39[s] | 손실 0.03\n",
            "| 에폭 7 |  반복 301 / 351 | 시간 42[s] | 손실 0.03\n",
            "| 에폭 7 |  반복 321 / 351 | 시간 45[s] | 손실 0.04\n",
            "| 에폭 7 |  반복 341 / 351 | 시간 47[s] | 손실 0.04\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[91m☒\u001b[0m 676 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[91m☒\u001b[0m 847 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 95.780%\n",
            "| 에폭 8 |  반복 1 / 351 | 시간 0[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 21 / 351 | 시간 2[s] | 손실 0.03\n",
            "| 에폭 8 |  반복 41 / 351 | 시간 5[s] | 손실 0.04\n",
            "| 에폭 8 |  반복 61 / 351 | 시간 8[s] | 손실 0.05\n",
            "| 에폭 8 |  반복 81 / 351 | 시간 11[s] | 손실 0.04\n",
            "| 에폭 8 |  반복 101 / 351 | 시간 14[s] | 손실 0.03\n",
            "| 에폭 8 |  반복 121 / 351 | 시간 17[s] | 손실 0.03\n",
            "| 에폭 8 |  반복 141 / 351 | 시간 20[s] | 손실 0.03\n",
            "| 에폭 8 |  반복 161 / 351 | 시간 23[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 181 / 351 | 시간 26[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 201 / 351 | 시간 29[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 221 / 351 | 시간 31[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 241 / 351 | 시간 34[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 261 / 351 | 시간 37[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 281 / 351 | 시간 40[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 301 / 351 | 시간 43[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 321 / 351 | 시간 46[s] | 손실 0.02\n",
            "| 에폭 8 |  반복 341 / 351 | 시간 49[s] | 손실 0.02\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 98.020%\n",
            "| 에폭 9 |  반복 1 / 351 | 시간 0[s] | 손실 0.02\n",
            "| 에폭 9 |  반복 21 / 351 | 시간 3[s] | 손실 0.01\n",
            "| 에폭 9 |  반복 41 / 351 | 시간 6[s] | 손실 0.01\n",
            "| 에폭 9 |  반복 61 / 351 | 시간 9[s] | 손실 0.01\n",
            "| 에폭 9 |  반복 81 / 351 | 시간 12[s] | 손실 0.01\n",
            "| 에폭 9 |  반복 101 / 351 | 시간 15[s] | 손실 0.01\n",
            "| 에폭 9 |  반복 121 / 351 | 시간 17[s] | 손실 0.01\n",
            "| 에폭 9 |  반복 141 / 351 | 시간 20[s] | 손실 0.02\n",
            "| 에폭 9 |  반복 161 / 351 | 시간 23[s] | 손실 0.04\n",
            "| 에폭 9 |  반복 181 / 351 | 시간 26[s] | 손실 0.04\n",
            "| 에폭 9 |  반복 201 / 351 | 시간 29[s] | 손실 0.05\n",
            "| 에폭 9 |  반복 221 / 351 | 시간 32[s] | 손실 0.06\n",
            "| 에폭 9 |  반복 241 / 351 | 시간 34[s] | 손실 0.06\n",
            "| 에폭 9 |  반복 261 / 351 | 시간 37[s] | 손실 0.06\n",
            "| 에폭 9 |  반복 281 / 351 | 시간 40[s] | 손실 0.06\n",
            "| 에폭 9 |  반복 301 / 351 | 시간 43[s] | 손실 0.06\n",
            "| 에폭 9 |  반복 321 / 351 | 시간 46[s] | 손실 0.05\n",
            "| 에폭 9 |  반복 341 / 351 | 시간 49[s] | 손실 0.04\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[91m☒\u001b[0m 1039\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 95.680%\n",
            "| 에폭 10 |  반복 1 / 351 | 시간 0[s] | 손실 0.05\n",
            "| 에폭 10 |  반복 21 / 351 | 시간 2[s] | 손실 0.04\n",
            "| 에폭 10 |  반복 41 / 351 | 시간 5[s] | 손실 0.04\n",
            "| 에폭 10 |  반복 61 / 351 | 시간 8[s] | 손실 0.04\n",
            "| 에폭 10 |  반복 81 / 351 | 시간 11[s] | 손실 0.04\n",
            "| 에폭 10 |  반복 101 / 351 | 시간 14[s] | 손실 0.03\n",
            "| 에폭 10 |  반복 121 / 351 | 시간 17[s] | 손실 0.03\n",
            "| 에폭 10 |  반복 141 / 351 | 시간 19[s] | 손실 0.02\n",
            "| 에폭 10 |  반복 161 / 351 | 시간 22[s] | 손실 0.02\n",
            "| 에폭 10 |  반복 181 / 351 | 시간 25[s] | 손실 0.02\n",
            "| 에폭 10 |  반복 201 / 351 | 시간 28[s] | 손실 0.02\n",
            "| 에폭 10 |  반복 221 / 351 | 시간 31[s] | 손실 0.02\n",
            "| 에폭 10 |  반복 241 / 351 | 시간 33[s] | 손실 0.02\n",
            "| 에폭 10 |  반복 261 / 351 | 시간 36[s] | 손실 0.01\n",
            "| 에폭 10 |  반복 281 / 351 | 시간 39[s] | 손실 0.01\n",
            "| 에폭 10 |  반복 301 / 351 | 시간 42[s] | 손실 0.07\n",
            "| 에폭 10 |  반복 321 / 351 | 시간 45[s] | 손실 0.05\n",
            "| 에폭 10 |  반복 341 / 351 | 시간 48[s] | 손실 0.04\n",
            "Q 77+85  \n",
            "T 162 \n",
            "\u001b[92m☑\u001b[0m 162 \n",
            "---\n",
            "Q 975+164\n",
            "T 1139\n",
            "\u001b[92m☑\u001b[0m 1139\n",
            "---\n",
            "Q 582+84 \n",
            "T 666 \n",
            "\u001b[92m☑\u001b[0m 666 \n",
            "---\n",
            "Q 8+155  \n",
            "T 163 \n",
            "\u001b[92m☑\u001b[0m 163 \n",
            "---\n",
            "Q 367+55 \n",
            "T 422 \n",
            "\u001b[92m☑\u001b[0m 422 \n",
            "---\n",
            "Q 600+257\n",
            "T 857 \n",
            "\u001b[92m☑\u001b[0m 857 \n",
            "---\n",
            "Q 761+292\n",
            "T 1053\n",
            "\u001b[92m☑\u001b[0m 1053\n",
            "---\n",
            "Q 830+597\n",
            "T 1427\n",
            "\u001b[92m☑\u001b[0m 1427\n",
            "---\n",
            "Q 26+838 \n",
            "T 864 \n",
            "\u001b[92m☑\u001b[0m 864 \n",
            "---\n",
            "Q 143+93 \n",
            "T 236 \n",
            "\u001b[92m☑\u001b[0m 236 \n",
            "---\n",
            "검증 정확도 95.380%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}
